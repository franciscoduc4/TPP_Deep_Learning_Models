import os, sys
import tensorflow as tf
import numpy as np
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, Dense, Conv1D, Flatten, Concatenate,
    BatchNormalization, Dropout, LayerNormalization
)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import Huber
from collections import deque
import random
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Any, Optional, Union, Callable

PROJECT_ROOT = os.path.abspath(os.getcwd())
sys.path.append(PROJECT_ROOT) 

from models.config import DQN_CONFIG


class replay_buffer:
    """
    Buffer de experiencias para el algoritmo DQN.
    
    Almacena transiciones (state, action, reward, next_state, done)
    y permite muestrear lotes de manera aleatoria para el entrenamiento.
    
    Parámetros:
    -----------
    capacity : int, opcional
        Capacidad máxima del buffer (default: 10000)
    """
    def __init__(self, capacity: int = 10000) -> None:
        self.buffer = deque(maxlen=capacity)
    
    def add(self, state: np.ndarray, action: int, reward: float, 
           next_state: np.ndarray, done: float) -> None:
        """
        Añade una transición al buffer.
        
        Parámetros:
        -----------
        state : np.ndarray
            Estado actual
        action : int
            Acción tomada
        reward : float
            Recompensa recibida
        next_state : np.ndarray
            Estado siguiente
        done : float
            Indicador de fin de episodio (1.0 si terminó, 0.0 si no)
        """
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        Muestrea un lote aleatorio de transiciones.
        
        Parámetros:
        -----------
        batch_size : int
            Tamaño del lote a muestrear
            
        Retorna:
        --------
        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]
            (states, actions, rewards, next_states, dones)
        """
        if len(self.buffer) < batch_size:
            # Si no hay suficientes transiciones, devuelve lo que haya
            batch = random.sample(self.buffer, len(self.buffer))
        else:
            batch = random.sample(self.buffer, batch_size)
            
        states, actions, rewards, next_states, dones = zip(*batch)
        
        return (
            np.array(states, dtype=np.float32),
            np.array(actions, dtype=np.int32),
            np.array(rewards, dtype=np.float32),
            np.array(next_states, dtype=np.float32),
            np.array(dones, dtype=np.float32)
        )
    
    def __len__(self) -> int:
        """
        Retorna la cantidad de transiciones almacenadas.
        
        Retorna:
        --------
        int
            Número de transiciones en el buffer
        """
        return len(self.buffer)


class prioritized_replay_buffer(replay_buffer):
    """
    Buffer de experiencias con muestreo prioritario.
    
    Prioriza experiencias basadas en el TD error para muestreo eficiente.
    
    Parámetros:
    -----------
    capacity : int
        Capacidad máxima del buffer
    """
    def __init__(self, capacity: int) -> None:
        super().__init__(capacity)
        self.priorities = np.zeros(capacity, dtype=np.float32) + 1e-5
        self.pos = 0
    
    def add(self, state: np.ndarray, action: int, reward: float, 
           next_state: np.ndarray, done: float) -> None:
        """
        Añade una transición al buffer con prioridad máxima.
        
        Parámetros:
        -----------
        state : np.ndarray
            Estado actual
        action : int
            Acción tomada
        reward : float
            Recompensa recibida
        next_state : np.ndarray
            Estado siguiente
        done : float
            Indicador de fin de episodio (1.0 si terminó, 0.0 si no)
        """
        max_priority = max(np.max(self.priorities), 1e-5)
        
        if len(self.buffer) < self.buffer.maxlen:
            self.buffer.append((state, action, reward, next_state, done))
        else:
            self.buffer[self.pos] = (state, action, reward, next_state, done)
        
        self.priorities[self.pos] = max_priority
        self.pos = (self.pos + 1) % self.buffer.maxlen
    
    def sample(self, batch_size: int, beta: float = 0.4) -> Tuple[np.ndarray, np.ndarray, 
                                                                np.ndarray, np.ndarray, 
                                                                np.ndarray, List[int], np.ndarray]:
        """
        Muestrea un lote basado en prioridades.
        
        Parámetros:
        -----------
        batch_size : int
            Tamaño del lote
        beta : float, opcional
            Factor para corrección de sesgo (0-1) (default: 0.4)
            
        Retorna:
        --------
        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, List[int], np.ndarray]
            (states, actions, rewards, next_states, dones, índices, pesos de importancia)
        """
        if len(self.buffer) < batch_size:
            idx = list(range(len(self.buffer)))
        else:
            # Calcular probabilidades de muestreo basadas en prioridad
            priorities = self.priorities[:len(self.buffer)]
            probabilities = priorities ** DQN_CONFIG['priority_alpha']
            probabilities /= np.sum(probabilities)
            
            # Muestreo según distribución
            rng = np.random.default_rng(seed=42)
            idx = rng.choice(len(self.buffer), batch_size, p=probabilities, replace=False).tolist()
        
        # Extraer batch
        states, actions, rewards, next_states, dones = [], [], [], [], []
        for i in idx:
            s, a, r, ns, d = self.buffer[i]
            states.append(s)
            actions.append(a)
            rewards.append(r)
            next_states.append(ns)
            dones.append(d)
        
        # Calcular pesos de importancia (corrección del sesgo)
        weights = np.zeros(batch_size, dtype=np.float32)
        priorities = self.priorities[idx]
        # Evitar división por cero
        probabilities = priorities / np.sum(self.priorities[:len(self.buffer)])
        weights = (len(self.buffer) * probabilities) ** -beta
        weights /= np.max(weights)  # Normalizar
        
        return (
            np.array(states, dtype=np.float32),
            np.array(actions, dtype=np.int32),
            np.array(rewards, dtype=np.float32),
            np.array(next_states, dtype=np.float32),
            np.array(dones, dtype=np.float32),
            idx,
            np.array(weights, dtype=np.float32)
        )
    
    def update_priorities(self, indices: List[int], priorities: np.ndarray) -> None:
        """
        Actualiza las prioridades para los índices dados.
        
        Parámetros:
        -----------
        indices : List[int]
            Índices de las transiciones a actualizar
        priorities : np.ndarray
            Nuevas prioridades
        """
        for idx, priority in zip(indices, priorities):
            self.priorities[idx] = priority


class q_network(Model):
    """
    Red Q para DQN que mapea estados a valores Q.
    
    Implementa una arquitectura flexible para estimación de valores Q-state-action.
    
    Parámetros:
    -----------
    state_dim : int
        Dimensión del espacio de estados
    action_dim : int
        Dimensión del espacio de acciones
    hidden_units : Optional[List[int]], opcional
        Unidades en capas ocultas (default: None)
    dueling : bool, opcional
        Si usar arquitectura dueling (default: False)
    activation : str, opcional
        Función de activación a utilizar (default: 'relu')
    dropout_rate : float, opcional
        Tasa de dropout para regularización (default: 0.0)
    """
    def __init__(self, state_dim: int, action_dim: int, 
                hidden_units: Optional[List[int]] = None,
                dueling: bool = False,
                activation: str = 'relu',
                dropout_rate: float = 0.0) -> None:
        super(q_network, self).__init__()
        
        # Valores predeterminados para capas ocultas
        if hidden_units is None:
            hidden_units = DQN_CONFIG['hidden_units']
        
        self.dueling = dueling
        self.action_dim = action_dim
        self.activation_name = activation
        self.dropout_rate = dropout_rate
        
        # Capas para el procesamiento del estado
        self.feature_layers = []
        for i, units in enumerate(hidden_units):
            self.feature_layers.append(Dense(
                units, 
                name=f'feature_dense_{i}'
            ))
            self.feature_layers.append(LayerNormalization(
                epsilon=DQN_CONFIG['epsilon'],
                name=f'feature_ln_{i}'
            ))
            self.feature_layers.append(Dropout(
                dropout_rate,
                name=f'feature_dropout_{i}'
            ))
        
        # Para arquitectura Dueling DQN
        if dueling:
            # Ventaja: un valor por acción
            self.advantage_layers = []
            for i, units in enumerate(hidden_units[-2:]):
                self.advantage_layers.append(Dense(
                    units, 
                    name=f'advantage_dense_{i}'
                ))
            self.advantage_output = Dense(action_dim, name='advantage')
            
            # Valor de estado: un valor único
            self.value_layers = []
            for i, units in enumerate(hidden_units[-2:]):
                self.value_layers.append(Dense(
                    units, 
                    name=f'value_dense_{i}'
                ))
            self.value_output = Dense(1, name='value')
        else:
            # DQN clásica: predecir valor Q para cada acción
            self.q_output = Dense(action_dim, name='q_values')
    
    def _activation(self, x: tf.Tensor) -> tf.Tensor:
        """
        Aplica la función de activación especificada.
        
        Parámetros:
        -----------
        x : tf.Tensor
            Tensor de entrada
            
        Retorna:
        --------
        tf.Tensor
            Tensor con activación aplicada
        """
        if self.activation_name == 'relu':
            return tf.nn.relu(x)
        elif self.activation_name == 'tanh':
            return tf.nn.tanh(x)
        elif self.activation_name == 'leaky_relu':
            return tf.nn.leaky_relu(x)
        elif self.activation_name == 'gelu':
            return tf.nn.gelu(x)
        else:
            # Por defecto usar ReLU
            return tf.nn.relu(x)
    
    def _apply_layers(self, x: tf.Tensor, layers: List, training: bool = False) -> tf.Tensor:
        """
        Aplica una secuencia de capas al tensor de entrada.
        
        Parámetros:
        -----------
        x : tf.Tensor
            Tensor de entrada
        layers : List
            Lista de capas a aplicar
        training : bool, opcional
            Indica si está en modo entrenamiento (default: False)
            
        Retorna:
        --------
        tf.Tensor
            Tensor resultante
        """
        for layer in layers:
            if isinstance(layer, Dense):
                x = layer(x)
                x = self._activation(x)
            elif isinstance(layer, Dropout):
                x = layer(x, training=training)
            else:
                x = layer(x)
        return x
    
    def _process_dueling_network(self, x: tf.Tensor, training: bool = False) -> tf.Tensor:
        """
        Procesa las entradas a través de una arquitectura dueling.
        
        Parámetros:
        -----------
        x : tf.Tensor
            Tensor de características
        training : bool, opcional
            Indica si está en modo entrenamiento (default: False)
            
        Retorna:
        --------
        tf.Tensor
            Valores Q estimados
        """
        # Ventaja para cada acción
        advantage = self._apply_layers(x, self.advantage_layers, training)
        advantage = self.advantage_output(advantage)
        
        # Valor del estado
        value = self._apply_layers(x, self.value_layers, training)
        value = self.value_output(value)
        
        # Combinar ventaja y valor (restando la media de ventajas)
        return value + (advantage - tf.reduce_mean(advantage, axis=1, keepdims=True))
    
    def call(self, inputs: tf.Tensor, training: bool = False) -> tf.Tensor:
        """
        Pasa la entrada por la red Q.
        
        Parámetros:
        -----------
        inputs : tf.Tensor
            Tensor de entrada (estados)
        training : bool, opcional
            Indica si está en modo entrenamiento (default: False)
            
        Retorna:
        --------
        tf.Tensor
            Valores Q para cada acción
        """
        # Procesar características de estado
        x = self._apply_layers(inputs, self.feature_layers, training)
        
        # Aplicar arquitectura dueling o estándar
        if self.dueling:
            return self._process_dueling_network(x, training)
        else:
            # DQN estándar
            return self.q_output(x)
    
    def get_action(self, state: np.ndarray, epsilon: float = 0.0) -> int:
        """
        Obtiene una acción usando la política epsilon-greedy.
        
        Parámetros:
        -----------
        state : np.ndarray
            Estado actual del entorno
        epsilon : float, opcional
            Probabilidad de exploración (0-1) (default: 0.0)
            
        Retorna:
        --------
        int
            Acción seleccionada según la política
        """
        rng = np.random.default_rng(seed=42)
        if rng.random() < epsilon:
            # Explorar: acción aleatoria
            return int(rng.integers(0, self.action_dim))
        else:
            # Explotar: mejor acción según la red
            state = tf.convert_to_tensor([state], dtype=tf.float32)
            q_values = self(state)
            return int(tf.argmax(q_values[0]).numpy())


class dqn:
    """
    Implementación del algoritmo Deep Q-Network (DQN).
    
    Incluye mecanismos de Experience Replay y Target Network para
    mejorar la estabilidad del aprendizaje.
    
    Parámetros:
    -----------
    state_dim : int
        Dimensión del espacio de estados
    action_dim : int
        Dimensión del espacio de acciones
    config : Optional[Dict[str, Any]], opcional
        Configuración personalizada (default: None)
    hidden_units : Optional[List[int]], opcional
        Unidades en capas ocultas (default: None)
    seed : int, opcional
        Semilla para los generadores de números aleatorios (default: 42)
    """
    def __init__(
        self, 
        state_dim: int, 
        action_dim: int,
        config: Optional[Dict[str, Any]] = None,
        hidden_units: Optional[List[int]] = None,
        seed: int = 42
    ) -> None:
        # Configurar semillas para reproducibilidad
        tf.random.set_seed(seed)
        np.random.seed(seed)
        random.seed(seed)
        
        # Use provided config or default
        self.config = config or DQN_CONFIG
        
        # Extract configuration parameters
        learning_rate = self.config.get('learning_rate', 0.001)
        self.gamma = self.config.get('gamma', 0.99)
        self.epsilon_start = self.config.get('epsilon_start', 1.0)
        self.epsilon_end = self.config.get('epsilon_end', 0.01)
        self.epsilon_decay = self.config.get('epsilon_decay', 0.995)
        buffer_capacity = self.config.get('buffer_capacity', 10000)
        self.batch_size = self.config.get('batch_size', 64)
        self.target_update_freq = self.config.get('target_update_freq', 100)
        dueling = self.config.get('dueling', False)
        self.double = self.config.get('double', False)
        self.prioritized = self.config.get('prioritized', False)
        dropout_rate = self.config.get('dropout_rate', 0.0)
        activation = self.config.get('activation', 'relu')
        
        # Parámetros del entorno y del modelo
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.epsilon = self.epsilon_start
        
        # Cantidad de actualizaciones realizadas
        self.update_counter = 0
        
        # Valores predeterminados para capas ocultas
        if hidden_units is None:
            self.hidden_units = DQN_CONFIG['hidden_units']
        else:
            self.hidden_units = hidden_units
        
        # Crear modelo Q y modelo Q Target
        self.q_network = q_network(
            state_dim, 
            action_dim, 
            self.hidden_units, 
            dueling,
            activation,
            dropout_rate
        )
        self.target_q_network = q_network(
            state_dim, 
            action_dim, 
            self.hidden_units, 
            dueling,
            activation,
            dropout_rate
        )
        
        # Asegurar que ambos modelos estén construidos
        dummy_state = np.zeros((1, state_dim), dtype=np.float32)
        _ = self.q_network(dummy_state)
        _ = self.target_q_network(dummy_state)
        
        # Actualizar pesos del target para que sean iguales al modelo principal
        self.update_target_network()
        
        # Optimizador y función de pérdida
        self.optimizer = Adam(learning_rate=learning_rate)
        self.loss_fn = Huber()  # Menos sensible a outliers que MSE
        
        # Buffer de experiencias
        if self.prioritized:
            # Con prioridad de muestreo basada en TD-error
            self.replay_buffer = prioritized_replay_buffer(buffer_capacity)
            self.alpha = DQN_CONFIG['priority_alpha']
            self.beta = DQN_CONFIG['priority_beta']
            self.beta_increment = DQN_CONFIG['priority_beta_increment']
        else:
            # Buffer uniforme clásico
            self.replay_buffer = replay_buffer(buffer_capacity)
        
        # Métricas acumuladas
        self.loss_sum = 0.0
        self.q_value_sum = 0.0
        self.updates_count = 0
    
    def update_target_network(self) -> None:
        """
        Actualiza los pesos del modelo target con los del modelo principal.
        """
        self.target_q_network.set_weights(self.q_network.get_weights())
    
    @tf.function
    def _train_step(self, states: tf.Tensor, actions: tf.Tensor, rewards: tf.Tensor, 
                  next_states: tf.Tensor, dones: tf.Tensor, 
                  importance_weights: Optional[tf.Tensor] = None) -> Tuple[tf.Tensor, tf.Tensor]:
        """
        Realiza un paso de entrenamiento para actualizar la red Q.
        
        Parámetros:
        -----------
        states : tf.Tensor
            Estados observados
        actions : tf.Tensor
            Acciones tomadas
        rewards : tf.Tensor
            Recompensas recibidas
        next_states : tf.Tensor
            Estados siguientes
        dones : tf.Tensor
            Indicadores de fin de episodio
        importance_weights : Optional[tf.Tensor], opcional
            Pesos de importancia para PER (default: None)
            
        Retorna:
        --------
        Tuple[tf.Tensor, tf.Tensor]
            (pérdida, td_errors)
        """
        # Convertir acciones a índices one-hot para gather
        action_indices = tf.one_hot(actions, self.action_dim)
        
        with tf.GradientTape() as tape:
            # Q-values para los estados actuales
            q_values = self.q_network(states, training=True)
            q_values_for_actions = tf.reduce_sum(q_values * action_indices, axis=1)
            
            # Q-values objetivos para los siguientes estados
            if self.double:
                # Double DQN: seleccionar acción con red primaria
                next_actions = tf.argmax(self.q_network(next_states), axis=1)
                next_action_indices = tf.one_hot(next_actions, self.action_dim)
                next_q_values = self.target_q_network(next_states)
                next_q_values = tf.reduce_sum(next_q_values * next_action_indices, axis=1)
            else:
                # DQN estándar: target Q-network para seleccionar y evaluar
                next_q_values = tf.reduce_max(self.target_q_network(next_states), axis=1)
                
            # Calcular targets usando ecuación de Bellman
            targets = rewards + (1 - dones) * self.gamma * next_q_values
            
            # Calcular TD-error
            td_errors = targets - q_values_for_actions
            
            # Aplicar pesos de importancia para PER si es necesario
            if importance_weights is not None:
                # Usar MSE ponderada para PER
                loss = tf.reduce_mean(importance_weights * tf.square(td_errors), axis=0)
            else:
                # Función Huber para DQN estándar
                loss = self.loss_fn(targets, q_values_for_actions)
        
        # Calcular gradientes y actualizar pesos
        grads = tape.gradient(loss, self.q_network.trainable_variables)
        # Clipeo de gradientes opcional para estabilidad
        if DQN_CONFIG.get('grad_clip', False):
            grads, _ = tf.clip_by_global_norm(grads, DQN_CONFIG.get('max_grad_norm', 10.0))
        self.optimizer.apply_gradients(zip(grads, self.q_network.trainable_variables))
        
        # Actualizar métricas acumuladas
        self.loss_sum += loss
        self.q_value_sum += tf.reduce_mean(q_values_for_actions, axis=0)
        self.updates_count += 1
        
        return loss, td_errors
    
    def _sample_batch(self) -> Tuple:
        """
        Muestrea un lote del buffer de experiencias.
        
        Retorna:
        --------
        Tuple
            Datos muestreados, según el tipo de buffer
        """
        if self.prioritized:
            (states, actions, rewards, next_states, dones, 
             indices, importance_weights) = self.replay_buffer.sample(
                 self.batch_size, self.beta)
            self.beta = min(1.0, self.beta + self.beta_increment)
            
            return (
                states, actions, rewards, next_states, dones,
                indices, importance_weights
            )
        else:
            states, actions, rewards, next_states, dones = self.replay_buffer.sample(
                self.batch_size)
            
            return states, actions, rewards, next_states, dones, None, None
    
    def _update_model(self, episode_loss: List[float], update_every: int, update_after: int) -> None:
        """
        Actualiza el modelo si es necesario.
        
        Parámetros:
        -----------
        episode_loss : List[float]
            Lista para almacenar pérdidas del episodio
        update_every : int
            Frecuencia de actualización
        update_after : int
            Pasos antes de empezar a actualizar la red
        """
        # Entrenar modelo si hay suficientes datos
        if (len(self.replay_buffer) > self.batch_size and 
            self.update_counter >= update_after and 
            self.update_counter % update_every == 0):
            
            # Muestrear batch
            if self.prioritized:
                states, actions, rewards, next_states, dones, indices, importance_weights = self._sample_batch()
            else:
                states, actions, rewards, next_states, dones, _, _ = self._sample_batch()
                importance_weights = None
            
            # Convertir a tensores para TensorFlow
            states = tf.convert_to_tensor(states, dtype=tf.float32)
            actions = tf.convert_to_tensor(actions, dtype=tf.int32)
            rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)
            next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)
            dones = tf.convert_to_tensor(dones, dtype=tf.float32)
            if importance_weights is not None:
                importance_weights = tf.convert_to_tensor(importance_weights, dtype=tf.float32)
            
            # Entrenar red
            loss, td_errors = self._train_step(
                states, actions, rewards, next_states, dones, importance_weights)
            episode_loss.append(float(loss.numpy()))
            
            # Actualizar prioridades si es PER
            if self.prioritized and indices is not None:
                priorities = np.abs(td_errors.numpy()) + 1e-6
                self.replay_buffer.update_priorities(indices, priorities)
                
        # Actualizar target network periódicamente
        if self.update_counter % self.target_update_freq == 0 and self.update_counter > 0:
            self.update_target_network()
        
        self.update_counter += 1
            
    def _run_episode(self, env: Any, max_steps: int, render: bool, 
                   update_every: int, update_after: int) -> Tuple[float, List[float]]:
        """
        Ejecuta un episodio completo de entrenamiento.
        
        Parámetros:
        -----------
        env : Any
            Entorno donde ejecutar el episodio
        max_steps : int
            Pasos máximos por episodio
        render : bool
            Si se debe renderizar el entorno
        update_every : int
            Frecuencia de actualización del modelo
        update_after : int
            Pasos antes de empezar a actualizar la red
            
        Retorna:
        --------
        Tuple[float, List[float]]
            (episode_reward, episode_loss)
        """
        state, _ = env.reset()
        state = np.array(state, dtype=np.float32)
        episode_reward = 0.0
        episode_loss = []
        
        for _ in range(max_steps):
            # Seleccionar acción
            action = self.q_network.get_action(state, self.epsilon)
            
            # Ejecutar acción
            next_state, reward, done, _, _ = env.step(action)
            next_state = np.array(next_state, dtype=np.float32)
            
            # Renderizar si es necesario
            if render:
                env.render()
            
            # Guardar transición en buffer
            self.replay_buffer.add(state, action, reward, next_state, float(done))
            
            # Actualizar estado y recompensa
            state = next_state
            episode_reward += reward
            
            # Actualizar modelo
            self._update_model(episode_loss, update_every, update_after)
            
            if done:
                break
                
        return episode_reward, episode_loss
        
    def _update_history(self, history: Dict, episode_reward: float, episode_loss: List[float], 
                      episode_reward_history: List[float], log_interval: int) -> List[float]:
        """
        Actualiza el historial de entrenamiento y métricas.
        
        Parámetros:
        -----------
        history : Dict
            Historial de entrenamiento a actualizar
        episode_reward : float
            Recompensa del episodio
        episode_loss : List[float]
            Lista de pérdidas del episodio
        episode_reward_history : List[float]
            Historial reciente de recompensas
        log_interval : int
            Intervalo para mostrar información
            
        Retorna:
        --------
        List[float]
            Historial de recompensas actualizado
        """
        # Almacenar métricas
        history['episode_rewards'].append(episode_reward)
        history['epsilons'].append(self.epsilon)
        
        if episode_loss:
            history['losses'].append(float(np.mean(episode_loss)))
        else:
            history['losses'].append(0.0)
            
        # Calcular y guardar promedio del valor Q
        if self.updates_count > 0:
            avg_q_value = self.q_value_sum / self.updates_count
            avg_q_value = float(avg_q_value.numpy())
            self.q_value_sum = 0.0
            self.updates_count = 0
        else:
            avg_q_value = 0.0
            
        history['avg_q_values'].append(avg_q_value)
        
        # Actualizar epsilon (decaimiento)
        self.epsilon = max(
            self.epsilon_end, 
            self.epsilon * self.epsilon_decay
        )
        
        # Guardar últimas recompensas para promedio
        episode_reward_history.append(episode_reward)
        if len(episode_reward_history) > log_interval:
            episode_reward_history.pop(0)
            
        return episode_reward_history
        
    def train(self, env: Any, episodes: int = 1000, max_steps: int = 1000, 
             update_after: int = 1000, update_every: int = 4, 
             render: bool = False, log_interval: int = 10) -> Dict:
        """
        Entrena el agente DQN en un entorno dado.
        
        Parámetros:
        -----------
        env : Any
            Entorno de Gym o compatible
        episodes : int, opcional
            Número máximo de episodios (default: 1000)
        max_steps : int, opcional
            Pasos máximos por episodio (default: 1000)
        update_after : int, opcional
            Pasos antes de empezar a actualizar la red (default: 1000)
        update_every : int, opcional
            Frecuencia de actualización (default: 4)
        render : bool, opcional
            Mostrar entorno gráficamente (default: False)
        log_interval : int, opcional
            Intervalo para mostrar información (default: 10)
            
        Retorna:
        --------
        Dict
            Historia de entrenamiento
        """
        history = {
            'episode_rewards': [],
            'losses': [],
            'epsilons': [],
            'avg_q_values': []
        }
        
        # Variables para seguimiento de progreso
        best_reward = -float('inf')
        episode_reward_history = []
        
        for episode in range(episodes):
            # Ejecutar un episodio
            episode_reward, episode_loss = self._run_episode(
                env, max_steps, render, update_every, update_after)
            
            # Actualizar historial
            episode_reward_history = self._update_history(
                history, episode_reward, episode_loss, episode_reward_history, log_interval)
            
            # Mostrar progreso periódicamente
            if (episode + 1) % log_interval == 0:
                avg_reward = np.mean(episode_reward_history)
                print(f"Episodio {episode+1}/{episodes} - Recompensa Promedio: {avg_reward:.2f}, "
                      f"Epsilon: {self.epsilon:.4f}, Pérdida: {history['losses'][-1]:.4f}")
                
                # Guardar mejor modelo
                if avg_reward > best_reward:
                    best_reward = avg_reward
                    print(f"Nuevo mejor modelo con recompensa: {best_reward:.2f}")
        
        return history
    
    def evaluate(self, env: Any, episodes: int = 10, render: bool = False) -> float:
        """
        Evalúa el agente DQN entrenado en un entorno.
        
        Parámetros:
        -----------
        env : Any
            Entorno de Gym o compatible
        episodes : int, opcional
            Número de episodios para evaluar (default: 10)
        render : bool, opcional
            Mostrar entorno gráficamente (default: False)
            
        Retorna:
        --------
        float
            Recompensa promedio durante la evaluación
        """
        rewards = []
        
        for episode in range(episodes):
            state, _ = env.reset()
            state = np.array(state, dtype=np.float32)
            episode_reward = 0
            done = False
            step = 0
            
            while not done:
                # Seleccionar acción determinística (sin exploración)
                action = self.q_network.get_action(state)
                
                # Ejecutar acción
                next_state, reward, done, _, _ = env.step(action)
                next_state = np.array(next_state, dtype=np.float32)
                
                # Renderizar si es necesario
                if render:
                    env.render()
                
                # Actualizar estado y recompensa
                state = next_state
                episode_reward += reward
                step += 1
            
            rewards.append(episode_reward)
            print(f"Episodio de evaluación {episode+1}/{episodes} - "
                  f"Recompensa: {episode_reward:.2f}, Pasos: {step}")
        
        avg_reward = np.mean(rewards)
        print(f"Recompensa promedio de evaluación: {avg_reward:.2f}")
        
        return avg_reward
    
    def save_model(self, filepath: str) -> None:
        """
        Guarda los parámetros del modelo en un archivo.
        
        Parámetros:
        -----------
        filepath : str
            Ruta donde guardar el modelo
        """
        self.q_network.save_weights(filepath)
        print(f"Modelo guardado en {filepath}")
    
    def load_model(self, filepath: str) -> None:
        """
        Carga los parámetros del modelo desde un archivo.
        
        Parámetros:
        -----------
        filepath : str
            Ruta desde donde cargar el modelo
        """
        # Asegurarse de que el modelo está construido primero
        dummy_state = np.zeros((1, self.state_dim), dtype=np.float32)
        _ = self.q_network(dummy_state)
        
        self.q_network.load_weights(filepath)
        self.update_target_network()  # Sincronizar target network
        print(f"Modelo cargado desde {filepath}")
    
    def visualize_training(self, history: Dict, window_size: int = 10) -> None:
        """
        Visualiza los resultados del entrenamiento.
        
        Parámetros:
        -----------
        history : Dict
            Historia de entrenamiento
        window_size : int, opcional
            Tamaño de ventana para suavizado (default: 10)
        """
        # Función para aplicar suavizado
        def smooth(data, window_size):
            kernel = np.ones(window_size) / window_size
            return np.convolve(data, kernel, mode='valid')
        
        # Crear figura con múltiples subplots
        _, axs = plt.subplots(2, 2, figsize=(15, 10))
        
        # Graficar recompensas
        rewards = history['episode_rewards']
        axs[0, 0].plot(rewards, alpha=0.3, color='blue', label='Raw')
        if len(rewards) > window_size:
            axs[0, 0].plot(range(window_size-1, len(rewards)), smooth(rewards, window_size), 
                         color='blue', label=f'Suavizado (ventana={window_size})')
        axs[0, 0].set_title('Recompensas por Episodio')
        axs[0, 0].set_xlabel('Episodio')
        axs[0, 0].set_ylabel('Recompensa')
        axs[0, 0].legend()
        axs[0, 0].grid(alpha=0.3)
        
        # Graficar epsilon
        epsilons = history['epsilons']
        axs[0, 1].plot(epsilons, color='green')
        axs[0, 1].set_title('Valor Epsilon')
        axs[0, 1].set_xlabel('Episodio')
        axs[0, 1].set_ylabel('Epsilon')
        axs[0, 1].grid(alpha=0.3)
        
        # Graficar pérdida
        losses = history['losses']
        axs[1, 0].plot(losses, alpha=0.3, color='red', label='Raw')
        if len(losses) > window_size:
            axs[1, 0].plot(range(window_size-1, len(losses)), smooth(losses, window_size), 
                         color='red', label=f'Suavizado (ventana={window_size})')
        axs[1, 0].set_title('Pérdida')
        axs[1, 0].set_xlabel('Episodio')
        axs[1, 0].set_ylabel('Pérdida')
        axs[1, 0].legend()
        axs[1, 0].grid(alpha=0.3)
        
        # Graficar valores Q promedio
        q_values = history['avg_q_values']
        axs[1, 1].plot(q_values, alpha=0.3, color='purple', label='Raw')
        if len(q_values) > window_size:
            axs[1, 1].plot(range(window_size-1, len(q_values)), smooth(q_values, window_size), 
                         color='purple', label=f'Suavizado (ventana={window_size})')
        axs[1, 1].set_title('Valores Q Promedio')
        axs[1, 1].set_xlabel('Episodio')
        axs[1, 1].set_ylabel('Valor Q')
        axs[1, 1].legend()
        axs[1, 1].grid(alpha=0.3)
        
        plt.tight_layout()
        plt.show()