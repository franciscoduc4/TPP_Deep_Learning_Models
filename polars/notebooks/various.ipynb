{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos Varios\n",
    "\n",
    "En este notebook están los modelos:\n",
    "\n",
    "+ CNN (Convolutional Neural Network)\n",
    "+ Transformer\n",
    "+ TCN (Temporal Convolutional Network)\n",
    "+ GRU (Gated Recurrent Unit)\n",
    "+ Wavenet\n",
    "+ Tanmet\n",
    "+ Attention-Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (25.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: polars in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.25.2)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.10.1)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.4.2)\n",
      "Requirement already satisfied: openpyxl in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.1.5)\n",
      "Requirement already satisfied: fastexcel in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.13.0)\n",
      "Requirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.16.2)\n",
      "Requirement already satisfied: tensorflow.keras in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/tomas/Library/Python/3.12/lib/python/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: et-xmlfile in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fastexcel) (19.0.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (4.25.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (75.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/tomas/Library/Python/3.12/lib/python/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-macos (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-macos\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install --upgrade pip\n",
    "%pip install polars numpy scikit-learn matplotlib joblib openpyxl fastexcel tensorflow tensorflow.keras\n",
    "\n",
    "# For TensorFlow on Mac, you need to install tensorflow-macos\n",
    "%pip install tensorflow-macos tensorflow-metal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 02:15:56.135851: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, Input, Concatenate, BatchNormalization,\n",
    "    Conv1D, MaxPooling1D, LayerNormalization, MultiHeadAttention,\n",
    "    Add, GlobalAveragePooling1D, GRU, Activation, SimpleRNN, Bidirectional\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from datetime import timedelta\n",
    "import openpyxl\n",
    "\n",
    "# Configuración de Matplotlib para evitar errores con Tkinter\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sujetos: 54\n"
     ]
    }
   ],
   "source": [
    "# Definición de la ruta del proyecto\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "SUBJECTS_RELATIVE_PATH = \"data/Subjects\"\n",
    "SUBJECTS_PATH = os.path.join(PROJECT_ROOT, SUBJECTS_RELATIVE_PATH)\n",
    "\n",
    "# Crear directorios para resultados\n",
    "FIGURES_DIR = os.path.join(PROJECT_ROOT, \"figures\", \"various_models\")\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "MODELS_DIR = os.path.join(PROJECT_ROOT, \"models\")\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "subject_files = [f for f in os.listdir(SUBJECTS_PATH) if f.startswith(\"Subject\") and f.endswith(\".xlsx\")]\n",
    "print(f\"Total sujetos: {len(subject_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento y Procesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Subject17.xlsx (3/54)...\n",
      "Procesando Subject37.xlsx (2/54)...\n",
      "Procesando Subject21.xlsx (1/54)...\n",
      "Procesando Subject7.xlsx (6/54)...\n",
      "Procesando Subject40.xlsx (4/54)...\n",
      "Procesando Subject6.xlsx (5/54)...\n",
      "Procesando Subject41.xlsx (7/54)...\n",
      "Procesando Subject16.xlsx (8/54)...\n",
      "Procesando Subject36.xlsx (9/54)...\n",
      "Procesando Subject20.xlsx (10/54)...\n",
      "Procesando Subject11.xlsx (11/54)...\n",
      "Procesando Subject46.xlsx (12/54)...\n",
      "Procesando Subject50.xlsx (13/54)...\n",
      "Procesando Subject27.xlsx (14/54)...\n",
      "Procesando Subject31.xlsx (15/54)...\n",
      "Procesando Subject30.xlsx (16/54)...\n",
      "Procesando Subject26.xlsx (17/54)...\n",
      "Procesando Subject1.xlsx (18/54)...\n",
      "Procesando Subject51.xlsx (19/54)...\n",
      "Procesando Subject47.xlsx (20/54)...\n",
      "Procesando Subject10.xlsx (21/54)...\n",
      "Procesando Subject29.xlsx (22/54)...\n",
      "Procesando Subject2.xlsx (23/54)...\n",
      "Procesando Subject52.xlsx (24/54)...\n",
      "Procesando Subject44.xlsx (25/54)...\n",
      "Procesando Subject13.xlsx (26/54)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not determine dtype for column 5, falling back to string\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Subject33.xlsx (27/54)...\n",
      "Procesando Subject25.xlsx (28/54)...\n",
      "Procesando Subject48.xlsx (29/54)...\n",
      "Procesando Subject49.xlsx (30/54)...\n",
      "Procesando Subject24.xlsx (31/54)...\n",
      "Procesando Subject32.xlsx (32/54)...\n",
      "Procesando Subject12.xlsx (33/54)...\n",
      "Procesando Subject45.xlsx (34/54)...\n",
      "Procesando Subject53.xlsx (35/54)...\n",
      "Procesando Subject3.xlsx (36/54)...\n",
      "Procesando Subject28.xlsx (37/54)...\n",
      "Procesando Subject35.xlsx (38/54)...\n",
      "Procesando Subject23.xlsx (39/54)...\n",
      "Procesando Subject8.xlsx (40/54)...\n",
      "Procesando Subject19.xlsx (41/54)...\n",
      "Procesando Subject39.xlsx (42/54)...\n",
      "Procesando Subject4.xlsx (43/54)...\n",
      "Procesando Subject54.xlsx (44/54)...\n",
      "Procesando Subject42.xlsx (45/54)...\n",
      "Procesando Subject15.xlsx (46/54)...\n",
      "Procesando Subject14.xlsx (47/54)...\n",
      "Procesando Subject43.xlsx (48/54)...\n",
      "Procesando Subject5.xlsx (49/54)...\n",
      "Procesando Subject38.xlsx (50/54)...\n",
      "Procesando Subject18.xlsx (51/54)...\n",
      "Procesando Subject9.xlsx (52/54)...\n",
      "Procesando Subject22.xlsx (53/54)...\n",
      "Procesando Subject34.xlsx (54/54)...\n",
      "Muestra de datos procesados combinados:\n",
      "shape: (5, 9)\n",
      "┌────────────┬────────────┬───────────┬─────────┬───┬────────────┬────────────┬───────────┬────────┐\n",
      "│ subject_id ┆ cgm_window ┆ carbInput ┆ bgInput ┆ … ┆ insulinSen ┆ insulinOnB ┆ hour_of_d ┆ normal │\n",
      "│ ---        ┆ ---        ┆ ---       ┆ ---     ┆   ┆ sitivityFa ┆ oard       ┆ ay        ┆ ---    │\n",
      "│ i64        ┆ object     ┆ i64       ┆ i64     ┆   ┆ ctor       ┆ ---        ┆ ---       ┆ f64    │\n",
      "│            ┆            ┆           ┆         ┆   ┆ ---        ┆ f64        ┆ f64       ┆        │\n",
      "│            ┆            ┆           ┆         ┆   ┆ f64        ┆            ┆           ┆        │\n",
      "╞════════════╪════════════╪═══════════╪═════════╪═══╪════════════╪════════════╪═══════════╪════════╡\n",
      "│ 0          ┆ [112 111   ┆ 0         ┆ 167     ┆ … ┆ 54.119548  ┆ 0.0        ┆ 0.565217  ┆ 1.238  │\n",
      "│            ┆ 110 112    ┆           ┆         ┆   ┆            ┆            ┆           ┆        │\n",
      "│            ┆ 106 104    ┆           ┆         ┆   ┆            ┆            ┆           ┆        │\n",
      "│            ┆ 109 1…     ┆           ┆         ┆   ┆            ┆            ┆           ┆        │\n",
      "│ 0          ┆ [106 114   ┆ 0         ┆ 303     ┆ … ┆ 100.744417 ┆ 0.0        ┆ 0.608696  ┆ 2.015  │\n",
      "│            ┆ 128 124    ┆           ┆         ┆   ┆            ┆            ┆           ┆        │\n",
      "│            ┆ 118 110    ┆           ┆         ┆   ┆            ┆            ┆           ┆        │\n",
      "│            ┆ 112 1…     ┆           ┆         ┆   ┆            ┆            ┆           ┆        │\n",
      "│ 0          ┆ [281 300   ┆ 0         ┆ 320     ┆ … ┆ 135.384615 ┆ 0.0        ┆ 0.652174  ┆ 1.625  │\n",
      "│            ┆ 234 237    ┆           ┆         ┆   ┆            ┆            ┆           ┆        │\n",
      "│            ┆ 256 264    ┆           ┆         ┆   ┆            ┆            ┆           ┆        │\n",
      "│            ┆ 272 2…     ┆           ┆         ┆   ┆            ┆            ┆           ┆        │\n",
      "│ 0          ┆ [312 329   ┆ 0         ┆ 217     ┆ … ┆ 218.283582 ┆ 0.0        ┆ 0.695652  ┆ 0.536  │\n",
      "│            ┆ 336 343    ┆           ┆         ┆   ┆            ┆            ┆           ┆        │\n",
      "│            ┆ 342 326    ┆           ┆         ┆   ┆            ┆            ┆           ┆        │\n",
      "│            ┆ 320 3…     ┆           ┆         ┆   ┆            ┆            ┆           ┆        │\n",
      "│ 0          ┆ [118 115   ┆ 0         ┆ 161     ┆ … ┆ 51.260504  ┆ 0.0        ┆ 0.869565  ┆ 1.19   │\n",
      "│            ┆ 113 114    ┆           ┆         ┆   ┆            ┆            ┆           ┆        │\n",
      "│            ┆ 114 114    ┆           ┆         ┆   ┆            ┆            ┆           ┆        │\n",
      "│            ┆ 120 1…     ┆           ┆         ┆   ┆            ┆            ┆           ┆        │\n",
      "└────────────┴────────────┴───────────┴─────────┴───┴────────────┴────────────┴───────────┴────────┘\n",
      "Total muestras: 44651\n"
     ]
    }
   ],
   "source": [
    "def get_cgm_window(bolus_time, cgm_df: pl.DataFrame, window_hours: int = 2) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Obtiene la ventana de datos CGM para un tiempo de bolo específico.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    bolus_time : datetime\n",
    "        Tiempo del bolo de insulina\n",
    "    cgm_df : pl.DataFrame\n",
    "        DataFrame con datos CGM\n",
    "    window_hours : int, opcional\n",
    "        Horas de la ventana de datos (default: 2)\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    np.ndarray\n",
    "        Ventana de datos CGM o None si no hay suficientes datos\n",
    "    \"\"\"\n",
    "    window_start = bolus_time - timedelta(hours=window_hours)\n",
    "    window = cgm_df.filter(\n",
    "        (pl.col(\"date\") >= window_start) & (pl.col(\"date\") <= bolus_time)\n",
    "    ).sort(\"date\").tail(24)\n",
    "    \n",
    "    if window.height < 24:\n",
    "        return None\n",
    "    return window.get_column(\"mg/dl\").to_numpy()\n",
    "\n",
    "def calculate_iob(bolus_time, basal_df: pl.DataFrame, half_life_hours: float = 4.0) -> float:\n",
    "    \"\"\"\n",
    "    Calcula la insulina activa en el cuerpo (IOB).\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    bolus_time : datetime\n",
    "        Tiempo del bolo de insulina\n",
    "    basal_df : pl.DataFrame\n",
    "        DataFrame con datos de insulina basal\n",
    "    half_life_hours : float, opcional\n",
    "        Vida media de la insulina en horas (default: 4.0)\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    float\n",
    "        Cantidad de insulina activa\n",
    "    \"\"\"\n",
    "    if basal_df is None or basal_df.is_empty():\n",
    "        return 0.0\n",
    "    \n",
    "    iob = 0.0\n",
    "    for row in basal_df.iter_rows(named=True):\n",
    "        start_time = row[\"date\"]\n",
    "        duration_hours = row[\"duration\"] / (1000 * 3600)\n",
    "        end_time = start_time + timedelta(hours=duration_hours)\n",
    "        rate = row[\"rate\"] if row[\"rate\"] is not None else 0.9\n",
    "        \n",
    "        if start_time <= bolus_time <= end_time:\n",
    "            time_since_start = (bolus_time - start_time).total_seconds() / 3600\n",
    "            remaining = rate * (1 - (time_since_start / half_life_hours))\n",
    "            iob += max(0.0, remaining)\n",
    "    return iob\n",
    "\n",
    "def process_subject(subject_path: str, idx: int) -> list:\n",
    "    \"\"\"\n",
    "    Procesa los datos de un sujeto.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    subject_path : str\n",
    "        Ruta al archivo del sujeto\n",
    "    idx : int\n",
    "        Índice del sujeto\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    list\n",
    "        Lista de diccionarios con características procesadas\n",
    "    \"\"\"\n",
    "    print(f\"Procesando {os.path.basename(subject_path)} ({idx+1}/{len(subject_files)})...\")\n",
    "    \n",
    "    try:\n",
    "        cgm_df = pl.read_excel(subject_path, sheet_name=\"CGM\")\n",
    "        bolus_df = pl.read_excel(subject_path, sheet_name=\"Bolus\")\n",
    "        try:\n",
    "            basal_df = pl.read_excel(subject_path, sheet_name=\"Basal\")\n",
    "        except Exception:\n",
    "            basal_df = None\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar {os.path.basename(subject_path)}: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Conversión de fechas\n",
    "    cgm_df = cgm_df.with_columns(pl.col(\"date\").cast(pl.Datetime))\n",
    "    bolus_df = bolus_df.with_columns(pl.col(\"date\").cast(pl.Datetime))\n",
    "    if basal_df is not None:\n",
    "        basal_df = basal_df.with_columns(pl.col(\"date\").cast(pl.Datetime))\n",
    "    \n",
    "    cgm_df = cgm_df.sort(\"date\")\n",
    "\n",
    "    processed_data = []\n",
    "    for row in bolus_df.iter_rows(named=True):\n",
    "        bolus_time = row[\"date\"]\n",
    "        cgm_window = get_cgm_window(bolus_time, cgm_df)\n",
    "        \n",
    "        if cgm_window is not None:\n",
    "            iob = calculate_iob(bolus_time, basal_df)\n",
    "            hour_of_day = bolus_time.hour / 23.0\n",
    "            bg_input = row[\"bgInput\"] if row[\"bgInput\"] is not None else cgm_window[-1]\n",
    "            normal = row[\"normal\"] if row[\"normal\"] is not None else 0.0\n",
    "            \n",
    "            # Cálculo del factor de sensibilidad personalizado\n",
    "            isf_custom = 50.0\n",
    "            if normal > 0 and bg_input > 100:\n",
    "                isf_custom = (bg_input - 100) / normal\n",
    "            \n",
    "            features = {\n",
    "                'subject_id': idx,\n",
    "                'cgm_window': cgm_window,\n",
    "                'carbInput': row[\"carbInput\"] if row[\"carbInput\"] is not None else 0.0,\n",
    "                'bgInput': bg_input,\n",
    "                'insulinCarbRatio': row[\"insulinCarbRatio\"] if row[\"insulinCarbRatio\"] is not None else 10.0,\n",
    "                'insulinSensitivityFactor': isf_custom,\n",
    "                'insulinOnBoard': iob,\n",
    "                'hour_of_day': hour_of_day,\n",
    "                'normal': normal\n",
    "            }\n",
    "            processed_data.append(features)\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# Ejecución en paralelo\n",
    "all_processed_data = Parallel(n_jobs=-1)(\n",
    "    delayed(process_subject)(\n",
    "        os.path.join(SUBJECTS_PATH, f), \n",
    "        idx\n",
    "    ) for idx, f in enumerate(subject_files)\n",
    ")\n",
    "\n",
    "all_processed_data = [item for sublist in all_processed_data for item in sublist]\n",
    "\n",
    "# Conversión a DataFrame\n",
    "df_processed = pl.DataFrame(all_processed_data)\n",
    "print(\"Muestra de datos procesados combinados:\")\n",
    "print(df_processed.head())\n",
    "print(f\"Total muestras: {len(df_processed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División de Ventana CGM y Valores Nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificación de valores nulos en df_processed:\n",
      "shape: (1, 32)\n",
      "┌───────┬───────┬───────┬───────┬───┬──────────────────────┬────────────────┬─────────────┬────────┐\n",
      "│ cgm_0 ┆ cgm_1 ┆ cgm_2 ┆ cgm_3 ┆ … ┆ insulinSensitivityFa ┆ insulinOnBoard ┆ hour_of_day ┆ normal │\n",
      "│ ---   ┆ ---   ┆ ---   ┆ ---   ┆   ┆ ctor                 ┆ ---            ┆ ---         ┆ ---    │\n",
      "│ u32   ┆ u32   ┆ u32   ┆ u32   ┆   ┆ ---                  ┆ u32            ┆ u32         ┆ u32    │\n",
      "│       ┆       ┆       ┆       ┆   ┆ u32                  ┆                ┆             ┆        │\n",
      "╞═══════╪═══════╪═══════╪═══════╪═══╪══════════════════════╪════════════════╪═════════════╪════════╡\n",
      "│ 0     ┆ 0     ┆ 0     ┆ 0     ┆ … ┆ 0                    ┆ 0              ┆ 0           ┆ 0      │\n",
      "└───────┴───────┴───────┴───────┴───┴──────────────────────┴────────────────┴─────────────┴────────┘\n"
     ]
    }
   ],
   "source": [
    "# Dividir ventana CGM y otras características\n",
    "cgm_columns = [f'cgm_{i}' for i in range(24)]\n",
    "df_cgm = pl.DataFrame({\n",
    "    col: [row['cgm_window'][i] for row in all_processed_data]\n",
    "    for i, col in enumerate(cgm_columns)\n",
    "}, schema={col: pl.Float64 for col in cgm_columns})\n",
    "\n",
    "# Combinar con otras características\n",
    "df_processed = pl.concat([\n",
    "    df_cgm,\n",
    "    df_processed.drop('cgm_window')\n",
    "], how=\"horizontal\")\n",
    "\n",
    "# Verificar valores nulos\n",
    "print(\"Verificación de valores nulos en df_processed:\")\n",
    "print(df_processed.null_count())\n",
    "df_processed = df_processed.drop_nulls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN en X_cgm: 0\n",
      "NaN en X_other: 0\n",
      "NaN en y: 0\n"
     ]
    }
   ],
   "source": [
    "# Normalizar características\n",
    "scaler_cgm = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_other = StandardScaler()\n",
    "\n",
    "# Normalizar CGM\n",
    "X_cgm = scaler_cgm.fit_transform(df_processed.select(cgm_columns).to_numpy())\n",
    "X_cgm = X_cgm.reshape(X_cgm.shape[0], X_cgm.shape[1], 1)\n",
    "\n",
    "# Normalizar otras características (incluyendo hour_of_day)\n",
    "other_features = ['carbInput', 'bgInput', 'insulinOnBoard', 'insulinCarbRatio', \n",
    "                  'insulinSensitivityFactor', 'subject_id', 'hour_of_day']\n",
    "X_other = scaler_other.fit_transform(df_processed.select(other_features).to_numpy())\n",
    "\n",
    "# Etiquetas\n",
    "y = df_processed.get_column('normal').to_numpy()\n",
    "\n",
    "# Verificar NaN\n",
    "print(\"NaN en X_cgm:\", np.isnan(X_cgm).sum())\n",
    "print(\"NaN en X_other:\", np.isnan(X_other).sum())\n",
    "print(\"NaN en y:\", np.isnan(y).sum())\n",
    "if np.isnan(X_cgm).sum() > 0 or np.isnan(X_other).sum() > 0 or np.isnan(y).sum() > 0:\n",
    "    raise ValueError(\"Valores NaN detectados en X_cgm, X_other o y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División por Sujeto de los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# División por sujeto\n",
    "subject_ids = df_processed.get_column('subject_id').unique().to_numpy()\n",
    "train_subjects, temp_subjects = train_test_split(subject_ids, test_size=0.2, random_state=42)\n",
    "val_subjects, test_subjects = train_test_split(temp_subjects, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de Máscaras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento CGM: (33272, 24, 1), Validación CGM: (2743, 24, 1), Prueba CGM: (8636, 24, 1)\n",
      "Entrenamiento Otros: (33272, 7), Validación Otros: (2743, 7), Prueba Otros: (8636, 7)\n",
      "Sujetos de prueba: [ 5 19 32 13 48 49]\n"
     ]
    }
   ],
   "source": [
    "# Crear máscaras\n",
    "train_mask = df_processed.get_column('subject_id').is_in(train_subjects).to_numpy()\n",
    "val_mask = df_processed.get_column('subject_id').is_in(val_subjects).to_numpy()\n",
    "test_mask = df_processed.get_column('subject_id').is_in(test_subjects).to_numpy()\n",
    "\n",
    "X_cgm_train, X_cgm_val, X_cgm_test = X_cgm[train_mask], X_cgm[val_mask], X_cgm[test_mask]\n",
    "X_other_train, X_other_val, X_other_test = X_other[train_mask], X_other[val_mask], X_other[test_mask]\n",
    "y_train, y_val, y_test = y[train_mask], y[val_mask], y[test_mask]\n",
    "subject_test = df_processed.filter(pl.col('subject_id').is_in(test_subjects)).get_column('subject_id').to_numpy()\n",
    "\n",
    "print(f\"Entrenamiento CGM: {X_cgm_train.shape}, Validación CGM: {X_cgm_val.shape}, Prueba CGM: {X_cgm_test.shape}\")\n",
    "print(f\"Entrenamiento Otros: {X_other_train.shape}, Validación Otros: {X_other_val.shape}, Prueba Otros: {X_other_test.shape}\")\n",
    "print(f\"Sujetos de prueba: {test_subjects}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TCN_CONFIG = {\n",
    "    'filters': 64,\n",
    "    'kernel_size': 3,\n",
    "    'dilations': [2**i for i in range(4)],\n",
    "    'dropout_rate': [0.3, 0.2],\n",
    "    'epsilon': 1e-6\n",
    "}\n",
    "\n",
    "TRANSFORMER_CONFIG = {\n",
    "    'num_heads': 4,\n",
    "    'key_dim': 32,\n",
    "    'ff_dim': 128,\n",
    "    'dropout_rate': 0.2,\n",
    "    'epsilon': 1e-6\n",
    "}\n",
    "\n",
    "WAVENET_CONFIG = {\n",
    "    'filters': [32, 64, 128],\n",
    "    'kernel_size': 2,\n",
    "    'dilations': [2**i for i in range(8)],  # [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "    'dropout_rate': 0.2\n",
    "}\n",
    "\n",
    "TABNET_CONFIG = {\n",
    "    'feature_dim': 64,\n",
    "    'output_dim': 32,\n",
    "    'num_decision_steps': 5,\n",
    "    'relaxation_factor': 1.5,\n",
    "    'sparsity_coefficient': 1e-5,\n",
    "    'batch_momentum': 0.98\n",
    "}\n",
    "\n",
    "ATTENTION_CONFIG = {\n",
    "    'num_heads': 8,\n",
    "    'key_dim': 64,\n",
    "    'num_layers': 4,\n",
    "    'ff_dim': 256,\n",
    "    'dropout_rate': 0.1\n",
    "}\n",
    "\n",
    "GRU_CONFIG = {\n",
    "    'hidden_units': [128, 64],\n",
    "    'dropout_rate': 0.2\n",
    "}\n",
    "\n",
    "CNN_CONFIG = {\n",
    "    'filters': [32, 64, 128, 256],\n",
    "    'kernel_size': 3,\n",
    "    'pool_size': 2,\n",
    "    'dropout_rate': 0.2\n",
    "}\n",
    "\n",
    "RNN_CONFIG = {\n",
    "    'hidden_units': [128, 64, 32],\n",
    "    'dropout_rate': 0.3,\n",
    "    'recurrent_dropout': 0.2,\n",
    "    'bidirectional': True,\n",
    "    'epsilon': 1e-6\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention-Only Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attention_block(x: tf.Tensor, num_heads: int, key_dim: int, ff_dim: int, dropout_rate: float) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Crea un bloque de atención con feed-forward network.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    x : tf.Tensor\n",
    "        Tensor de entrada\n",
    "    num_heads : int\n",
    "        Número de cabezas de atención\n",
    "    key_dim : int\n",
    "        Dimensión de la clave\n",
    "    ff_dim : int\n",
    "        Dimensión de la red feed-forward\n",
    "    dropout_rate : float\n",
    "        Tasa de dropout\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    tf.Tensor\n",
    "        Tensor de salida del bloque de atención\n",
    "    \"\"\"\n",
    "    # Multi-head attention\n",
    "    attention_output = MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=key_dim\n",
    "    )(x, x)\n",
    "    attention_output = Dropout(dropout_rate)(attention_output)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x + attention_output)\n",
    "    \n",
    "    # Feed-forward network\n",
    "    ffn = Dense(ff_dim, activation='relu')(x)\n",
    "    ffn = Dense(x.shape[-1])(ffn)\n",
    "    ffn = Dropout(dropout_rate)(ffn)\n",
    "    \n",
    "    return LayerNormalization(epsilon=1e-6)(x + ffn)\n",
    "\n",
    "def create_attention_model(cgm_shape: tuple, other_features_shape: tuple) -> Model:\n",
    "    \"\"\"\n",
    "    Crea un modelo basado únicamente en mecanismos de atención.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    cgm_shape : tuple\n",
    "        Forma de los datos CGM (samples, timesteps, features)\n",
    "    other_features_shape : tuple\n",
    "        Forma de otras características (samples, features)\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    Model\n",
    "        Modelo de atención compilado\n",
    "    \"\"\"\n",
    "    cgm_input = Input(shape=cgm_shape[1:])\n",
    "    other_input = Input(shape=(other_features_shape[1],))\n",
    "    \n",
    "    x = cgm_input\n",
    "    \n",
    "    # Stack attention blocks\n",
    "    for _ in range(ATTENTION_CONFIG['num_layers']):\n",
    "        x = create_attention_block(\n",
    "            x,\n",
    "            ATTENTION_CONFIG['num_heads'],\n",
    "            ATTENTION_CONFIG['key_dim'],\n",
    "            ATTENTION_CONFIG['ff_dim'],\n",
    "            ATTENTION_CONFIG['dropout_rate']\n",
    "        )\n",
    "    \n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Concatenate()([x, other_input])\n",
    "    \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(ATTENTION_CONFIG['dropout_rate'])(x)\n",
    "    \n",
    "    output = Dense(1)(x)\n",
    "    \n",
    "    return Model(inputs=[cgm_input, other_input], outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(cgm_shape: tuple, other_features_shape: tuple) -> Model:\n",
    "    \"\"\"\n",
    "    Crea un modelo CNN (Convolutional Neural Network) con entrada dual para datos CGM y otras características.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    cgm_shape : tuple\n",
    "        Forma de los datos CGM (samples, timesteps, features)\n",
    "    other_features_shape : tuple\n",
    "        Forma de otras características (samples, features)\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    Model\n",
    "        Modelo CNN compilado\n",
    "    \"\"\"\n",
    "    # Entrada CGM\n",
    "    cgm_input = Input(shape=cgm_shape[1:], name='cgm_input')\n",
    "    \n",
    "    # Capas CNN\n",
    "    conv = Conv1D(filters=CNN_CONFIG['filters'][1], kernel_size=CNN_CONFIG['kernel_size'], activation='relu')(cgm_input)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    \n",
    "    conv = Conv1D(filters=CNN_CONFIG['filters'][0], kernel_size=CNN_CONFIG['kernel_size'], activation='relu')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = GlobalAveragePooling1D()(conv)\n",
    "    \n",
    "    # Entrada de otras características\n",
    "    other_input = Input(shape=(other_features_shape[1],), name='other_input')\n",
    "    \n",
    "    # Combinar características\n",
    "    combined = Concatenate()([conv, other_input])\n",
    "    \n",
    "    # Capas densas\n",
    "    dense = Dense(64, activation='relu')(combined)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    dense = Dropout(CNN_CONFIG['dropout_rate'])(dense)\n",
    "    \n",
    "    output = Dense(1, activation='linear')(dense)\n",
    "    \n",
    "    return Model(inputs=[cgm_input, other_input], outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gru_model(cgm_shape: tuple, other_features_shape: tuple) -> Model:\n",
    "    '''\n",
    "    Crea un modelo GRU (Gated Recurrent Unit) con entrada dual para datos CGM y otras características.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    cgm_shape : tuple\n",
    "        Forma de los datos CGM (samples, timesteps, features)\n",
    "    other_features_shape : tuple\n",
    "        Forma de otras características (samples, features)\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    Model\n",
    "        Modelo GRU compilado\n",
    "    '''\n",
    "    cgm_input = Input(shape=cgm_shape[1:])\n",
    "    other_input = Input(shape=(other_features_shape[1],))\n",
    "    \n",
    "    x = GRU(GRU_CONFIG['hidden_units'][0], return_sequences=True)(cgm_input)\n",
    "    x = GRU(GRU_CONFIG['hidden_units'][1])(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Concatenate()([x, other_input])\n",
    "    x = Dense(GRU_CONFIG['hidden_units'][1], activation='relu')(x)\n",
    "    x = Dropout(GRU_CONFIG['dropout_rate'])(x)\n",
    "    output = Dense(1)(x)\n",
    "    \n",
    "    return Model(inputs=[cgm_input, other_input], outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_model(cgm_shape: tuple, other_features_shape: tuple) -> Model:\n",
    "    \"\"\"\n",
    "    Crea un modelo RNN con capas bidireccionales y skip connections.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    cgm_shape : tuple\n",
    "        Forma de los datos CGM (samples, timesteps, features)\n",
    "    other_features_shape : tuple\n",
    "        Forma de otras características (samples, features)\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    Model\n",
    "        Modelo RNN compilado\n",
    "    \"\"\"\n",
    "    # Entradas\n",
    "    cgm_input = Input(shape=cgm_shape[1:])\n",
    "    other_input = Input(shape=(other_features_shape[1],))\n",
    "    \n",
    "    # Capas RNN\n",
    "    x = cgm_input\n",
    "    skip_connections = []\n",
    "    \n",
    "    for units in RNN_CONFIG['hidden_units']:\n",
    "        rnn_layer = SimpleRNN(\n",
    "            units,\n",
    "            dropout=RNN_CONFIG['dropout_rate'],\n",
    "            recurrent_dropout=RNN_CONFIG['recurrent_dropout'],\n",
    "            return_sequences=True\n",
    "        )\n",
    "        \n",
    "        if RNN_CONFIG['bidirectional']:\n",
    "            x = Bidirectional(rnn_layer)(x)\n",
    "        else:\n",
    "            x = rnn_layer(x)\n",
    "            \n",
    "        x = BatchNormalization(epsilon=RNN_CONFIG['epsilon'])(x)\n",
    "        skip_connections.append(x)\n",
    "    \n",
    "    # Último RNN sin return_sequences\n",
    "    final_rnn = SimpleRNN(\n",
    "        RNN_CONFIG['hidden_units'][-1],\n",
    "        dropout=RNN_CONFIG['dropout_rate'],\n",
    "        recurrent_dropout=RNN_CONFIG['recurrent_dropout']\n",
    "    )\n",
    "    \n",
    "    if RNN_CONFIG['bidirectional']:\n",
    "        x = Bidirectional(final_rnn)(x)\n",
    "    else:\n",
    "        x = final_rnn(x)\n",
    "    \n",
    "    # Combinar con otras características\n",
    "    x = Concatenate()([x, other_input])\n",
    "    \n",
    "    # Capas densas finales\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization(epsilon=RNN_CONFIG['epsilon'])(x)\n",
    "    x = Dropout(RNN_CONFIG['dropout_rate'])(x)\n",
    "    \n",
    "    output = Dense(1)(x)\n",
    "    \n",
    "    return Model(inputs=[cgm_input, other_input], outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLU(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Gated Linear Unit como capa personalizada.\n",
    "    \"\"\"\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.dense = Dense(units * 2)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense(inputs)\n",
    "        return x[:, :self.units] * tf.nn.sigmoid(x[:, self.units:])\n",
    "\n",
    "class FeatureTransformer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Transformador de características como capa personalizada.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim, batch_momentum=0.98, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.glu = GLU(feature_dim)\n",
    "        self.bn = BatchNormalization(momentum=batch_momentum)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.glu(inputs)\n",
    "        return self.bn(x)\n",
    "\n",
    "def custom_softmax(x: tf.Tensor, axis: int=-1) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Implementación de softmax con estabilidad numérica.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    x : tf.Tensor\n",
    "        Tensor de entrada\n",
    "    axis : int\n",
    "        Eje de normalización\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    tf.Tensor\n",
    "        Tensor normal\n",
    "    \"\"\"\n",
    "    exp_x = tf.exp(x - tf.reduce_max(x, axis=axis, keepdims=True))\n",
    "    return exp_x / tf.reduce_sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "def glu(x: tf.Tensor, n_units: int) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Gated Linear Unit.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    x : tf.Tensor\n",
    "        Tensor de entrada\n",
    "    n_units : int\n",
    "        Número de unidades\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    tf.Tensor\n",
    "        Tensor GLU\n",
    "    \"\"\"\n",
    "    return x[:, :n_units] * tf.nn.sigmoid(x[:, n_units:])\n",
    "\n",
    "def feature_transformer(x: tf.Tensor, feature_dim: int, batch_momentum: float=0.98) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Transformador de características.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    x : tf.Tensor\n",
    "        Tensor de entrada\n",
    "    feature_dim : int\n",
    "        Dimensión de las características\n",
    "    batch_momentum : float\n",
    "        Momento de la normalización por lotes\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    tf.Tensor\n",
    "        Tensor transform\n",
    "    \"\"\"\n",
    "    transform = Dense(feature_dim * 2)(x)\n",
    "    transform = glu(transform, feature_dim)\n",
    "    return BatchNormalization(momentum=batch_momentum)(transform)\n",
    "\n",
    "def create_tabnet_model(cgm_shape: tuple, other_features_shape: tuple) -> Model:\n",
    "    \"\"\"\n",
    "    Crea un modelo TabNet modificado para procesamiento de datos tabulares.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    cgm_shape : tuple\n",
    "        Forma de los datos CGM\n",
    "    other_features_shape : tuple\n",
    "        Forma de otras características\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    Model\n",
    "        Modelo TabNet compilado\n",
    "    \"\"\"\n",
    "    cgm_input = Input(shape=cgm_shape[1:])\n",
    "    other_input = Input(shape=(other_features_shape[1],))\n",
    "    \n",
    "    cgm_flat = tf.keras.layers.Flatten()(cgm_input)\n",
    "    x = Concatenate()([cgm_flat, other_input])\n",
    "    \n",
    "    for _ in range(TABNET_CONFIG['num_decision_steps']):\n",
    "        transformer = FeatureTransformer(\n",
    "            TABNET_CONFIG['feature_dim'],\n",
    "            TABNET_CONFIG['batch_momentum']\n",
    "        )\n",
    "        x = transformer(x)\n",
    "        \n",
    "        mask = Dense(x.shape[-1], activation='softmax')(x)\n",
    "        x = tf.keras.layers.Multiply()([x, mask])\n",
    "    \n",
    "    # Final layers\n",
    "    x = Dense(TABNET_CONFIG['output_dim'], activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    output = Dense(1)(x)\n",
    "    \n",
    "    return Model(inputs=[cgm_input, other_input], outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Convolutional Network (TCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalPadding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Capa personalizada para padding causal.\n",
    "    \"\"\"\n",
    "    def __init__(self, padding_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.padding_size = padding_size\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.pad(inputs, [[0, 0], [self.padding_size, 0], [0, 0]])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1] + self.padding_size, input_shape[2])\n",
    "\n",
    "def create_tcn_block(input_layer: tf.Tensor, filters: int, kernel_size: int, \n",
    "                    dilation_rate: int, dropout_rate: float) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Crea un bloque TCN (Temporal Convolutional Network).\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    input_layer : tf.Tensor\n",
    "        Capa de entrada\n",
    "    filters : int\n",
    "        Número de filtros\n",
    "    kernel_size : int\n",
    "        Tamaño del kernel\n",
    "    dilation_rate : int\n",
    "        Tasa de dilatación\n",
    "    dropout_rate : float\n",
    "        Tasa de dropout\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    tf.Tensor\n",
    "        Salida del bloque TCN\n",
    "    \"\"\"\n",
    "    # Padding causal para mantener causalidad temporal\n",
    "    padding_size = (kernel_size - 1) * dilation_rate\n",
    "    padded_input = CausalPadding(padding_size)(input_layer)\n",
    "    \n",
    "    # Convolución dilatada\n",
    "    conv = Conv1D(\n",
    "        filters=filters,\n",
    "        kernel_size=kernel_size,\n",
    "        dilation_rate=dilation_rate,\n",
    "        padding='valid',\n",
    "        activation='relu'\n",
    "    )(padded_input)\n",
    "    \n",
    "    # Normalización y regularización\n",
    "    conv = LayerNormalization(epsilon=TCN_CONFIG['epsilon'])(conv)\n",
    "    conv = Dropout(dropout_rate)(conv)\n",
    "    \n",
    "    # Conexión residual si las dimensiones coinciden\n",
    "    if input_layer.shape[-1] == filters:\n",
    "        cropped_input = input_layer[:, -conv.shape[1]:, :]\n",
    "        return Add()([conv, cropped_input])\n",
    "    return conv\n",
    "\n",
    "def create_tcn_model(input_shape: tuple, other_features_shape: tuple) -> Model:\n",
    "    \"\"\"\n",
    "    Crea un modelo TCN completo.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    input_shape : tuple\n",
    "        Forma de los datos CGM\n",
    "    other_features_shape : tuple\n",
    "        Forma de otras características\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    Model\n",
    "        Modelo TCN compilado\n",
    "    \"\"\"\n",
    "    # Entradas\n",
    "    cgm_input = Input(shape=input_shape[1:], name='cgm_input')\n",
    "    other_input = Input(shape=(other_features_shape[1],), name='other_input')\n",
    "    \n",
    "    # Bloques TCN\n",
    "    x = cgm_input\n",
    "    skip_connections = []\n",
    "    \n",
    "    for dilation_rate in TCN_CONFIG['dilations']:\n",
    "        tcn_out = create_tcn_block(\n",
    "            x,\n",
    "            filters=TCN_CONFIG['filters'],\n",
    "            kernel_size=TCN_CONFIG['kernel_size'],\n",
    "            dilation_rate=dilation_rate,\n",
    "            dropout_rate=TCN_CONFIG['dropout_rate'][0]  # Using first dropout rate for TCN blocks\n",
    "        )\n",
    "        skip_connections.append(tcn_out)\n",
    "        x = tcn_out\n",
    "    \n",
    "    # Combinar skip connections\n",
    "    if skip_connections:\n",
    "        target_len = skip_connections[-1].shape[1]\n",
    "        aligned_skips = [\n",
    "            skip[:, -target_len:, :] for skip in skip_connections\n",
    "        ]\n",
    "        x = Add()(aligned_skips)\n",
    "    \n",
    "    # Global pooling\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Combinar con otras características\n",
    "    x = tf.keras.layers.Concatenate()([x, other_input])\n",
    "    \n",
    "    # Capas densas finales\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(TCN_CONFIG['dropout_rate'][0])(x)  # First dropout rate\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(TCN_CONFIG['dropout_rate'][1])(x)  # Second dropout rate\n",
    "    \n",
    "    output = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    return Model(inputs=[cgm_input, other_input], outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformer_model(cgm_shape: tuple, other_features_shape: tuple) -> Model:\n",
    "    \"\"\"\n",
    "    Crea un modelo Transformer con entrada dual para datos CGM y otras características.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    cgm_shape : tuple\n",
    "        Forma de los datos CGM (samples, timesteps, features)\n",
    "    other_features_shape : tuple\n",
    "        Forma de otras características (samples, features)\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    Model\n",
    "        Modelo Transformer compilado\n",
    "    \"\"\"\n",
    "    # Entrada CGM\n",
    "    cgm_input = Input(shape=cgm_shape[1:], name='cgm_input')\n",
    "    \n",
    "    # Transformer block\n",
    "    attention = MultiHeadAttention(num_heads=TRANSFORMER_CONFIG['num_heads'], key_dim=TRANSFORMER_CONFIG['key_dim'])(cgm_input, cgm_input)\n",
    "    attention = LayerNormalization(epsilon=TRANSFORMER_CONFIG['epsilon'])(attention + cgm_input)\n",
    "    \n",
    "    # Feed-forward network\n",
    "    ff = Dense(128, activation='relu')(attention)\n",
    "    ff = Dense(cgm_shape[-1])(ff)\n",
    "    ff = LayerNormalization(epsilon=TRANSFORMER_CONFIG['epsilon'])(ff + attention)\n",
    "    \n",
    "    # Global pooling\n",
    "    pooled = GlobalAveragePooling1D()(ff)\n",
    "    \n",
    "    # Entrada de otras características\n",
    "    other_input = Input(shape=(other_features_shape[1],), name='other_input')\n",
    "    \n",
    "    # Combinar características\n",
    "    combined = Concatenate()([pooled, other_input])\n",
    "    \n",
    "    # Capas densas finales\n",
    "    dense = Dense(64, activation='relu')(combined)\n",
    "    dense = LayerNormalization(epsilon=TRANSFORMER_CONFIG['epsilon'])(dense)\n",
    "    dense = Dropout(TRANSFORMER_CONFIG['dropout_rate'])(dense)\n",
    "    \n",
    "    output = Dense(1, activation='linear')(dense)\n",
    "    \n",
    "    return Model(inputs=[cgm_input, other_input], outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wavenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNetBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Bloque WaveNet personalizado.\n",
    "    \"\"\"\n",
    "    def __init__(self, filters, kernel_size, dilation_rate, dropout_rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.conv = Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation_rate=dilation_rate,\n",
    "            padding='causal'\n",
    "        )\n",
    "        self.bn = BatchNormalization()\n",
    "        self.activation = Activation('relu')\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.add = Add()\n",
    "        self.residual_proj = Conv1D(filters, 1, padding='same')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        x = self.bn(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Ensure residual has same number of filters\n",
    "        residual = self.residual_proj(inputs)\n",
    "        \n",
    "        # Match temporal dimension\n",
    "        residual = residual[:, -x.shape[1]:, :]\n",
    "        return self.add([x, residual])\n",
    "\n",
    "def create_wavenet_block(x, filters, kernel_size, dilation_rate, dropout_rate):\n",
    "    \"\"\"\n",
    "    Crea un bloque WaveNet con conexiones residuales y skip connections.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    x : tf.Tensor\n",
    "        Tensor de entrada\n",
    "    filters : int\n",
    "        Número de filtros de la capa convolucional\n",
    "    kernel_size : int\n",
    "        Tamaño del kernel de la capa convolucional\n",
    "    dilation_rate : int\n",
    "        Tasa de dilatación de la capa convolucional\n",
    "    dropout_rate : float\n",
    "        Tasa de dropout\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    tf.Tensor\n",
    "        Tensor de salida del bloque WaveNet\n",
    "    \"\"\"\n",
    "    # Convolución dilatada\n",
    "    conv = Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                 dilation_rate=dilation_rate, padding='causal')(x)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    conv = Dropout(dropout_rate)(conv)\n",
    "    \n",
    "    # Conexión residual con proyección 1x1 si es necesario\n",
    "    if x.shape[-1] != filters:\n",
    "        x = Conv1D(filters, 1, padding='same')(x)\n",
    "    \n",
    "    # Alinear dimensiones temporales\n",
    "    x = x[:, -conv.shape[1]:, :]\n",
    "    res = Add()([conv, x])\n",
    "    \n",
    "    return res, conv\n",
    "\n",
    "def create_wavenet_model(cgm_shape: tuple, other_features_shape: tuple) -> Model:\n",
    "    \"\"\"\n",
    "    Crea un modelo WaveNet para predicción de series temporales.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    cgm_shape : tuple\n",
    "        Forma de los datos CGM (samples, timesteps, features)\n",
    "    other_features_shape : tuple\n",
    "        Forma de otras características (samples, features)\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    Model\n",
    "        Modelo WaveNet compilado\n",
    "    \"\"\"\n",
    "    cgm_input = Input(shape=cgm_shape[1:])\n",
    "    other_input = Input(shape=(other_features_shape[1],))\n",
    "    \n",
    "    x = Conv1D(WAVENET_CONFIG['filters'][0], 1, padding='same')(cgm_input)\n",
    "    current_filters = WAVENET_CONFIG['filters'][0]\n",
    "    \n",
    "    skip_outputs = []\n",
    "    \n",
    "    for filters in WAVENET_CONFIG['filters']:\n",
    "        for dilation in WAVENET_CONFIG['dilations']:\n",
    "            wavenet_block = WaveNetBlock(\n",
    "                filters=filters,\n",
    "                kernel_size=WAVENET_CONFIG['kernel_size'],\n",
    "                dilation_rate=dilation,\n",
    "                dropout_rate=WAVENET_CONFIG['dropout_rate']\n",
    "            )\n",
    "            x = wavenet_block(x)\n",
    "            \n",
    "            # Project skip connection to match final filter size\n",
    "            skip_proj = Conv1D(WAVENET_CONFIG['filters'][-1], 1, padding='same')(x)\n",
    "            skip_outputs.append(skip_proj)\n",
    "    \n",
    "    # Combinar skip connections\n",
    "    if skip_outputs:\n",
    "        target_len = skip_outputs[-1].shape[1]\n",
    "        aligned_skips = [\n",
    "            skip[:, -target_len:, :] for skip in skip_outputs\n",
    "        ]\n",
    "        x = Add()(aligned_skips)\n",
    "    \n",
    "    x = Activation('relu')(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Concatenate()([x, other_input])\n",
    "    \n",
    "    # Combinar con otras características\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(WAVENET_CONFIG['dropout_rate'])(x)\n",
    "    \n",
    "    output = Dense(1)(x)\n",
    "    \n",
    "    return Model(inputs=[cgm_input, other_input], outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones Visualización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(histories: dict, model_names: list):\n",
    "    \"\"\"\n",
    "    Visualiza el historial de entrenamiento de múltiples modelos.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    histories : dict\n",
    "        Diccionario con historiales de entrenamiento por modelo\n",
    "    model_names : list\n",
    "        Lista de nombres de modelos\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for name, history in histories.items():\n",
    "        plt.plot(history.history['loss'], label=f'{name} (train)')\n",
    "        plt.plot(history.history['val_loss'], label=f'{name} (val)', linestyle='--')\n",
    "    \n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Pérdida MSE')\n",
    "    plt.title('Comparación de Historiales de Entrenamiento')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, 'training_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_predictions_comparison(y_test: np.ndarray, predictions: dict):\n",
    "    \"\"\"\n",
    "    Visualiza comparación de predicciones de múltiples modelos.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    y_test : np.ndarray\n",
    "        Valores reales de prueba\n",
    "    predictions : dict\n",
    "        Diccionario con predicciones por modelo\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Scatter plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for name, y_pred in predictions.items():\n",
    "        plt.scatter(y_test, y_pred, alpha=0.5, label=name)\n",
    "    plt.plot([0, 15], [0, 15], 'r--')\n",
    "    plt.xlabel('Dosis Real (u. de insulina)')\n",
    "    plt.ylabel('Dosis Predicha (u. de insulina)')\n",
    "    plt.legend()\n",
    "    plt.title('Predicción vs. Real (Todos los Modelos)')\n",
    "    \n",
    "    # Residuals\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for name, y_pred in predictions.items():\n",
    "        plt.hist(y_test - y_pred, bins=20, alpha=0.5, label=name)\n",
    "    plt.xlabel('Residuo (u. de insulina)')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.legend()\n",
    "    plt.title('Distribución de Residuos')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, 'predictions_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento y Evaluación de los Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model: Model, model_name: str, \n",
    "                           X_cgm_train: np.ndarray, X_other_train: np.ndarray, \n",
    "                           y_train: np.ndarray, X_cgm_val: np.ndarray, \n",
    "                           X_other_val: np.ndarray, y_val: np.ndarray,\n",
    "                           X_cgm_test: np.ndarray, X_other_test: np.ndarray, \n",
    "                           y_test: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Entrena y evalúa un modelo específico.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    model : Model\n",
    "        Modelo a entrenar\n",
    "    model_name : str\n",
    "        Nombre del modelo para guardado/logging\n",
    "    X_cgm_train, X_other_train, y_train : np.ndarray\n",
    "        Datos de entrenamiento\n",
    "    X_cgm_val, X_other_val, y_val : np.ndarray\n",
    "        Datos de validación\n",
    "    X_cgm_test, X_other_test, y_test : np.ndarray\n",
    "        Datos de prueba\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    tuple\n",
    "        (history, y_pred, metrics_dict)\n",
    "    \"\"\"\n",
    "    # Compilar modelo\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mse'\n",
    "    )\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    history = model.fit(\n",
    "        [X_cgm_train, X_other_train],\n",
    "        y_train,\n",
    "        validation_data=([X_cgm_val, X_other_val], y_val),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=10,\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "        ],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Predecir y evaluar\n",
    "    y_pred = model.predict([X_cgm_test, X_other_test]).flatten()\n",
    "    \n",
    "    # Calcular métricas\n",
    "    metrics = {\n",
    "        'mae': mean_absolute_error(y_test, y_pred),\n",
    "        'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        'r2': r2_score(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    # Guardar modelo\n",
    "    model.save(os.path.join(MODELS_DIR, f'{model_name}.keras'))\n",
    "    \n",
    "    return history, y_pred, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creando y entrenando modelos...\n",
      "\n",
      "Entrenando modelo CNN...\n",
      "Epoch 1/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - loss: 10.6842 - val_loss: 1.5433\n",
      "Epoch 2/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 15ms/step - loss: 4.0211 - val_loss: 0.7095\n",
      "Epoch 3/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 16ms/step - loss: 3.2494 - val_loss: 0.8790\n",
      "Epoch 4/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - loss: 2.8521 - val_loss: 0.8370\n",
      "Epoch 5/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 17ms/step - loss: 2.6801 - val_loss: 1.1820\n",
      "Epoch 6/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - loss: 2.6114 - val_loss: 1.3664\n",
      "Epoch 7/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 27ms/step - loss: 2.4738 - val_loss: 1.1093\n",
      "Epoch 8/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 22ms/step - loss: 2.4368 - val_loss: 0.6290\n",
      "Epoch 9/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 16ms/step - loss: 2.3504 - val_loss: 0.6774\n",
      "Epoch 10/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - loss: 2.2362 - val_loss: 0.5419\n",
      "Epoch 11/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - loss: 2.3022 - val_loss: 1.7975\n",
      "Epoch 12/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 21ms/step - loss: 2.2723 - val_loss: 0.5348\n",
      "Epoch 13/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 26ms/step - loss: 2.1136 - val_loss: 0.7242\n",
      "Epoch 14/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 30ms/step - loss: 2.1810 - val_loss: 0.7098\n",
      "Epoch 15/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 29ms/step - loss: 2.1490 - val_loss: 0.5587\n",
      "Epoch 16/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 30ms/step - loss: 2.1050 - val_loss: 0.5566\n",
      "Epoch 17/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 24ms/step - loss: 2.1156 - val_loss: 0.5677\n",
      "Epoch 18/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 29ms/step - loss: 1.9753 - val_loss: 0.5191\n",
      "Epoch 19/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 23ms/step - loss: 2.0833 - val_loss: 0.7375\n",
      "Epoch 20/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - loss: 2.0390 - val_loss: 0.5414\n",
      "Epoch 21/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 25ms/step - loss: 2.0142 - val_loss: 0.3857\n",
      "Epoch 22/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - loss: 1.9429 - val_loss: 0.5823\n",
      "Epoch 23/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - loss: 1.9411 - val_loss: 0.5608\n",
      "Epoch 24/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - loss: 1.8408 - val_loss: 0.5905\n",
      "Epoch 25/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - loss: 2.0389 - val_loss: 0.5965\n",
      "Epoch 26/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - loss: 1.7669 - val_loss: 0.4127\n",
      "Epoch 27/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 15ms/step - loss: 1.7114 - val_loss: 0.4222\n",
      "Epoch 28/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 17ms/step - loss: 1.7711 - val_loss: 0.5837\n",
      "Epoch 29/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - loss: 1.8502 - val_loss: 0.6468\n",
      "Epoch 30/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - loss: 1.8477 - val_loss: 0.7700\n",
      "Epoch 31/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 18ms/step - loss: 1.8005 - val_loss: 0.4919\n",
      "\u001b[1m270/270\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step\n",
      "\n",
      "Entrenando modelo Transformer...\n",
      "Epoch 1/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 28ms/step - loss: 9.7918 - val_loss: 1.1207\n",
      "Epoch 2/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 24ms/step - loss: 3.2981 - val_loss: 0.9365\n",
      "Epoch 3/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 30ms/step - loss: 2.4540 - val_loss: 0.9138\n",
      "Epoch 4/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 28ms/step - loss: 2.0165 - val_loss: 0.5882\n",
      "Epoch 5/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 29ms/step - loss: 1.7550 - val_loss: 0.7288\n",
      "Epoch 6/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 23ms/step - loss: 1.5738 - val_loss: 0.6124\n",
      "Epoch 7/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 23ms/step - loss: 1.5528 - val_loss: 0.5373\n",
      "Epoch 8/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 22ms/step - loss: 1.3737 - val_loss: 0.6716\n",
      "Epoch 9/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 22ms/step - loss: 1.4104 - val_loss: 0.4656\n",
      "Epoch 10/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - loss: 1.3692 - val_loss: 0.4753\n",
      "Epoch 11/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - loss: 1.2944 - val_loss: 0.3756\n",
      "Epoch 12/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 24ms/step - loss: 1.2759 - val_loss: 0.4218\n",
      "Epoch 13/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 24ms/step - loss: 1.2382 - val_loss: 0.4006\n",
      "Epoch 14/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 23ms/step - loss: 1.1480 - val_loss: 0.3917\n",
      "Epoch 15/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 24ms/step - loss: 1.1644 - val_loss: 0.3833\n",
      "Epoch 16/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 22ms/step - loss: 1.0836 - val_loss: 0.4168\n",
      "Epoch 17/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 22ms/step - loss: 1.1327 - val_loss: 0.3974\n",
      "Epoch 18/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 23ms/step - loss: 1.0764 - val_loss: 0.3678\n",
      "Epoch 19/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - loss: 1.0192 - val_loss: 0.4371\n",
      "Epoch 20/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - loss: 1.0680 - val_loss: 0.3512\n",
      "Epoch 21/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - loss: 1.0358 - val_loss: 0.5285\n",
      "Epoch 22/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - loss: 1.1146 - val_loss: 0.3994\n",
      "Epoch 23/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - loss: 1.0441 - val_loss: 0.7038\n",
      "Epoch 24/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 30ms/step - loss: 0.9653 - val_loss: 0.3780\n",
      "Epoch 25/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - loss: 1.1187 - val_loss: 0.3530\n",
      "Epoch 26/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - loss: 0.9643 - val_loss: 0.4195\n",
      "Epoch 27/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - loss: 0.9675 - val_loss: 0.3691\n",
      "Epoch 28/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - loss: 1.0262 - val_loss: 0.3383\n",
      "Epoch 29/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - loss: 1.0239 - val_loss: 0.3147\n",
      "Epoch 30/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 22ms/step - loss: 0.9592 - val_loss: 0.3219\n",
      "Epoch 31/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - loss: 0.9339 - val_loss: 0.3827\n",
      "Epoch 32/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 22ms/step - loss: 1.0261 - val_loss: 0.3333\n",
      "Epoch 33/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - loss: 1.0725 - val_loss: 0.3346\n",
      "Epoch 34/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 17ms/step - loss: 0.9706 - val_loss: 0.3318\n",
      "Epoch 35/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 17ms/step - loss: 0.8555 - val_loss: 0.4053\n",
      "Epoch 36/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - loss: 1.0000 - val_loss: 0.2985\n",
      "Epoch 37/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 18ms/step - loss: 0.9447 - val_loss: 0.4039\n",
      "Epoch 38/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 22ms/step - loss: 0.9696 - val_loss: 0.4494\n",
      "Epoch 39/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 22ms/step - loss: 0.9657 - val_loss: 0.4626\n",
      "Epoch 40/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - loss: 0.9411 - val_loss: 0.2914\n",
      "Epoch 41/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - loss: 0.9129 - val_loss: 0.3208\n",
      "Epoch 42/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - loss: 0.9007 - val_loss: 0.2998\n",
      "Epoch 43/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 24ms/step - loss: 1.0114 - val_loss: 0.3076\n",
      "Epoch 44/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 27ms/step - loss: 0.8940 - val_loss: 0.3302\n",
      "Epoch 45/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 25ms/step - loss: 0.8300 - val_loss: 0.3565\n",
      "Epoch 46/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 24ms/step - loss: 0.8665 - val_loss: 0.3409\n",
      "Epoch 47/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 22ms/step - loss: 0.8736 - val_loss: 0.5898\n",
      "Epoch 48/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - loss: 0.8204 - val_loss: 0.3335\n",
      "Epoch 49/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - loss: 0.8937 - val_loss: 0.3426\n",
      "Epoch 50/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - loss: 0.8117 - val_loss: 0.3394\n",
      "\u001b[1m270/270\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step\n",
      "\n",
      "Entrenando modelo GRU...\n",
      "Epoch 1/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 86ms/step - loss: 9.9409 - val_loss: 2.5332\n",
      "Epoch 2/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 82ms/step - loss: 4.8942 - val_loss: 1.7929\n",
      "Epoch 3/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 80ms/step - loss: 4.2101 - val_loss: 3.4729\n",
      "Epoch 4/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 84ms/step - loss: 4.0351 - val_loss: 0.9788\n",
      "Epoch 5/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 88ms/step - loss: 3.1287 - val_loss: 0.7211\n",
      "Epoch 6/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 79ms/step - loss: 2.8994 - val_loss: 0.9571\n",
      "Epoch 7/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 80ms/step - loss: 2.5206 - val_loss: 1.6368\n",
      "Epoch 8/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 83ms/step - loss: 2.3341 - val_loss: 1.0887\n",
      "Epoch 9/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 78ms/step - loss: 2.3940 - val_loss: 1.0829\n",
      "Epoch 10/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 77ms/step - loss: 2.3418 - val_loss: 1.1580\n",
      "Epoch 11/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 82ms/step - loss: 1.9902 - val_loss: 0.6664\n",
      "Epoch 12/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 81ms/step - loss: 1.9205 - val_loss: 1.0983\n",
      "Epoch 13/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 79ms/step - loss: 1.9582 - val_loss: 1.0886\n",
      "Epoch 14/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 80ms/step - loss: 1.7216 - val_loss: 2.5751\n",
      "Epoch 15/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 76ms/step - loss: 1.7622 - val_loss: 3.8635\n",
      "Epoch 16/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 83ms/step - loss: 1.7289 - val_loss: 8.4253\n",
      "Epoch 17/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 106ms/step - loss: 1.6275 - val_loss: 8.5349\n",
      "Epoch 18/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m343s\u001b[0m 329ms/step - loss: 1.5356 - val_loss: 7.6992\n",
      "Epoch 19/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 296ms/step - loss: 1.4989 - val_loss: 3.7089\n",
      "Epoch 20/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 421ms/step - loss: 1.3281 - val_loss: 1.3260\n",
      "Epoch 21/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 143ms/step - loss: 1.2813 - val_loss: 23.1184\n",
      "\u001b[1m270/270\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 47ms/step\n",
      "\n",
      "Entrenando modelo Attention...\n",
      "Epoch 1/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 256ms/step - loss: 9.6894 - val_loss: 1.3402\n",
      "Epoch 2/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 233ms/step - loss: 3.6683 - val_loss: 1.0970\n",
      "Epoch 3/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 242ms/step - loss: 3.0915 - val_loss: 0.9720\n",
      "Epoch 4/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m934s\u001b[0m 898ms/step - loss: 2.7175 - val_loss: 0.8183\n",
      "Epoch 5/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 393ms/step - loss: 2.4595 - val_loss: 0.7000\n",
      "Epoch 6/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 280ms/step - loss: 2.1846 - val_loss: 0.5512\n",
      "Epoch 7/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 202ms/step - loss: 2.1451 - val_loss: 0.6006\n",
      "Epoch 8/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 194ms/step - loss: 1.7671 - val_loss: 0.5746\n",
      "Epoch 9/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 185ms/step - loss: 1.8005 - val_loss: 0.5316\n",
      "Epoch 10/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 207ms/step - loss: 1.5895 - val_loss: 0.5635\n",
      "Epoch 11/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 192ms/step - loss: 1.4323 - val_loss: 0.5607\n",
      "Epoch 12/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 175ms/step - loss: 1.3166 - val_loss: 0.4410\n",
      "Epoch 13/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 189ms/step - loss: 1.2709 - val_loss: 0.4152\n",
      "Epoch 14/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 191ms/step - loss: 1.1281 - val_loss: 0.4417\n",
      "Epoch 15/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 195ms/step - loss: 1.1046 - val_loss: 0.3750\n",
      "Epoch 16/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 177ms/step - loss: 1.0521 - val_loss: 0.4404\n",
      "Epoch 17/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 182ms/step - loss: 0.9537 - val_loss: 0.3796\n",
      "Epoch 18/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 191ms/step - loss: 1.0199 - val_loss: 0.3561\n",
      "Epoch 19/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 192ms/step - loss: 0.9624 - val_loss: 0.3853\n",
      "Epoch 20/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 183ms/step - loss: 0.9843 - val_loss: 0.3494\n",
      "Epoch 21/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 183ms/step - loss: 0.9311 - val_loss: 0.3309\n",
      "Epoch 22/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 186ms/step - loss: 0.8708 - val_loss: 0.3605\n",
      "Epoch 23/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 177ms/step - loss: 0.9028 - val_loss: 0.3133\n",
      "Epoch 24/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 182ms/step - loss: 1.0179 - val_loss: 0.4019\n",
      "Epoch 25/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 161ms/step - loss: 0.9041 - val_loss: 0.3378\n",
      "Epoch 26/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 167ms/step - loss: 0.8884 - val_loss: 0.3559\n",
      "Epoch 27/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 165ms/step - loss: 0.8790 - val_loss: 0.3229\n",
      "Epoch 28/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 166ms/step - loss: 0.8706 - val_loss: 0.3551\n",
      "Epoch 29/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 172ms/step - loss: 0.9367 - val_loss: 0.3775\n",
      "Epoch 30/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 185ms/step - loss: 0.9066 - val_loss: 0.3571\n",
      "Epoch 31/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 192ms/step - loss: 0.9367 - val_loss: 0.3325\n",
      "Epoch 32/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 169ms/step - loss: 0.9055 - val_loss: 0.3637\n",
      "Epoch 33/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 169ms/step - loss: 0.8758 - val_loss: 0.3599\n",
      "\u001b[1m270/270\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 66ms/step\n",
      "\n",
      "Entrenando modelo RNN...\n",
      "Epoch 1/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 141ms/step - loss: 14.6970 - val_loss: 1.6517\n",
      "Epoch 2/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 126ms/step - loss: 5.3448 - val_loss: 1.3823\n",
      "Epoch 3/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 121ms/step - loss: 4.8437 - val_loss: 2.1917\n",
      "Epoch 4/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 125ms/step - loss: 5.1285 - val_loss: 8.6607\n",
      "Epoch 5/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 123ms/step - loss: 5.0789 - val_loss: 1.8878\n",
      "Epoch 6/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 120ms/step - loss: 5.0808 - val_loss: 1.7318\n",
      "Epoch 7/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 130ms/step - loss: 5.6884 - val_loss: 1.6044\n",
      "Epoch 8/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 120ms/step - loss: 5.3779 - val_loss: 2.0085\n",
      "Epoch 9/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 126ms/step - loss: 5.2183 - val_loss: 1.6764\n",
      "Epoch 10/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 127ms/step - loss: 5.3140 - val_loss: 2.2788\n",
      "Epoch 11/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 126ms/step - loss: 5.7230 - val_loss: 1.6249\n",
      "Epoch 12/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 129ms/step - loss: 5.8125 - val_loss: 1.6537\n",
      "\u001b[1m270/270\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 54ms/step\n",
      "\n",
      "Entrenando modelo TabNet...\n",
      "Epoch 1/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 21ms/step - loss: 11.0983 - val_loss: 1.0991\n",
      "Epoch 2/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 15ms/step - loss: 3.6670 - val_loss: 1.1632\n",
      "Epoch 3/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 15ms/step - loss: 2.4730 - val_loss: 1.0067\n",
      "Epoch 4/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 2.2530 - val_loss: 0.8612\n",
      "Epoch 5/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 15ms/step - loss: 1.9914 - val_loss: 0.9026\n",
      "Epoch 6/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 25ms/step - loss: 1.8313 - val_loss: 0.4614\n",
      "Epoch 7/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 17ms/step - loss: 1.6648 - val_loss: 0.6260\n",
      "Epoch 8/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 1.6973 - val_loss: 0.4455\n",
      "Epoch 9/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 14ms/step - loss: 1.5990 - val_loss: 0.5379\n",
      "Epoch 10/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 1.4055 - val_loss: 0.8103\n",
      "Epoch 11/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 1.4814 - val_loss: 0.5766\n",
      "Epoch 12/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 15ms/step - loss: 1.4096 - val_loss: 2.1091\n",
      "Epoch 13/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 16ms/step - loss: 1.6426 - val_loss: 0.3412\n",
      "Epoch 14/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 1.3387 - val_loss: 0.4000\n",
      "Epoch 15/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 15ms/step - loss: 1.3480 - val_loss: 0.5449\n",
      "Epoch 16/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 17ms/step - loss: 1.2770 - val_loss: 3.7288\n",
      "Epoch 17/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 14ms/step - loss: 1.3447 - val_loss: 0.3993\n",
      "Epoch 18/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 12ms/step - loss: 1.2214 - val_loss: 0.9175\n",
      "Epoch 19/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 15ms/step - loss: 1.3106 - val_loss: 1.7598\n",
      "Epoch 20/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 15ms/step - loss: 1.3218 - val_loss: 0.6469\n",
      "Epoch 21/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 14ms/step - loss: 1.1885 - val_loss: 8.4859\n",
      "Epoch 22/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 15ms/step - loss: 1.2635 - val_loss: 0.2550\n",
      "Epoch 23/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 15ms/step - loss: 1.2101 - val_loss: 0.6728\n",
      "Epoch 24/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 13ms/step - loss: 1.1508 - val_loss: 3.3864\n",
      "Epoch 25/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 14ms/step - loss: 1.2257 - val_loss: 1.4415\n",
      "Epoch 26/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 15ms/step - loss: 1.1322 - val_loss: 0.4439\n",
      "Epoch 27/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 13ms/step - loss: 1.1867 - val_loss: 0.4302\n",
      "Epoch 28/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 14ms/step - loss: 1.1127 - val_loss: 0.3469\n",
      "Epoch 29/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 9ms/step - loss: 1.1810 - val_loss: 0.4922\n",
      "Epoch 30/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 12ms/step - loss: 1.0898 - val_loss: 0.6229\n",
      "Epoch 31/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 14ms/step - loss: 1.0593 - val_loss: 0.7328\n",
      "Epoch 32/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 1.1251 - val_loss: 0.6366\n",
      "\u001b[1m270/270\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step\n",
      "\n",
      "Entrenando modelo TCN...\n",
      "Epoch 1/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 45ms/step - loss: 12.5446 - val_loss: 1.8994\n",
      "Epoch 2/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 50ms/step - loss: 5.8108 - val_loss: 1.6864\n",
      "Epoch 3/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 48ms/step - loss: 5.1632 - val_loss: 3.7288\n",
      "Epoch 4/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 52ms/step - loss: 4.7896 - val_loss: 1.3983\n",
      "Epoch 5/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 47ms/step - loss: 4.7285 - val_loss: 1.7656\n",
      "Epoch 6/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 47ms/step - loss: 4.3485 - val_loss: 15.9451\n",
      "Epoch 7/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 50ms/step - loss: 4.1429 - val_loss: 2.6086\n",
      "Epoch 8/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 43ms/step - loss: 4.0525 - val_loss: 1.7547\n",
      "Epoch 9/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 50ms/step - loss: 3.7605 - val_loss: 1.4837\n",
      "Epoch 10/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 42ms/step - loss: 3.7947 - val_loss: 1.7377\n",
      "Epoch 11/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 43ms/step - loss: 3.4826 - val_loss: 1.7643\n",
      "Epoch 12/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 48ms/step - loss: 3.4907 - val_loss: 1.5018\n",
      "Epoch 13/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 43ms/step - loss: 3.3764 - val_loss: 1.9983\n",
      "Epoch 14/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 41ms/step - loss: 3.2724 - val_loss: 1.3501\n",
      "Epoch 15/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 47ms/step - loss: 3.1827 - val_loss: 1.5807\n",
      "Epoch 16/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 50ms/step - loss: 3.2410 - val_loss: 2.3736\n",
      "Epoch 17/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 48ms/step - loss: 3.1493 - val_loss: 2.8039\n",
      "Epoch 18/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 46ms/step - loss: 2.9451 - val_loss: 0.9264\n",
      "Epoch 19/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 46ms/step - loss: 2.7052 - val_loss: 1.0036\n",
      "Epoch 20/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 46ms/step - loss: 3.0241 - val_loss: 0.8567\n",
      "Epoch 21/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 46ms/step - loss: 2.8036 - val_loss: 0.9758\n",
      "Epoch 22/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 48ms/step - loss: 2.5541 - val_loss: 0.5658\n",
      "Epoch 23/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 48ms/step - loss: 2.4510 - val_loss: 1.8646\n",
      "Epoch 24/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 41ms/step - loss: 2.5010 - val_loss: 1.2863\n",
      "Epoch 25/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 49ms/step - loss: 2.4242 - val_loss: 2.4568\n",
      "Epoch 26/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 44ms/step - loss: 2.5091 - val_loss: 1.2741\n",
      "Epoch 27/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 43ms/step - loss: 2.4398 - val_loss: 0.6561\n",
      "Epoch 28/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 46ms/step - loss: 2.2666 - val_loss: 0.8256\n",
      "Epoch 29/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 46ms/step - loss: 2.2756 - val_loss: 0.9104\n",
      "Epoch 30/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 49ms/step - loss: 2.2540 - val_loss: 0.6804\n",
      "Epoch 31/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 47ms/step - loss: 2.2146 - val_loss: 1.0036\n",
      "Epoch 32/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 50ms/step - loss: 2.0881 - val_loss: 0.8604\n",
      "\u001b[1m270/270\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step\n",
      "\n",
      "Entrenando modelo WaveNet...\n",
      "Epoch 1/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m570s\u001b[0m 483ms/step - loss: 17.8789 - val_loss: 8.6400\n",
      "Epoch 2/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m475s\u001b[0m 457ms/step - loss: 13.6924 - val_loss: 9.0091\n",
      "Epoch 3/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m471s\u001b[0m 453ms/step - loss: 13.7600 - val_loss: 9.6409\n",
      "Epoch 4/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m505s\u001b[0m 485ms/step - loss: 13.9570 - val_loss: 8.1522\n",
      "Epoch 5/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m927s\u001b[0m 892ms/step - loss: 14.0410 - val_loss: 8.5714\n",
      "Epoch 6/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2902s\u001b[0m 3s/step - loss: 13.5092 - val_loss: 8.5589\n",
      "Epoch 7/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m862s\u001b[0m 825ms/step - loss: 13.3983 - val_loss: 15.0716\n",
      "Epoch 8/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 296ms/step - loss: 13.2761 - val_loss: 8.4495\n",
      "Epoch 9/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 367ms/step - loss: 13.7207 - val_loss: 8.6019\n",
      "Epoch 10/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 348ms/step - loss: 13.2472 - val_loss: 8.1029\n",
      "Epoch 11/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 364ms/step - loss: 13.1048 - val_loss: 9.4458\n",
      "Epoch 12/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 470ms/step - loss: 12.6186 - val_loss: 8.5450\n",
      "Epoch 13/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 408ms/step - loss: 12.9308 - val_loss: 10.1786\n",
      "Epoch 14/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 366ms/step - loss: 12.7859 - val_loss: 8.0927\n",
      "Epoch 15/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 401ms/step - loss: 13.1120 - val_loss: 9.4288\n",
      "Epoch 16/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 353ms/step - loss: 12.9685 - val_loss: 13.2348\n",
      "Epoch 17/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 363ms/step - loss: 13.4863 - val_loss: 7.8053\n",
      "Epoch 18/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 404ms/step - loss: 12.6455 - val_loss: 7.7369\n",
      "Epoch 19/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m423s\u001b[0m 407ms/step - loss: 13.2428 - val_loss: 8.6458\n",
      "Epoch 20/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m429s\u001b[0m 394ms/step - loss: 12.7709 - val_loss: 7.8557\n",
      "Epoch 21/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 377ms/step - loss: 13.2612 - val_loss: 8.8975\n",
      "Epoch 22/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 382ms/step - loss: 12.9742 - val_loss: 7.4407\n",
      "Epoch 23/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 380ms/step - loss: 13.0662 - val_loss: 7.5576\n",
      "Epoch 24/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m486s\u001b[0m 467ms/step - loss: 13.1862 - val_loss: 8.0093\n",
      "Epoch 25/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m403s\u001b[0m 387ms/step - loss: 12.7121 - val_loss: 8.6245\n",
      "Epoch 26/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 328ms/step - loss: 13.1406 - val_loss: 8.5106\n",
      "Epoch 27/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 393ms/step - loss: 12.9653 - val_loss: 8.1751\n",
      "Epoch 28/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m425s\u001b[0m 408ms/step - loss: 13.1078 - val_loss: 8.2673\n",
      "Epoch 29/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 414ms/step - loss: 13.0167 - val_loss: 9.5237\n",
      "Epoch 30/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 396ms/step - loss: 12.8536 - val_loss: 9.4400\n",
      "Epoch 31/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m433s\u001b[0m 416ms/step - loss: 12.8606 - val_loss: 9.7609\n",
      "Epoch 32/100\n",
      "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 401ms/step - loss: 12.8345 - val_loss: 8.0597\n",
      "\u001b[1m270/270\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 170ms/step\n",
      "\n",
      "Rendimiento por sujeto:\n",
      "\n",
      "Sujeto 5:\n",
      "----------------------------------------\n",
      "CNN             MAE=0.60, RMSE=2.58, R²=-3.00\n",
      "Transformer     MAE=0.45, RMSE=2.14, R²=-1.75\n",
      "GRU             MAE=0.44, RMSE=1.15, R²=0.21\n",
      "Attention       MAE=0.25, RMSE=0.94, R²=0.47\n",
      "RNN             MAE=1.29, RMSE=5.67, R²=-18.27\n",
      "TabNet          MAE=0.65, RMSE=5.45, R²=-16.85\n",
      "TCN             MAE=0.40, RMSE=1.12, R²=0.25\n",
      "WaveNet         MAE=1.54, RMSE=1.70, R²=-0.73\n",
      "\n",
      "Sujeto 19:\n",
      "----------------------------------------\n",
      "CNN             MAE=2.12, RMSE=3.03, R²=-0.46\n",
      "Transformer     MAE=3.75, RMSE=4.21, R²=-1.82\n",
      "GRU             MAE=1.92, RMSE=2.25, R²=0.20\n",
      "Attention       MAE=0.86, RMSE=1.13, R²=0.80\n",
      "RNN             MAE=1.51, RMSE=1.82, R²=0.47\n",
      "TabNet          MAE=2.00, RMSE=2.71, R²=-0.17\n",
      "TCN             MAE=2.59, RMSE=5.38, R²=-3.60\n",
      "WaveNet         MAE=2.81, RMSE=3.30, R²=-0.74\n",
      "\n",
      "Sujeto 32:\n",
      "----------------------------------------\n",
      "CNN             MAE=0.40, RMSE=0.75, R²=0.89\n",
      "Transformer     MAE=0.26, RMSE=0.45, R²=0.96\n",
      "GRU             MAE=0.43, RMSE=0.75, R²=0.89\n",
      "Attention       MAE=0.24, RMSE=0.51, R²=0.95\n",
      "RNN             MAE=0.79, RMSE=1.49, R²=0.56\n",
      "TabNet          MAE=0.27, RMSE=0.46, R²=0.96\n",
      "TCN             MAE=0.40, RMSE=0.75, R²=0.89\n",
      "WaveNet         MAE=1.56, RMSE=2.08, R²=0.14\n",
      "\n",
      "Sujeto 13:\n",
      "----------------------------------------\n",
      "CNN             MAE=0.22, RMSE=0.39, R²=0.79\n",
      "Transformer     MAE=0.11, RMSE=0.27, R²=0.90\n",
      "GRU             MAE=0.37, RMSE=0.52, R²=0.63\n",
      "Attention       MAE=0.16, RMSE=0.35, R²=0.83\n",
      "RNN             MAE=0.42, RMSE=0.57, R²=0.56\n",
      "TabNet          MAE=0.22, RMSE=0.35, R²=0.83\n",
      "TCN             MAE=0.28, RMSE=0.50, R²=0.65\n",
      "WaveNet         MAE=1.03, RMSE=1.19, R²=-0.96\n",
      "\n",
      "Sujeto 48:\n",
      "----------------------------------------\n",
      "CNN             MAE=0.73, RMSE=1.12, R²=0.39\n",
      "Transformer     MAE=0.45, RMSE=0.72, R²=0.75\n",
      "GRU             MAE=0.92, RMSE=1.25, R²=0.24\n",
      "Attention       MAE=0.61, RMSE=1.05, R²=0.47\n",
      "RNN             MAE=0.95, RMSE=1.24, R²=0.26\n",
      "TabNet          MAE=0.50, RMSE=0.87, R²=0.63\n",
      "TCN             MAE=0.91, RMSE=1.25, R²=0.24\n",
      "WaveNet         MAE=1.25, RMSE=1.60, R²=-0.24\n",
      "\n",
      "Sujeto 49:\n",
      "----------------------------------------\n",
      "CNN             MAE=3.35, RMSE=4.74, R²=-0.06\n",
      "Transformer     MAE=2.60, RMSE=3.18, R²=0.52\n",
      "GRU             MAE=3.24, RMSE=3.98, R²=0.25\n",
      "Attention       MAE=1.72, RMSE=2.37, R²=0.73\n",
      "RNN             MAE=3.04, RMSE=5.38, R²=-0.37\n",
      "TabNet          MAE=1.84, RMSE=3.64, R²=0.38\n",
      "TCN             MAE=3.78, RMSE=7.84, R²=-1.90\n",
      "WaveNet         MAE=4.38, RMSE=5.93, R²=-0.66\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento y evaluación de modelos\n",
    "print(\"\\nCreando y entrenando modelos...\")\n",
    "\n",
    "models = {\n",
    "    'CNN': create_cnn_model(X_cgm_train.shape, X_other_train.shape),\n",
    "    'Transformer': create_transformer_model(X_cgm_train.shape, X_other_train.shape),\n",
    "    'GRU': create_gru_model(X_cgm_train.shape, X_other_train.shape),\n",
    "    'Attention': create_attention_model(X_cgm_train.shape, X_other_train.shape),\n",
    "    'RNN': create_rnn_model(X_cgm_train.shape, X_other_train.shape),\n",
    "    'TabNet': create_tabnet_model(X_cgm_train.shape, X_other_train.shape),\n",
    "    'TCN': create_tcn_model(X_cgm_train.shape, X_other_train.shape),\n",
    "    'WaveNet': create_wavenet_model(X_cgm_train.shape, X_other_train.shape),\n",
    "}\n",
    "\n",
    "\n",
    "histories = {}\n",
    "predictions = {}\n",
    "metrics = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nEntrenando modelo {name}...\")\n",
    "    history, y_pred, model_metrics = train_and_evaluate_model(\n",
    "        model, name,\n",
    "        X_cgm_train, X_other_train, y_train,\n",
    "        X_cgm_val, X_other_val, y_val,\n",
    "        X_cgm_test, X_other_test, y_test\n",
    "    )\n",
    "    \n",
    "    histories[name] = history\n",
    "    predictions[name] = y_pred\n",
    "    metrics[name] = model_metrics\n",
    "\n",
    "# Evaluación por sujeto\n",
    "print(\"\\nRendimiento por sujeto:\")\n",
    "for subject_id in test_subjects:\n",
    "    mask = subject_test == subject_id\n",
    "    y_test_sub = y_test[mask]\n",
    "    \n",
    "    print(f\"\\nSujeto {subject_id}:\")\n",
    "    print(\"-\" * 40)\n",
    "    for name, y_pred in predictions.items():\n",
    "        y_pred_sub = y_pred[mask]\n",
    "        mae_sub = mean_absolute_error(y_test_sub, y_pred_sub)\n",
    "        rmse_sub = np.sqrt(mean_squared_error(y_test_sub, y_pred_sub))\n",
    "        r2_sub = r2_score(y_test_sub, y_pred_sub)\n",
    "        print(f\"{name:<15} MAE={mae_sub:.2f}, RMSE={rmse_sub:.2f}, R²={r2_sub:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de los Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de resultados\n",
    "plot_training_history(histories, list(models.keys()))\n",
    "plot_predictions_comparison(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas Comparativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparación de métricas:\n",
      "--------------------------------------------------\n",
      "Modelo               MAE     RMSE       R²\n",
      "--------------------------------------------------\n",
      "CNN                 0.85     1.97     0.32\n",
      "Transformer         0.64     1.53     0.59\n",
      "GRU                 0.93     1.56     0.57\n",
      "Attention           0.55     1.11     0.79\n",
      "RNN                 1.13     3.07    -0.65\n",
      "TabNet              0.62     2.68    -0.27\n",
      "TCN                 0.96     2.52    -0.12\n",
      "WaveNet             1.60     2.32     0.05\n"
     ]
    }
   ],
   "source": [
    "# Imprimir métricas comparativas\n",
    "print(\"\\nComparación de métricas:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Modelo':<15} {'MAE':>8} {'RMSE':>8} {'R²':>8}\")\n",
    "print(\"-\" * 50)\n",
    "for name, metric in metrics.items():\n",
    "    print(f\"{name:<15} {metric['mae']:8.2f} {metric['rmse']:8.2f} {metric['r2']:8.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
