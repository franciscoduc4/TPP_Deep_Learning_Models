{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from datetime import timedelta\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "PROJECT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Global configuration\n",
    "CONFIG = {\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 300,\n",
    "    \"patience\": 30,\n",
    "    \"learning_rate\": 0.0005,\n",
    "    \"embedding_dim\": 16,\n",
    "    \"window_hours\": 2,\n",
    "    \"cap_normal\": 30,\n",
    "    \"cap_bg\": 300,\n",
    "    \"cap_iob\": 5,\n",
    "    \"cap_carb\": 150,\n",
    "    \"data_path\": os.path.join(PROJECT_DIR, \"data\", \"subjects\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_device():\n",
    "    print(\"Device:\", CONFIG[\"device\"])\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"Device count:\", torch.cuda.device_count())\n",
    "    print(\"Current device:\", torch.cuda.current_device())\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "    if torch.cuda.is_available():\n",
    "        a = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], device=CONFIG[\"device\"])\n",
    "        b = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], device=CONFIG[\"device\"])\n",
    "        c = torch.matmul(a, b)\n",
    "        print(\"Test GPU operation successful:\", c)\n",
    "\n",
    "check_device()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cgm_window(bolus_time, cgm_df, window_hours=CONFIG[\"window_hours\"]):\n",
    "    \"\"\"\n",
    "    Obtiene una ventana de datos CGM alrededor del tiempo del bolo de insulina.\n",
    "    \n",
    "    Args:\n",
    "        bolus_time: Tiempo del bolo de insulina\n",
    "        cgm_df: DataFrame con datos CGM\n",
    "        window_hours: Tamaño de la ventana en horas\n",
    "    \n",
    "    Returns:\n",
    "        Array numpy con los últimos 24 valores CGM o None si no hay suficientes datos\n",
    "    \"\"\"\n",
    "    # Calcula inicio de la ventana\n",
    "    window_start = bolus_time - timedelta(hours=window_hours)\n",
    "    \n",
    "    # Obtiene datos CGM dentro de la ventana de tiempo\n",
    "    window = cgm_df[(cgm_df['date'] >= window_start) & (cgm_df['date'] <= bolus_time)]\n",
    "    \n",
    "    # Ordena y toma los últimos 24 valores\n",
    "    window = window.sort_values('date').tail(24)\n",
    "    \n",
    "    return window['mg/dl'].values if len(window) >= 24 else None\n",
    "\n",
    "def calculate_iob(bolus_time, basal_df, half_life_hours=4):\n",
    "    \"\"\"\n",
    "    Calcula la insulina activa (IOB) en un momento dado.\n",
    "    \n",
    "    Args:\n",
    "        bolus_time: Tiempo para calcular IOB\n",
    "        basal_df: DataFrame con datos de insulina basal\n",
    "        half_life_hours: Vida media de la insulina en horas\n",
    "    \n",
    "    Returns:\n",
    "        Float con cantidad de insulina activa\n",
    "    \"\"\"\n",
    "    if basal_df is None or basal_df.empty:\n",
    "        return 0.0\n",
    "        \n",
    "    iob = 0\n",
    "    # Itera sobre cada registro de insulina basal\n",
    "    for _, row in basal_df.iterrows():\n",
    "        start_time = row['date']\n",
    "        # Convierte duración de milisegundos a horas\n",
    "        duration_hours = row['duration'] / (1000 * 3600)\n",
    "        end_time = start_time + timedelta(hours=duration_hours)\n",
    "        # Usa tasa de 0.9 si no hay valor\n",
    "        rate = row['rate'] if pd.notna(row['rate']) else 0.9\n",
    "        \n",
    "        # Si el tiempo del bolo está dentro del período activo\n",
    "        if start_time <= bolus_time <= end_time:\n",
    "            # Calcula tiempo transcurrido y restante\n",
    "            time_since_start = (bolus_time - start_time).total_seconds() / 3600\n",
    "            remaining = rate * (1 - (time_since_start / half_life_hours))\n",
    "            iob += max(0, remaining)\n",
    "            \n",
    "    return iob\n",
    "\n",
    "def process_subject(subject_path, idx):\n",
    "    \"\"\"\n",
    "    Procesa los datos de un sujeto del estudio.\n",
    "    \n",
    "    Args:\n",
    "        subject_path: Ruta al archivo Excel con datos del sujeto\n",
    "        idx: Índice/ID del sujeto\n",
    "    \n",
    "    Returns:\n",
    "        Lista de diccionarios con características procesadas\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Carga datos desde Excel\n",
    "    try:\n",
    "        excel_file = pd.ExcelFile(subject_path)\n",
    "        cgm_df = pd.read_excel(excel_file, sheet_name=\"CGM\")\n",
    "        bolus_df = pd.read_excel(excel_file, sheet_name=\"Bolus\")\n",
    "        try:\n",
    "            basal_df = pd.read_excel(excel_file, sheet_name=\"Basal\")\n",
    "        except ValueError:\n",
    "            basal_df = None\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar {os.path.basename(subject_path)}: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Convierte fechas a datetime\n",
    "    cgm_df['date'] = pd.to_datetime(cgm_df['date'])\n",
    "    cgm_df = cgm_df.sort_values('date')\n",
    "    bolus_df['date'] = pd.to_datetime(bolus_df['date'])\n",
    "    if basal_df is not None:\n",
    "        basal_df['date'] = pd.to_datetime(basal_df['date'])\n",
    "\n",
    "    processed_data = []\n",
    "    # Procesa cada registro de bolo\n",
    "    for _, row in tqdm(bolus_df.iterrows(), total=len(bolus_df), desc=f\"Procesando {os.path.basename(subject_path)}\", leave=False):\n",
    "        bolus_time = row['date']\n",
    "        cgm_window = get_cgm_window(bolus_time, cgm_df)\n",
    "        \n",
    "        if cgm_window is not None:\n",
    "            # Calcula características\n",
    "            iob = calculate_iob(bolus_time, basal_df)\n",
    "            hour_of_day = bolus_time.hour / 23.0  # Normaliza hora del día\n",
    "            \n",
    "            # Usa último valor CGM si no hay bgInput\n",
    "            bg_input = row['bgInput'] if pd.notna(row['bgInput']) else cgm_window[-1]\n",
    "            \n",
    "            # Procesa dosis normal y límites\n",
    "            normal = row['normal'] if pd.notna(row['normal']) else 0.0\n",
    "            normal = np.clip(normal, 0, CONFIG[\"cap_normal\"])\n",
    "            \n",
    "            # Calcula factor de sensibilidad personalizado\n",
    "            isf_custom = 50.0 if normal <= 0 or bg_input <= 100 else (bg_input - 100) / normal\n",
    "            \n",
    "            # Aplica límites a variables\n",
    "            bg_input = np.clip(bg_input, 0, CONFIG[\"cap_bg\"])\n",
    "            iob = np.clip(iob, 0, CONFIG[\"cap_iob\"])\n",
    "            carb_input = row['carbInput'] if pd.notna(row['carbInput']) else 0.0\n",
    "            carb_input = np.clip(carb_input, 0, CONFIG[\"cap_carb\"])\n",
    "            \n",
    "            # Crea diccionario de características\n",
    "            features = {\n",
    "                'subject_id': idx,\n",
    "                'cgm_window': cgm_window,\n",
    "                'carbInput': carb_input,\n",
    "                'bgInput': bg_input,\n",
    "                'insulinCarbRatio': row['insulinCarbRatio'] if pd.notna(row['insulinCarbRatio']) else 10.0,\n",
    "                'insulinSensitivityFactor': isf_custom,\n",
    "                'insulinOnBoard': iob,\n",
    "                'hour_of_day': hour_of_day,\n",
    "                'normal': normal\n",
    "            }\n",
    "            processed_data.append(features)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Procesado {os.path.basename(subject_path)} (Sujeto {idx+1}) en {elapsed_time:.2f} segundos\")\n",
    "    return processed_data\n",
    "\n",
    "def preprocess_data(subject_folder):\n",
    "    \"\"\"\n",
    "    Preprocesa los datos de todos los sujetos para el entrenamiento del modelo.\n",
    "    \n",
    "    Args:\n",
    "        subject_folder: Ruta a la carpeta que contiene los archivos Excel de los sujetos\n",
    "        \n",
    "    Returns:\n",
    "        X_cgm: Array de datos CGM normalizados\n",
    "        X_other: Array de otras características normalizadas\n",
    "        X_subject: Array de IDs de sujetos\n",
    "        y: Array de dosis objetivo normalizadas\n",
    "        df_final: DataFrame con todos los datos procesados\n",
    "        scaler_cgm: Scaler usado para normalizar datos CGM\n",
    "        scaler_other: Scaler usado para normalizar otras características\n",
    "        scaler_y: Scaler usado para normalizar objetivos\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Obtener lista de archivos Excel de sujetos\n",
    "    subject_files = [f for f in os.listdir(subject_folder) if f.startswith(\"Subject\") and f.endswith(\".xlsx\")]\n",
    "    print(f\"\\nFound Subject files ({len(subject_files)}):\")\n",
    "    for f in subject_files:\n",
    "        print(f)\n",
    "\n",
    "    # Procesar datos de cada sujeto en paralelo\n",
    "    all_processed_data = Parallel(n_jobs=-1)(delayed(process_subject)(os.path.join(subject_folder, f), idx) \n",
    "                                            for idx, f in enumerate(subject_files))\n",
    "    all_processed_data = [item for sublist in all_processed_data for item in sublist]\n",
    "\n",
    "    # Crear DataFrame con todos los datos procesados\n",
    "    df_processed = pd.DataFrame(all_processed_data)\n",
    "    print(\"Muestra de datos procesados combinados:\")\n",
    "    print(df_processed.head())\n",
    "    print(f\"Total de muestras: {len(df_processed)}\")\n",
    "\n",
    "    # Mostrar estadísticas de dosis de insulina por sujeto\n",
    "    print(\"\\nEstadísticas de 'normal' por sujeto (antes de normalización):\")\n",
    "    for subject_id in df_processed['subject_id'].unique():\n",
    "        subject_data = df_processed[df_processed['subject_id'] == subject_id]['normal']\n",
    "        print(f\"Sujeto {subject_id}: min={subject_data.min():.2f}, max={subject_data.max():.2f}, mean={subject_data.mean():.2f}, std={subject_data.std():.2f}\")\n",
    "\n",
    "    # Expandir columna de ventana CGM en múltiples columnas\n",
    "    cgm_columns = [f'cgm_{i}' for i in range(24)]\n",
    "    df_cgm = pd.DataFrame(df_processed['cgm_window'].tolist(), columns=cgm_columns, index=df_processed.index)\n",
    "    df_final = pd.concat([df_cgm, df_processed.drop(columns=['cgm_window'])], axis=1)\n",
    "\n",
    "    # Eliminar filas con valores faltantes\n",
    "    df_final = df_final.dropna()\n",
    "    print(\"Verificación de NaN en df_final:\")\n",
    "    print(df_final.isna().sum())\n",
    "\n",
    "    # Inicializar scalers para normalización\n",
    "    scaler_cgm = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_other = StandardScaler()\n",
    "    scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    # Preparar arrays de características y objetivos\n",
    "    X_cgm = scaler_cgm.fit_transform(df_final[cgm_columns]).reshape(-1, 24, 1)\n",
    "    X_subject = df_final['subject_id'].values\n",
    "    other_features = ['carbInput', 'bgInput', 'insulinOnBoard', 'insulinCarbRatio', \n",
    "                        'insulinSensitivityFactor', 'hour_of_day']\n",
    "    X_other = scaler_other.fit_transform(df_final[other_features])\n",
    "    y = scaler_y.fit_transform(df_final['normal'].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Verificar que no hay valores NaN\n",
    "    print(\"NaN en X_cgm:\", np.isnan(X_cgm).sum())\n",
    "    print(\"NaN en X_other:\", np.isnan(X_other).sum())\n",
    "    print(\"NaN en X_subject:\", np.isnan(X_subject).sum())\n",
    "    print(\"NaN in y:\", np.isnan(y).sum())\n",
    "    if np.isnan(X_cgm).sum() > 0 or np.isnan(X_other).sum() > 0 or np.isnan(X_subject).sum() > 0 or np.isnan(y).sum() > 0:\n",
    "        raise ValueError(\"Valores NaN detectados\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Preprocesamiento completo en {elapsed_time:.2f} segundos\")\n",
    "    return X_cgm, X_other, X_subject, y, df_final, scaler_cgm, scaler_other, scaler_y\n",
    "\n",
    "def split_data(X_cgm, X_other, X_subject, y, df_final):\n",
    "    \"\"\"\n",
    "    Divide los datos en conjuntos de entrenamiento, validación y prueba.\n",
    "    \n",
    "    Args:\n",
    "        X_cgm: Array de datos CGM\n",
    "        X_other: Array de otras características\n",
    "        X_subject: Array de IDs de sujetos\n",
    "        y: Array de objetivos\n",
    "        df_final: DataFrame con todos los datos\n",
    "        \n",
    "    Returns:\n",
    "        Tupla con datos divididos para entrenamiento, validación y prueba\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Dividir sujetos en conjuntos de entrenamiento, validación y prueba\n",
    "    subject_ids = df_final['subject_id'].unique()\n",
    "    train_subjects, temp_subjects = train_test_split(subject_ids, test_size=0.2, random_state=42)\n",
    "    val_subjects, test_subjects = train_test_split(temp_subjects, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Crear máscaras para cada conjunto\n",
    "    train_mask = df_final['subject_id'].isin(train_subjects)\n",
    "    val_mask = df_final['subject_id'].isin(val_subjects)\n",
    "    test_mask = df_final['subject_id'].isin(test_subjects)\n",
    "\n",
    "    # Aplicar máscaras para dividir los datos\n",
    "    X_cgm_train, X_cgm_val, X_cgm_test = X_cgm[train_mask], X_cgm[val_mask], X_cgm[test_mask]\n",
    "    X_other_train, X_other_val, X_other_test = X_other[train_mask], X_other[val_mask], X_other[test_mask]\n",
    "    X_subject_train, X_subject_val, X_subject_test = X_subject[train_mask], X_subject[val_mask], X_subject[test_mask]\n",
    "    y_train, y_val, y_test = y[train_mask], y[val_mask], y[test_mask]\n",
    "    subject_test = df_final[test_mask]['subject_id'].values\n",
    "\n",
    "    # Imprimir información sobre las divisiones\n",
    "    print(f\"Entrenamiento CGM: {X_cgm_train.shape}, Validación CGM: {X_cgm_val.shape}, Prueba CGM: {X_cgm_test.shape}\")\n",
    "    print(f\"Entrenamiento Otros: {X_other_train.shape}, Validación Otros: {X_other_val.shape}, Prueba Otros: {X_other_test.shape}\")\n",
    "    print(f\"Entrenamiento Subject: {X_subject_train.shape}, Validación Subject: {X_subject_val.shape}, Prueba Subject: {X_subject_test.shape}\")\n",
    "    print(f\"Sujetos de prueba: {test_subjects}\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"División de datos completa en {elapsed_time:.2f} segundos\")\n",
    "    return (X_cgm_train, X_cgm_val, X_cgm_test,\n",
    "            X_other_train, X_other_val, X_other_test,\n",
    "            X_subject_train, X_subject_val, X_subject_test,\n",
    "            y_train, y_val, y_test, subject_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(X_cgm_train, X_cgm_val, X_cgm_test,\n",
    "                      X_other_train, X_other_val, X_other_test,\n",
    "                      X_subject_train, X_subject_val, X_subject_test,\n",
    "                      y_train, y_val, y_test):\n",
    "    \"\"\"\n",
    "    Crea los DataLoaders para entrenamiento, validación y prueba.\n",
    "    \n",
    "    Args:\n",
    "        X_cgm_train/val/test: Datos CGM para cada conjunto\n",
    "        X_other_train/val/test: Otras características para cada conjunto  \n",
    "        X_subject_train/val/test: IDs de sujetos para cada conjunto\n",
    "        y_train/val/test: Variables objetivo para cada conjunto\n",
    "\n",
    "    Returns:\n",
    "        train_loader: DataLoader para entrenamiento\n",
    "        val_loader: DataLoader para validación  \n",
    "        test_loader: DataLoader para prueba\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Convertir datos a tensores PyTorch y mover a GPU/CPU\n",
    "    X_cgm_train = torch.tensor(X_cgm_train, dtype=torch.float32).to(CONFIG[\"device\"])\n",
    "    X_cgm_val = torch.tensor(X_cgm_val, dtype=torch.float32).to(CONFIG[\"device\"])\n",
    "    X_cgm_test = torch.tensor(X_cgm_test, dtype=torch.float32).to(CONFIG[\"device\"])\n",
    "    X_other_train = torch.tensor(X_other_train, dtype=torch.float32).to(CONFIG[\"device\"])\n",
    "    X_other_val = torch.tensor(X_other_val, dtype=torch.float32).to(CONFIG[\"device\"]) \n",
    "    X_other_test = torch.tensor(X_other_test, dtype=torch.float32).to(CONFIG[\"device\"])\n",
    "    X_subject_train = torch.tensor(X_subject_train, dtype=torch.long).to(CONFIG[\"device\"])\n",
    "    X_subject_val = torch.tensor(X_subject_val, dtype=torch.long).to(CONFIG[\"device\"])\n",
    "    X_subject_test = torch.tensor(X_subject_test, dtype=torch.long).to(CONFIG[\"device\"])\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32).to(CONFIG[\"device\"])\n",
    "    y_val = torch.tensor(y_val, dtype=torch.float32).to(CONFIG[\"device\"])\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32).to(CONFIG[\"device\"])\n",
    "\n",
    "    # Crear datasets\n",
    "    train_dataset = TensorDataset(X_cgm_train, X_other_train, X_subject_train, y_train)\n",
    "    val_dataset = TensorDataset(X_cgm_val, X_other_val, X_subject_val, y_val)\n",
    "    test_dataset = TensorDataset(X_cgm_test, X_other_test, X_subject_test, y_test)\n",
    "\n",
    "    # Crear dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Creación de DataLoaders completa en {elapsed_time:.2f} segundos\")\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo LSTM mejorado con embeddings de sujetos y capas adicionales.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_subjects, embedding_dim=16):\n",
    "        super().__init__()\n",
    "        # Capa de embedding para codificar IDs de sujetos\n",
    "        self.subject_embedding = nn.Embedding(num_subjects, embedding_dim)\n",
    "        \n",
    "        # LSTM bidireccional con dropout\n",
    "        self.lstm = nn.LSTM(1, 128, num_layers=2, batch_first=True, dropout=0.3)\n",
    "        \n",
    "        # Capas de normalización y regularización\n",
    "        self.batch_norm1 = nn.BatchNorm1d(128)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        # Capas fully connected\n",
    "        self.concat_dense = nn.Linear(128 + 6 + embedding_dim, 128)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(64)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.output_layer = nn.Linear(64, 1)\n",
    "\n",
    "        # Inicialización de pesos\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name and param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, cgm_input, other_input, subject_ids):\n",
    "        subject_embed = self.subject_embedding(subject_ids)\n",
    "        lstm_out, _ = self.lstm(cgm_input)\n",
    "        lstm_out = lstm_out[:, -1, :] # Tomar último estado\n",
    "        lstm_out = self.batch_norm1(lstm_out)\n",
    "        lstm_out = self.dropout1(lstm_out)\n",
    "        combined = torch.cat((lstm_out, other_input, subject_embed), dim=1)\n",
    "        dense_out = torch.relu(self.concat_dense(combined))\n",
    "        dense_out = self.batch_norm2(dense_out)\n",
    "        dense_out = self.dropout2(dense_out)\n",
    "        return self.output_layer(dense_out)\n",
    "\n",
    "class TCNTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo híbrido que combina TCN (Temporal Convolutional Network) y Transformer.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_subjects, embedding_dim=CONFIG[\"embedding_dim\"]):\n",
    "        super().__init__()\n",
    "        # Embedding de sujetos\n",
    "        self.subject_embedding = nn.Embedding(num_subjects, embedding_dim)\n",
    "        \n",
    "        # Capas convolucionales temporales\n",
    "        self.conv1 = nn.Conv1d(1, 128, kernel_size=3, padding='same')\n",
    "        self.pool1 = nn.MaxPool1d(2) \n",
    "        self.conv2 = nn.Conv1d(128, 128, kernel_size=3, padding='same')\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.conv3 = nn.Conv1d(128, 128, kernel_size=3, padding='same')\n",
    "        \n",
    "        # Capas del transformer\n",
    "        self.ln1 = nn.LayerNorm(128)\n",
    "        self.transformer = nn.TransformerEncoderLayer(128, nhead=8, dropout=0.2, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer, num_layers=1)\n",
    "        self.ln2 = nn.LayerNorm(128)\n",
    "        \n",
    "        # Pooling y capas fully connected\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.concat_dense1 = nn.Linear(128 + 6 + embedding_dim, 256)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        self.dense2 = nn.Linear(256, 128)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "        self.output_layer = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, cgm_input, other_input, subject_ids):\n",
    "        # Proceso de embedding\n",
    "        subject_embed = self.subject_embedding(subject_ids)\n",
    "        \n",
    "        # Proceso TCN\n",
    "        cgm_input = cgm_input.permute(0, 2, 1)\n",
    "        tcn_out = torch.relu(self.conv1(cgm_input))\n",
    "        tcn_out = self.pool1(tcn_out)\n",
    "        tcn_out = torch.relu(self.conv2(tcn_out))\n",
    "        tcn_out = self.pool2(tcn_out)\n",
    "        tcn_out = torch.relu(self.conv3(tcn_out))\n",
    "        \n",
    "        # Proceso Transformer\n",
    "        tcn_out = tcn_out.permute(0, 2, 1)\n",
    "        tcn_out = self.ln1(tcn_out)\n",
    "        transformer_out = self.transformer_encoder(tcn_out)\n",
    "        transformer_out = transformer_out + tcn_out # Conexión residual\n",
    "        transformer_out = self.ln2(transformer_out)\n",
    "        \n",
    "        # Proceso final\n",
    "        transformer_out = transformer_out.permute(0, 2, 1)\n",
    "        pooled = self.global_pool(transformer_out).squeeze(-1)\n",
    "        combined = torch.cat((pooled, other_input, subject_embed), dim=1)\n",
    "        dense_out = torch.relu(self.concat_dense1(combined))\n",
    "        dense_out = self.batch_norm1(dense_out)\n",
    "        dense_out = self.dropout1(dense_out)\n",
    "        dense_out = torch.relu(self.dense2(dense_out))\n",
    "        dense_out = self.batch_norm2(dense_out)\n",
    "        dense_out = self.dropout2(dense_out)\n",
    "        return self.output_layer(dense_out)\n",
    "\n",
    "class EnhancedGRU(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo GRU mejorado con regularización y embeddings de sujetos.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_subjects, embedding_dim=16):\n",
    "        super().__init__()\n",
    "        # Capa de embedding\n",
    "        self.subject_embedding = nn.Embedding(num_subjects, embedding_dim)\n",
    "        \n",
    "        # GRU simple pero efectiva\n",
    "        self.gru = nn.GRU(1, 128, num_layers=2, batch_first=True, dropout=0.2)\n",
    "        \n",
    "        # Capas de regularización\n",
    "        self.batch_norm1 = nn.BatchNorm1d(128)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        # Capas fully connected más pequeñas\n",
    "        self.concat_dense = nn.Linear(128 + 6 + embedding_dim, 64)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(64)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.output_layer = nn.Linear(64, 1)\n",
    "\n",
    "        # Inicialización de pesos\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name and param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, cgm_input, other_input, subject_ids):\n",
    "        subject_embed = self.subject_embedding(subject_ids)\n",
    "        gru_out, _ = self.gru(cgm_input)\n",
    "        gru_out = gru_out[:, -1, :] # Último estado\n",
    "        gru_out = self.batch_norm1(gru_out)\n",
    "        gru_out = self.dropout1(gru_out)\n",
    "        combined = torch.cat((gru_out, other_input, subject_embed), dim=1)\n",
    "        dense_out = torch.relu(self.concat_dense(combined))\n",
    "        dense_out = self.batch_norm2(dense_out)\n",
    "        dense_out = self.dropout2(dense_out)\n",
    "        return self.output_layer(dense_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Funciones de entrenamiento y evaluación de modelos de aprendizaje profundo.\n",
    "\"\"\"\n",
    "\n",
    "def custom_mse(y_true, y_pred, subject_ids, problem_subject=49, weight_reduction=0.5):\n",
    "    \"\"\"\n",
    "    Error cuadrático medio personalizado con penalización por sobrepredicción.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Valores objetivo reales\n",
    "        y_pred: Predicciones del modelo \n",
    "        subject_ids: IDs de los sujetos\n",
    "        problem_subject: ID del sujeto problemático (default: 49)\n",
    "        weight_reduction: Factor de reducción de peso para sujeto problemático (default: 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        Error medio ponderado con penalización\n",
    "    \"\"\"\n",
    "    error = y_true - y_pred\n",
    "    # Penaliza más las sobrepredicciones (2x) que las subpredicciones\n",
    "    overprediction_penalty = torch.where(error < 0, 2 * error**2, error**2)\n",
    "    # Asigna pesos reducidos al sujeto problemático\n",
    "    weights = torch.ones_like(y_true)\n",
    "    weights[subject_ids == problem_subject] = weight_reduction\n",
    "    return torch.mean(overprediction_penalty * weights)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, model_name=None):\n",
    "    \"\"\"\n",
    "    Entrena un modelo usando early stopping y learning rate scheduling.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo a entrenar \n",
    "        train_loader: DataLoader con datos de entrenamiento\n",
    "        val_loader: DataLoader con datos de validación\n",
    "        \n",
    "    Returns:\n",
    "        Tuple con historiales de pérdida de entrenamiento y validación\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    lr = CONFIG[\"learning_rate\"]\n",
    "    if model_name == \"LSTM\":\n",
    "        lr = 0.001  # Aumentar para el LSTM\n",
    "        \n",
    "    # Configuración del optimizador y scheduler\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, min_lr=1e-6)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    # Bucle principal de entrenamiento\n",
    "    for epoch in tqdm(range(CONFIG[\"epochs\"]), desc=\"Entrenamiento\", unit=\"época\"):\n",
    "        # Fase de entrenamiento\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for cgm_batch, other_batch, subject_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(cgm_batch, other_batch, subject_batch).squeeze()\n",
    "            loss = custom_mse(y_batch, y_pred, subject_batch)\n",
    "            loss.backward()\n",
    "            # Clip de gradientes para estabilidad\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * len(y_batch)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Fase de validación\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for cgm_batch, other_batch, subject_batch, y_batch in val_loader:\n",
    "                y_pred = model(cgm_batch, other_batch, subject_batch).squeeze()\n",
    "                loss = custom_mse(y_batch, y_pred, subject_batch)\n",
    "                val_loss += loss.item() * len(y_batch)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Actualización de learning rate y early stopping\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{CONFIG['epochs']}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= CONFIG[\"patience\"]:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    # Restaurar mejor modelo\n",
    "    model.load_state_dict(best_model_state)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Entrenamiento completo en {elapsed_time:.2f} segundos\")\n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, scaler_y):\n",
    "    \"\"\"\n",
    "    Evalúa el modelo en un conjunto de datos.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo a evaluar\n",
    "        loader: DataLoader con datos de evaluación\n",
    "        scaler_y: Scaler para desnormalizar predicciones\n",
    "        \n",
    "    Returns:\n",
    "        Tuple con predicciones y valores objetivo desnormalizados\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    model.eval()\n",
    "    predictions, targets = [], []\n",
    "    \n",
    "    # Generar predicciones\n",
    "    with torch.no_grad():\n",
    "        for cgm_batch, other_batch, subject_batch, y_batch in tqdm(loader, desc=\"Evaluando\", leave=False):\n",
    "            y_pred = model(cgm_batch, other_batch, subject_batch).squeeze()\n",
    "            predictions.append(y_pred.cpu().numpy())\n",
    "            targets.append(y_batch.cpu().numpy())\n",
    "            \n",
    "    # Procesar y desnormalizar resultados\n",
    "    predictions = np.concatenate(predictions)\n",
    "    targets = np.concatenate(targets)\n",
    "    predictions = scaler_y.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "    targets = scaler_y.inverse_transform(targets.reshape(-1, 1)).flatten()\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Evaluación completa en {elapsed_time:.2f} segundos\")\n",
    "    return predictions, targets\n",
    "\n",
    "def rule_based_prediction(X_other, scaler_other, scaler_y, target_bg=100):\n",
    "    \"\"\"\n",
    "    Genera predicciones basadas en reglas médicas estándar.\n",
    "    \n",
    "    Args:\n",
    "        X_other: Características adicionales normalizadas\n",
    "        scaler_other: Scaler para desnormalizar características\n",
    "        scaler_y: Scaler para normalizar predicciones\n",
    "        target_bg: Nivel objetivo de glucosa en sangre\n",
    "        \n",
    "    Returns:\n",
    "        Array con predicciones de dosis\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Preparación de datos\n",
    "    if hasattr(X_other, \"cpu\"):\n",
    "        X_other_np = X_other.cpu().numpy()\n",
    "    else:\n",
    "        X_other_np = X_other\n",
    "        \n",
    "    # Desnormalizar características\n",
    "    inverse_transformed = scaler_other.inverse_transform(X_other_np)\n",
    "    carb_input, bg_input, icr, isf = (inverse_transformed[:, 0],\n",
    "                                     inverse_transformed[:, 1],\n",
    "                                     inverse_transformed[:, 3],\n",
    "                                     inverse_transformed[:, 4])\n",
    "    \n",
    "    # Evitar división por cero\n",
    "    icr = np.where(icr == 0, 1e-6, icr)\n",
    "    isf = np.where(isf == 0, 1e-6, isf)\n",
    "    \n",
    "    # Cálculo de componentes de la dosis\n",
    "    carb_component = np.divide(carb_input, icr, out=np.zeros_like(carb_input), where=icr!=0)\n",
    "    bg_component = np.divide(bg_input - target_bg, isf, out=np.zeros_like(bg_input), where=isf!=0)\n",
    "    \n",
    "    # Predicción final\n",
    "    prediction = carb_component + bg_component\n",
    "    prediction = np.clip(prediction, 0, CONFIG[\"cap_normal\"])\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Predicción basada en reglas completa en {elapsed_time:.2f} segundos\")\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(lstm_losses, tcn_losses, gru_losses):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(lstm_losses[0], label='Train LSTM')\n",
    "    plt.plot(lstm_losses[1], label='Val LSTM')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Custom MSE Loss')\n",
    "    plt.legend()\n",
    "    plt.title('LSTM Training History')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(tcn_losses[0], label='Train TCN')\n",
    "    plt.plot(tcn_losses[1], label='Val TCN')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Custom MSE Loss')\n",
    "    plt.legend()\n",
    "    plt.title('TCN Training History')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(gru_losses[0], label='Train GRU')\n",
    "    plt.plot(gru_losses[1], label='Val GRU')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Custom MSE Loss')\n",
    "    plt.legend()\n",
    "    plt.title('GRU Training History')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluation(y_test, y_pred_lstm, y_pred_tcn, y_pred_gru, y_rule, subject_test, scaler_y):\n",
    "    start_time = time.time()\n",
    "    y_test_denorm = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(y_test_denorm, y_pred_lstm, label='LSTM', alpha=0.5, color='blue')\n",
    "    plt.scatter(y_test_denorm, y_pred_tcn, label='TCN', alpha=0.5, color='green')\n",
    "    plt.scatter(y_test_denorm, y_pred_gru, label='GRU', alpha=0.5, color='red')\n",
    "    plt.scatter(y_test_denorm, y_rule, label='Rules', alpha=0.5, color='orange')\n",
    "    plt.plot([0, 15], [0, 15], 'r--')\n",
    "    plt.xlabel('Real Dose (units)')\n",
    "    plt.ylabel('Predicted Dose (units)')\n",
    "    plt.legend()\n",
    "    plt.title('Predictions vs Real')\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.hist(y_test_denorm - y_pred_lstm, bins=20, label='LSTM', alpha=0.5, color='blue')\n",
    "    plt.hist(y_test_denorm - y_pred_tcn, bins=20, label='TCN', alpha=0.5, color='green')\n",
    "    plt.hist(y_test_denorm - y_pred_gru, bins=20, label='GRU', alpha=0.5, color='red')\n",
    "    plt.hist(y_test_denorm - y_rule, bins=20, label='Rules', alpha=0.5, color='orange')\n",
    "    plt.xlabel('Residual (units)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.title('Residual Distribution')\n",
    "\n",
    "    test_subjects = np.unique(subject_test)\n",
    "    mae_lstm, mae_tcn, mae_gru, mae_rule = [], [], [], []\n",
    "    for sid in test_subjects:\n",
    "        mask = subject_test == sid\n",
    "        if np.sum(mask) > 0:\n",
    "            mae_lstm.append(mean_absolute_error(y_test_denorm[mask], y_pred_lstm[mask]))\n",
    "            mae_tcn.append(mean_absolute_error(y_test_denorm[mask], y_pred_tcn[mask]))\n",
    "            mae_gru.append(mean_absolute_error(y_test_denorm[mask], y_pred_gru[mask]))\n",
    "            mae_rule.append(mean_absolute_error(y_test_denorm[mask], y_rule[mask]))\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.bar(np.arange(len(test_subjects)) - 0.3, mae_lstm, width=0.2, label='LSTM', color='blue')\n",
    "    plt.bar(np.arange(len(test_subjects)) - 0.1, mae_tcn, width=0.2, label='TCN', color='green')\n",
    "    plt.bar(np.arange(len(test_subjects)) + 0.1, mae_gru, width=0.2, label='GRU', color='red')\n",
    "    plt.bar(np.arange(len(test_subjects)) + 0.3, mae_rule, width=0.2, label='Rules', color='orange')\n",
    "    plt.xlabel('Subject')\n",
    "    plt.ylabel('MAE (units)')\n",
    "    plt.xticks(np.arange(len(test_subjects)), test_subjects)\n",
    "    plt.legend()\n",
    "    plt.title('MAE by Subject')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Visualización completa en {elapsed_time:.2f} segundos\")\n",
    "\n",
    "\"\"\"\n",
    "Módulo principal de ejecución del modelo de predicción de insulina.\n",
    "Este módulo coordina el preprocesamiento de datos, entrenamiento y evaluación \n",
    "de múltiples modelos de deep learning para predecir dosis de insulina.\n",
    "\n",
    "Modelos implementados:\n",
    "- LSTM mejorado con embeddings de sujetos\n",
    "- TCN híbrido con transformer \n",
    "- GRU mejorado con regularización\n",
    "- Modelo basado en reglas médicas\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesa los datos y aplica normalización\n",
    "X_cgm, X_other, X_subject, y, df_final, scaler_cgm, scaler_other, scaler_y = preprocess_data(CONFIG[\"data_path\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# División estratificada de datos en conjuntos de train/val/test manteniendo la distribución de sujetos\n",
    "(X_cgm_train, X_cgm_val, X_cgm_test,\n",
    "    X_other_train, X_other_val, X_other_test,\n",
    "    X_subject_train, X_subject_val, X_subject_test,\n",
    "    y_train, y_val, y_test, subject_test) = split_data(X_cgm, X_other, X_subject, y, df_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea los DataLoaders para entrenamiento eficiente con batches\n",
    "train_loader, val_loader, test_loader = create_dataloaders(X_cgm_train, X_cgm_val, X_cgm_test,\n",
    "                                                            X_other_train, X_other_val, X_other_test,\n",
    "                                                            X_subject_train, X_subject_val, X_subject_test,\n",
    "                                                            y_train, y_val, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa los tres modelos de deep learning\n",
    "num_subjects = len(df_final['subject_id'].unique())\n",
    "models = {\n",
    "    \"LSTM\": EnhancedLSTM(num_subjects).to(CONFIG[\"device\"]),\n",
    "    \"TCN\": TCNTransformer(num_subjects).to(CONFIG[\"device\"]),\n",
    "    \"GRU\": EnhancedGRU(num_subjects).to(CONFIG[\"device\"])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrena cada modelo usando early stopping y learning rate scheduling\n",
    "losses = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nEntrenando {name}...\")\n",
    "    losses[name] = train_model(model, train_loader, val_loader, model_name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalúa los modelos en el conjunto de test\n",
    "y_pred = {name: evaluate_model(model, test_loader, scaler_y)[0] for name, model in models.items()}\n",
    "y_rule = rule_based_prediction(X_other_test, scaler_other, scaler_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula y muestra métricas globales para cada modelo\n",
    "for name in models:\n",
    "    mae = mean_absolute_error(scaler_y.inverse_transform(y_test.reshape(-1, 1)), y_pred[name])\n",
    "    rmse = np.sqrt(mean_squared_error(scaler_y.inverse_transform(y_test.reshape(-1, 1)), y_pred[name]))\n",
    "    r2 = r2_score(scaler_y.inverse_transform(y_test.reshape(-1, 1)), y_pred[name])\n",
    "    print(f\"{name} - MAE: {mae:.2f}, RMSE: {rmse:.2f}, R²: {r2:.2f}\")\n",
    "\n",
    "# Métricas para el modelo basado en reglas\n",
    "mae_rule = mean_absolute_error(scaler_y.inverse_transform(y_test.reshape(-1, 1)), y_rule)\n",
    "rmse_rule = np.sqrt(mean_squared_error(scaler_y.inverse_transform(y_test.reshape(-1, 1)), y_rule))\n",
    "r2_rule = r2_score(scaler_y.inverse_transform(y_test.reshape(-1, 1)), y_rule)\n",
    "print(f\"Rules - MAE: {mae_rule:.2f}, RMSE: {rmse_rule:.2f}, R²: {r2_rule:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera visualizaciones comparativas del entrenamiento y predicciones\n",
    "plot_training_history(losses[\"LSTM\"], losses[\"TCN\"], losses[\"GRU\"])\n",
    "plot_evaluation(y_test, y_pred[\"LSTM\"], y_pred[\"TCN\"], y_pred[\"GRU\"], y_rule, subject_test, scaler_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analiza el rendimiento individual por sujeto\n",
    "print(\"\\nRendimiento por sujeto:\")\n",
    "for subject_id in np.unique(subject_test):\n",
    "    mask = subject_test == subject_id\n",
    "    if np.sum(mask) > 0:\n",
    "        y_test_sub = scaler_y.inverse_transform(y_test[mask].reshape(-1, 1)).flatten()\n",
    "        print(f\"Sujeto {subject_id}: \", end=\"\")\n",
    "        for name in models:\n",
    "            mae_sub = mean_absolute_error(y_test_sub, y_pred[name][mask])\n",
    "            print(f\"{name} MAE={mae_sub:.2f}, \", end=\"\")\n",
    "        mae_rule_sub = mean_absolute_error(y_test_sub, y_rule[mask])\n",
    "        print(f\"Rules MAE={mae_rule_sub:.2f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
