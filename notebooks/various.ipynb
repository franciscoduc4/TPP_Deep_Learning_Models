{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos Varios\n",
    "\n",
    "En este notebook están los modelos:\n",
    "\n",
    "+ CNN (Convolutional Neural Network)\n",
    "+ Transformer\n",
    "+ TCN (Temporal Convolutional Network)\n",
    "+ GRU (Gated Recurrent Unit)\n",
    "+ Wavenet\n",
    "+ Tanmet\n",
    "+ Attention-Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install --upgrade pip\n",
    "%pip install polars numpy scikit-learn matplotlib joblib openpyxl fastexcel tensorflow tensorflow.keras\n",
    "\n",
    "# For TensorFlow on Mac, you need to install tensorflow-macos\n",
    "%pip install tensorflow-macos tensorflow-metal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.optimize import minimize\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, Input, Concatenate, BatchNormalization,\n",
    "    Conv1D, MaxPooling1D, LayerNormalization, MultiHeadAttention,\n",
    "    Add, GlobalAveragePooling1D, GRU, Activation, SimpleRNN, Bidirectional, TimeDistributed\n",
    ")\n",
    "from keras.saving import register_keras_serializable\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from datetime import timedelta\n",
    "import openpyxl\n",
    "\n",
    "# Configuración de Matplotlib para evitar errores con Tkinter\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de la ruta del proyecto\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "SUBJECTS_RELATIVE_PATH = \"data/Subjects\"\n",
    "SUBJECTS_PATH = os.path.join(PROJECT_ROOT, SUBJECTS_RELATIVE_PATH)\n",
    "\n",
    "# Crear directorios para resultados\n",
    "FIGURES_DIR = os.path.join(PROJECT_ROOT, \"figures\", \"various_models\")\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "MODELS_DIR = os.path.join(PROJECT_ROOT, \"models\", \"keras\")\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "subject_files = [f for f in os.listdir(SUBJECTS_PATH) if f.startswith(\"Subject\") and f.endswith(\".xlsx\")]\n",
    "print(f\"Total sujetos: {len(subject_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento y Procesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cgm_window(bolus_time, cgm_df: pl.DataFrame, window_hours: int = 2) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Obtiene la ventana de datos CGM para un tiempo de bolo específico.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    bolus_time : datetime\n",
    "        Tiempo del bolo de insulina\n",
    "    cgm_df : pl.DataFrame\n",
    "        DataFrame con datos CGM\n",
    "    window_hours : int, opcional\n",
    "        Horas de la ventana de datos (default: 2)\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    np.ndarray\n",
    "        Ventana de datos CGM o None si no hay suficientes datos\n",
    "    \"\"\"\n",
    "    window_start = bolus_time - timedelta(hours=window_hours)\n",
    "    window = cgm_df.filter(\n",
    "        (pl.col(\"date\") >= window_start) & (pl.col(\"date\") <= bolus_time)\n",
    "    ).sort(\"date\").tail(24)\n",
    "    \n",
    "    if window.height < 24:\n",
    "        return None\n",
    "    return window.get_column(\"mg/dl\").to_numpy()\n",
    "\n",
    "def calculate_iob(bolus_time, basal_df: pl.DataFrame, half_life_hours: float = 4.0) -> float:\n",
    "    \"\"\"\n",
    "    Calcula la insulina activa en el cuerpo (IOB).\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    bolus_time : datetime\n",
    "        Tiempo del bolo de insulina\n",
    "    basal_df : pl.DataFrame\n",
    "        DataFrame con datos de insulina basal\n",
    "    half_life_hours : float, opcional\n",
    "        Vida media de la insulina en horas (default: 4.0)\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    float\n",
    "        Cantidad de insulina activa\n",
    "    \"\"\"\n",
    "    if basal_df is None or basal_df.is_empty():\n",
    "        return 0.0\n",
    "    \n",
    "    iob = 0.0\n",
    "    for row in basal_df.iter_rows(named=True):\n",
    "        start_time = row[\"date\"]\n",
    "        duration_hours = row[\"duration\"] / (1000 * 3600)\n",
    "        end_time = start_time + timedelta(hours=duration_hours)\n",
    "        rate = row[\"rate\"] if row[\"rate\"] is not None else 0.9\n",
    "        \n",
    "        if start_time <= bolus_time <= end_time:\n",
    "            time_since_start = (bolus_time - start_time).total_seconds() / 3600\n",
    "            remaining = rate * (1 - (time_since_start / half_life_hours))\n",
    "            iob += max(0.0, remaining)\n",
    "    return iob\n",
    "\n",
    "def process_subject(subject_path: str, idx: int) -> list:\n",
    "    \"\"\"\n",
    "    Procesa los datos de un sujeto.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    subject_path : str\n",
    "        Ruta al archivo del sujeto\n",
    "    idx : int\n",
    "        Índice del sujeto\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    list\n",
    "        Lista de diccionarios con características procesadas\n",
    "    \"\"\"\n",
    "    print(f\"Procesando {os.path.basename(subject_path)} ({idx+1}/{len(subject_files)})...\")\n",
    "    \n",
    "    try:\n",
    "        cgm_df = pl.read_excel(subject_path, sheet_name=\"CGM\")\n",
    "        bolus_df = pl.read_excel(subject_path, sheet_name=\"Bolus\")\n",
    "        try:\n",
    "            basal_df = pl.read_excel(subject_path, sheet_name=\"Basal\")\n",
    "        except Exception:\n",
    "            basal_df = None\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar {os.path.basename(subject_path)}: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Conversión de fechas\n",
    "    cgm_df = cgm_df.with_columns(pl.col(\"date\").cast(pl.Datetime))\n",
    "    bolus_df = bolus_df.with_columns(pl.col(\"date\").cast(pl.Datetime))\n",
    "    if basal_df is not None:\n",
    "        basal_df = basal_df.with_columns(pl.col(\"date\").cast(pl.Datetime))\n",
    "    \n",
    "    cgm_df = cgm_df.sort(\"date\")\n",
    "\n",
    "    processed_data = []\n",
    "    for row in bolus_df.iter_rows(named=True):\n",
    "        bolus_time = row[\"date\"]\n",
    "        cgm_window = get_cgm_window(bolus_time, cgm_df)\n",
    "        \n",
    "        if cgm_window is not None:\n",
    "            iob = calculate_iob(bolus_time, basal_df)\n",
    "            hour_of_day = bolus_time.hour / 23.0\n",
    "            bg_input = row[\"bgInput\"] if row[\"bgInput\"] is not None else cgm_window[-1]\n",
    "            normal = row[\"normal\"] if row[\"normal\"] is not None else 0.0\n",
    "            \n",
    "            # Cálculo del factor de sensibilidad personalizado\n",
    "            isf_custom = 50.0\n",
    "            if normal > 0 and bg_input > 100:\n",
    "                isf_custom = (bg_input - 100) / normal\n",
    "            \n",
    "            features = {\n",
    "                'subject_id': idx,\n",
    "                'cgm_window': cgm_window,\n",
    "                'carbInput': row[\"carbInput\"] if row[\"carbInput\"] is not None else 0.0,\n",
    "                'bgInput': bg_input,\n",
    "                'insulinCarbRatio': row[\"insulinCarbRatio\"] if row[\"insulinCarbRatio\"] is not None else 10.0,\n",
    "                'insulinSensitivityFactor': isf_custom,\n",
    "                'insulinOnBoard': iob,\n",
    "                'hour_of_day': hour_of_day,\n",
    "                'normal': normal\n",
    "            }\n",
    "            processed_data.append(features)\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# Ejecución en paralelo\n",
    "all_processed_data = Parallel(n_jobs=-1)(\n",
    "    delayed(process_subject)(\n",
    "        os.path.join(SUBJECTS_PATH, f), \n",
    "        idx\n",
    "    ) for idx, f in enumerate(subject_files)\n",
    ")\n",
    "\n",
    "all_processed_data = [item for sublist in all_processed_data for item in sublist]\n",
    "\n",
    "# Conversión a DataFrame\n",
    "df_processed = pl.DataFrame(all_processed_data)\n",
    "print(\"Muestra de datos procesados combinados:\")\n",
    "print(df_processed.head())\n",
    "print(f\"Total muestras: {len(df_processed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División de Ventana CGM y Valores Nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir ventana CGM y otras características\n",
    "cgm_columns = [f'cgm_{i}' for i in range(24)]\n",
    "df_cgm = pl.DataFrame({\n",
    "    col: [row['cgm_window'][i] for row in all_processed_data]\n",
    "    for i, col in enumerate(cgm_columns)\n",
    "}, schema={col: pl.Float64 for col in cgm_columns})\n",
    "\n",
    "# Combinar con otras características\n",
    "df_processed = pl.concat([\n",
    "    df_cgm,\n",
    "    df_processed.drop('cgm_window')\n",
    "], how=\"horizontal\")\n",
    "\n",
    "# Verificar valores nulos\n",
    "print(\"Verificación de valores nulos en df_processed:\")\n",
    "print(df_processed.null_count())\n",
    "df_processed = df_processed.drop_nulls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar características\n",
    "scaler_cgm = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_other = StandardScaler()\n",
    "\n",
    "# Normalizar CGM\n",
    "x_cgm = scaler_cgm.fit_transform(df_processed.select(cgm_columns).to_numpy())\n",
    "x_cgm = x_cgm.reshape(x_cgm.shape[0], x_cgm.shape[1], 1)\n",
    "\n",
    "# Normalizar otras características (incluyendo hour_of_day)\n",
    "other_features = ['carbInput', 'bgInput', 'insulinOnBoard', 'insulinCarbRatio', \n",
    "                  'insulinSensitivityFactor', 'subject_id', 'hour_of_day']\n",
    "x_other = scaler_other.fit_transform(df_processed.select(other_features).to_numpy())\n",
    "\n",
    "# Etiquetas\n",
    "y = df_processed.get_column('normal').to_numpy()\n",
    "\n",
    "# Verificar NaN\n",
    "print(\"NaN en x_cgm:\", np.isnan(x_cgm).sum())\n",
    "print(\"NaN en x_other:\", np.isnan(x_other).sum())\n",
    "print(\"NaN en y:\", np.isnan(y).sum())\n",
    "if np.isnan(x_cgm).sum() > 0 or np.isnan(x_other).sum() > 0 or np.isnan(y).sum() > 0:\n",
    "    raise ValueError(\"Valores NaN detectados en x_cgm, x_other o y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División por Sujeto de los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# División por sujeto\n",
    "subject_ids = df_processed.get_column('subject_id').unique().to_numpy()\n",
    "train_subjects, temp_subjects = train_test_split(subject_ids, test_size=0.2, random_state=42)\n",
    "val_subjects, test_subjects = train_test_split(temp_subjects, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de Máscaras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear máscaras\n",
    "train_mask = df_processed.get_column('subject_id').is_in(train_subjects).to_numpy()\n",
    "val_mask = df_processed.get_column('subject_id').is_in(val_subjects).to_numpy()\n",
    "test_mask = df_processed.get_column('subject_id').is_in(test_subjects).to_numpy()\n",
    "\n",
    "x_cgm_train, x_cgm_val, x_cgm_test = x_cgm[train_mask], x_cgm[val_mask], x_cgm[test_mask]\n",
    "x_other_train, x_other_val, x_other_test = x_other[train_mask], x_other[val_mask], x_other[test_mask]\n",
    "y_train, y_val, y_test = y[train_mask], y[val_mask], y[test_mask]\n",
    "subject_test = df_processed.filter(pl.col('subject_id').is_in(test_subjects)).get_column('subject_id').to_numpy()\n",
    "\n",
    "print(f\"Entrenamiento CGM: {x_cgm_train.shape}, Validación CGM: {x_cgm_val.shape}, Prueba CGM: {x_cgm_test.shape}\")\n",
    "print(f\"Entrenamiento Otros: {x_other_train.shape}, Validación Otros: {x_other_val.shape}, Prueba Otros: {x_other_test.shape}\")\n",
    "print(f\"Sujetos de prueba: {test_subjects}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Model configuration parameters\"\"\"\n",
    "\n",
    "TCN_CONFIG = {\n",
    "    'filters': [32, 64, 128],\n",
    "    'kernel_size': 3,\n",
    "    'dilations': [1, 2, 4, 8, 16],\n",
    "    'dropout_rate': [0.2, 0.1],\n",
    "    'activation': 'gelu',\n",
    "    'epsilon': 1e-6,\n",
    "    'use_layer_norm': True,\n",
    "    'use_weight_norm': True,\n",
    "    'use_spatial_dropout': True,\n",
    "    'residual_dropout': 0.1\n",
    "}\n",
    "\n",
    "TRANSFORMER_CONFIG = {\n",
    "    'num_heads': 8,\n",
    "    'key_dim': 64,\n",
    "    'num_layers': 4,\n",
    "    'ff_dim': 256,\n",
    "    'dropout_rate': 0.1,\n",
    "    'epsilon': 1e-6,\n",
    "    'activation': 'gelu',\n",
    "    'use_relative_pos': True,\n",
    "    'max_position': 32,\n",
    "    'head_size': 32,\n",
    "    'use_bias': True,\n",
    "    'prenorm': True\n",
    "}\n",
    "\n",
    "WAVENET_CONFIG = {\n",
    "    'filters': [32, 64, 128],\n",
    "    'kernel_size': 3,\n",
    "    'dilations': [1, 2, 4, 8, 16],\n",
    "    'dropout_rate': 0.2,\n",
    "    'use_gating': True,\n",
    "    'use_skip_scale': True,\n",
    "    'use_residual_scale': 0.1,\n",
    "    'activation': 'elu'\n",
    "}\n",
    "\n",
    "TABNET_CONFIG = {\n",
    "    'feature_dim': 128,\n",
    "    'output_dim': 64,\n",
    "    'num_decision_steps': 8,\n",
    "    'relaxation_factor': 1.5,\n",
    "    'sparsity_coefficient': 1e-4,\n",
    "    'batch_momentum': 0.98,\n",
    "    'virtual_batch_size': 128,\n",
    "    'num_attention_heads': 4,\n",
    "    'attention_dropout': 0.2,\n",
    "    'feature_dropout': 0.1\n",
    "}\n",
    "\n",
    "ATTENTION_CONFIG = {\n",
    "    'num_heads': 8,\n",
    "    'key_dim': 64,\n",
    "    'num_layers': 4,\n",
    "    'ff_dim': 256,\n",
    "    'dropout_rate': 0.1,\n",
    "    'use_relative_attention': True,\n",
    "    'max_relative_position': 32,\n",
    "    'activation': 'gelu',\n",
    "    'head_size': 32,\n",
    "    'use_mask_future': False,\n",
    "    'layer_dropout': 0.1\n",
    "}\n",
    "\n",
    "GRU_CONFIG = {\n",
    "    'hidden_units': [64, 128, 256],\n",
    "    'dropout_rate': 0.3,\n",
    "    'recurrent_dropout': 0.2,\n",
    "    'epsilon': 1e-5,\n",
    "    'attention_heads': 4\n",
    "}\n",
    "\n",
    "CNN_CONFIG = {\n",
    "    'filters': [32, 64, 128, 256],\n",
    "    'kernel_size': 3,\n",
    "    'pool_size': 2,\n",
    "    'dropout_rate': 0.2,\n",
    "    'use_se_block': True,\n",
    "    'se_ratio': 16,\n",
    "    'use_layer_norm': True,\n",
    "    'activation': 'gelu',\n",
    "    'dilation_rates': [1, 2, 4]\n",
    "}\n",
    "\n",
    "GRU_CONFIG = {\n",
    "    'hidden_units': [64, 128, 256],\n",
    "    'dropout_rate': 0.3,\n",
    "    'recurrent_dropout': 0.2,\n",
    "    'epsilon': 1e-5,\n",
    "    'attention_heads': 4\n",
    "}\n",
    "\n",
    "RNN_CONFIG = {\n",
    "    'hidden_units': [64, 32],\n",
    "    'dropout_rate': 0.2,\n",
    "    'recurrent_dropout': 0.1,\n",
    "    'bidirectional': True,\n",
    "    'epsilon': 1e-6,\n",
    "    'use_time_distributed': True,\n",
    "    'activation': 'relu'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention-Only Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable()\n",
    "class RelativePositionEncoding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Codificación de posición relativa para mejorar la atención temporal.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_position: int, depth: int, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_position = max_position\n",
    "        self.depth = depth\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.rel_embeddings = self.add_weight(\n",
    "            name=\"rel_embeddings\",\n",
    "            shape=[2 * self.max_position - 1, self.depth],\n",
    "            initializer=\"glorot_uniform\"\n",
    "        )\n",
    "        \n",
    "    def call(self, length):\n",
    "        pos_emb = tf.gather(\n",
    "            self.rel_embeddings,\n",
    "            tf.range(length)[:, tf.newaxis] - tf.range(length)[tf.newaxis, :] + self.max_position - 1\n",
    "        )\n",
    "        return pos_emb\n",
    "\n",
    "def create_attention_block(x: tf.Tensor, num_heads: int, key_dim: int, \n",
    "                         ff_dim: int, dropout_rate: float, training: bool = None) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Crea un bloque de atención mejorado con posición relativa y gating.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    x : tf.Tensor\n",
    "        Tensor de entrada\n",
    "    num_heads : int\n",
    "        Número de cabezas de atención\n",
    "    key_dim : int\n",
    "        Dimensión de la clave\n",
    "    ff_dim : int\n",
    "        Dimensión de la red feed-forward\n",
    "    dropout_rate : float\n",
    "        Tasa de dropout\n",
    "    training : bool\n",
    "        Indica si está en modo entrenamiento\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    tf.Tensor\n",
    "        Tensor procesado\n",
    "    \"\"\"\n",
    "    # Relative position encoding\n",
    "    if ATTENTION_CONFIG['use_relative_attention']:\n",
    "        pos_encoding = RelativePositionEncoding(\n",
    "            ATTENTION_CONFIG['max_relative_position'],\n",
    "            key_dim\n",
    "        )(tf.shape(x)[1])\n",
    "        \n",
    "        attention_output = MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=key_dim,\n",
    "            value_dim=ATTENTION_CONFIG['head_size']\n",
    "        )(x, x, attention_bias=pos_encoding)\n",
    "    else:\n",
    "        attention_output = MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=key_dim\n",
    "        )(x, x)\n",
    "    \n",
    "    # Gating mechanism\n",
    "    gate = tf.keras.layers.Dense(attention_output.shape[-1], activation='sigmoid')(x)\n",
    "    attention_output = gate * attention_output\n",
    "    \n",
    "    attention_output = Dropout(dropout_rate)(attention_output, training=training)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x + attention_output)\n",
    "    \n",
    "    # Enhanced feed-forward network with GLU\n",
    "    ffn = Dense(ff_dim)(x)\n",
    "    ffn_gate = Dense(ff_dim, activation='sigmoid')(x)\n",
    "    ffn = ffn * ffn_gate\n",
    "    ffn = Dense(x.shape[-1])(ffn)\n",
    "    ffn = Dropout(dropout_rate)(ffn, training=training)\n",
    "    \n",
    "    return LayerNormalization(epsilon=1e-6)(x + ffn)\n",
    "\n",
    "def create_attention_model(cgm_shape: tuple, other_features_shape: tuple) -> Model:\n",
    "    \"\"\"\n",
    "    Crea un modelo basado únicamente en mecanismos de atención.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    cgm_shape : tuple\n",
    "        Forma de los datos CGM (samples, timesteps, features)\n",
    "    other_features_shape : tuple\n",
    "        Forma de otras características (samples, features)\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    Model\n",
    "        Modelo de atención compilado\n",
    "    \"\"\"\n",
    "    cgm_input = Input(shape=cgm_shape[1:])\n",
    "    other_input = Input(shape=(other_features_shape[1],))\n",
    "    \n",
    "    # Initial projection\n",
    "    x = Dense(ATTENTION_CONFIG['key_dim'] * ATTENTION_CONFIG['num_heads'])(cgm_input)\n",
    "    \n",
    "    # Stochastic depth (layer dropout)\n",
    "    survive_rates = tf.linspace(1.0, 0.5, ATTENTION_CONFIG['num_layers'])\n",
    "    \n",
    "    # Stack attention blocks with stochastic depth\n",
    "    for i in range(ATTENTION_CONFIG['num_layers']):\n",
    "        if tf.random.uniform([]) < survive_rates[i]:\n",
    "            x = create_attention_block(\n",
    "                x,\n",
    "                ATTENTION_CONFIG['num_heads'],\n",
    "                ATTENTION_CONFIG['key_dim'],\n",
    "                ATTENTION_CONFIG['ff_dim'],\n",
    "                ATTENTION_CONFIG['dropout_rate']\n",
    "            )\n",
    "    \n",
    "    # Global context\n",
    "    attention_pooled = GlobalAveragePooling1D()(x)\n",
    "    max_pooled = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "    x = Concatenate()([attention_pooled, max_pooled])\n",
    "    \n",
    "    # Combine with other features\n",
    "    x = Concatenate()([x, other_input])\n",
    "    \n",
    "    # Final MLP with residual\n",
    "    skip = x\n",
    "    x = Dense(128, activation=ATTENTION_CONFIG['activation'])(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = Dropout(ATTENTION_CONFIG['dropout_rate'])(x)\n",
    "    x = Dense(128, activation=ATTENTION_CONFIG['activation'])(x)\n",
    "    if skip.shape[-1] == 128:\n",
    "        x = Add()([x, skip])\n",
    "    \n",
    "    output = Dense(1)(x)\n",
    "    \n",
    "    return Model(inputs=[cgm_input, other_input], outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable()\n",
    "class SqueezeExcitationBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Bloque Squeeze-and-Excitation como capa personalizada.\n",
    "    \"\"\"\n",
    "    def __init__(self, filters: int, se_ratio: int = 16, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.se_ratio = se_ratio\n",
    "        \n",
    "        # Define layers\n",
    "        self.gap = GlobalAveragePooling1D()\n",
    "        self.dense1 = Dense(filters // se_ratio, activation='gelu')\n",
    "        self.dense2 = Dense(filters, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Squeeze\n",
    "        x = self.gap(inputs)\n",
    "        \n",
    "        # Excitation\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        \n",
    "        # Reshape for broadcasting\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        \n",
    "        # Scale\n",
    "        return inputs * x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"filters\": self.filters,\n",
    "            \"se_ratio\": self.se_ratio\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Update the residual block to use the new layer\n",
    "def create_residual_block(x, filters, dilation_rate=1):\n",
    "    \"\"\"\n",
    "    Crea un bloque residual mejorado con dilated convolutions y SE.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    x : tensor\n",
    "        Tensor de entrada\n",
    "    filters : int\n",
    "        Número de filtros\n",
    "    dilation_rate : int\n",
    "        Tasa de dilatación para las convoluciones\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    tensor\n",
    "        Tensor procesado\n",
    "    \"\"\"\n",
    "    skip = x\n",
    "    \n",
    "    # Convolution path\n",
    "    x = Conv1D(\n",
    "        filters=filters,\n",
    "        kernel_size=CNN_CONFIG['kernel_size'],\n",
    "        padding='same',\n",
    "        dilation_rate=dilation_rate\n",
    "    )(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Activation(CNN_CONFIG['activation'])(x)\n",
    "    \n",
    "    # Squeeze-and-Excitation\n",
    "    if CNN_CONFIG['use_se_block']:\n",
    "        x = SqueezeExcitationBlock(filters, CNN_CONFIG['se_ratio'])(x)\n",
    "    \n",
    "    # Project residual if needed\n",
    "    if skip.shape[-1] != filters:\n",
    "        skip = Conv1D(filters, 1, padding='same')(skip)\n",
    "    \n",
    "    return Add()([x, skip])\n",
    "\n",
    "def create_cnn_model(cgm_shape: tuple, other_features_shape: tuple) -> Model:\n",
    "    \"\"\"\n",
    "    Crea un modelo CNN (Convolutional Neural Network) con entrada dual para datos CGM y otras características.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    cgm_shape : tuple\n",
    "        Forma de los datos CGM (samples, timesteps, features)\n",
    "    other_features_shape : tuple\n",
    "        Forma de otras características (samples, features)\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    Model\n",
    "        Modelo CNN compilado\n",
    "    \"\"\"\n",
    "    # Entrada CGM\n",
    "    cgm_input = Input(shape=cgm_shape[1:], name='cgm_input')\n",
    "    \n",
    "    # Proyección inicial\n",
    "    x = Conv1D(CNN_CONFIG['filters'][0], 1, padding='same')(cgm_input)\n",
    "    x = LayerNormalization()(x) if CNN_CONFIG['use_layer_norm'] else BatchNormalization()(x)\n",
    "    \n",
    "    # Bloques residuales con different dilation rates\n",
    "    for filters in CNN_CONFIG['filters']:\n",
    "        for dilation_rate in CNN_CONFIG['dilation_rates']:\n",
    "            x = create_residual_block(x, filters, dilation_rate)\n",
    "        x = MaxPooling1D(pool_size=CNN_CONFIG['pool_size'])(x)\n",
    "    \n",
    "    # Pooling global con concat de max y average\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "    x = Concatenate()([avg_pool, max_pool])\n",
    "    \n",
    "    # Entrada de otras características\n",
    "    other_input = Input(shape=(other_features_shape[1],), name='other_input')\n",
    "    \n",
    "    # Combinar características\n",
    "    combined = Concatenate()([x, other_input])\n",
    "    \n",
    "    # Capas densas con residual connections\n",
    "    skip = combined\n",
    "    dense = Dense(256, activation=CNN_CONFIG['activation'])(combined)\n",
    "    dense = LayerNormalization()(dense) if CNN_CONFIG['use_layer_norm'] else BatchNormalization()(dense)\n",
    "    dense = Dropout(CNN_CONFIG['dropout_rate'])(dense)\n",
    "    dense = Dense(256, activation=CNN_CONFIG['activation'])(dense)\n",
    "    if skip.shape[-1] == 256:\n",
    "        dense = Add()([dense, skip])\n",
    "    \n",
    "    # Final layers\n",
    "    dense = Dense(128, activation=CNN_CONFIG['activation'])(dense)\n",
    "    dense = LayerNormalization()(dense) if CNN_CONFIG['use_layer_norm'] else BatchNormalization()(dense)\n",
    "    dense = Dropout(CNN_CONFIG['dropout_rate'] / 2)(dense)\n",
    "    \n",
    "    output = Dense(1)(dense)\n",
    "    \n",
    "    return Model(inputs=[cgm_input, other_input], outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gru_attention_block(x, units, num_heads=4):\n",
    "    \"\"\"\n",
    "    Crea un bloque GRU con self-attention y conexiones residuales.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    x : tensor\n",
    "        Tensor de entrada\n",
    "    units : int\n",
    "        Número de unidades GRU\n",
    "    num_heads : int\n",
    "        Número de cabezas de atención\n",
    "    \"\"\"\n",
    "    # GRU con skip connection\n",
    "    skip1 = x\n",
    "    x = GRU(\n",
    "        units,\n",
    "        return_sequences=True,\n",
    "        dropout=GRU_CONFIG['dropout_rate'],\n",
    "        recurrent_dropout=GRU_CONFIG['recurrent_dropout']\n",
    "    )(x)\n",
    "    x = LayerNormalization(epsilon=GRU_CONFIG['epsilon'])(x)\n",
    "    if skip1.shape[-1] == units:\n",
    "        x = Add()([x, skip1])\n",
    "    \n",
    "    # Multi-head attention\n",
    "    skip2 = x\n",
    "    attention_output = MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=units // num_heads\n",
    "    )(x, x)\n",
    "    x = LayerNormalization(epsilon=GRU_CONFIG['epsilon'])(attention_output + skip2)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def create_gru_model(cgm_shape: tuple, other_features_shape: tuple) -> Model:\n",
    "    \"\"\"\n",
    "    Crea un modelo GRU avanzado con self-attention y conexiones residuales.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    cgm_shape : tuple\n",
    "        Forma de los datos CGM (samples, timesteps, features)\n",
    "    other_features_shape : tuple\n",
    "        Forma de otras características (samples, features)\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    Model\n",
    "        Modelo GRU compilado\n",
    "    \"\"\"\n",
    "    # Entradas\n",
    "    cgm_input = Input(shape=cgm_shape[1:])\n",
    "    other_input = Input(shape=(other_features_shape[1],))\n",
    "    \n",
    "    # Proyección inicial\n",
    "    x = Dense(GRU_CONFIG['hidden_units'][0])(cgm_input)\n",
    "    x = LayerNormalization(epsilon=GRU_CONFIG['epsilon'])(x)\n",
    "    \n",
    "    # Bloques GRU con attention\n",
    "    for units in GRU_CONFIG['hidden_units']:\n",
    "        x = create_gru_attention_block(x, units)\n",
    "    \n",
    "    # Pooling global\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Combinar con otras características\n",
    "    combined = Concatenate()([x, other_input])\n",
    "    \n",
    "    # Red densa final con skip connections\n",
    "    for units in [128, 64]:\n",
    "        skip = combined\n",
    "        x = Dense(units, activation='relu')(combined)\n",
    "        x = LayerNormalization(epsilon=GRU_CONFIG['epsilon'])(x)\n",
    "        x = Dropout(GRU_CONFIG['dropout_rate'])(x)\n",
    "        if skip.shape[-1] == units:\n",
    "            combined = Add()([x, skip])\n",
    "        else:\n",
    "            combined = x\n",
    "    \n",
    "    output = Dense(1)(combined)\n",
    "    \n",
    "    return Model(inputs=[cgm_input, other_input], outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_model(cgm_shape: tuple, other_features_shape: tuple) -> Model:\n",
    "    \"\"\"\n",
    "    Crea un modelo RNN optimizado para velocidad con procesamiento temporal distribuido.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    cgm_shape : tuple\n",
    "        Forma de los datos CGM (samples, timesteps, features)\n",
    "    other_features_shape : tuple\n",
    "        Forma de otras características (samples, features)\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    Model\n",
    "        Modelo RNN compilado\n",
    "    \"\"\"\n",
    "    # Entradas\n",
    "    cgm_input = Input(shape=cgm_shape[1:])\n",
    "    other_input = Input(shape=(other_features_shape[1],))\n",
    "    \n",
    "    # Procesamiento temporal distribuido inicial\n",
    "    if RNN_CONFIG['use_time_distributed']:\n",
    "        x = TimeDistributed(Dense(32, activation=RNN_CONFIG['activation']))(cgm_input)\n",
    "        x = TimeDistributed(BatchNormalization(epsilon=RNN_CONFIG['epsilon']))(x)\n",
    "    else:\n",
    "        x = cgm_input\n",
    "    \n",
    "    # Reducir secuencia temporal para procesamiento más rápido\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    # Capas RNN con menos unidades pero bidireccionales\n",
    "    for units in RNN_CONFIG['hidden_units']:\n",
    "        rnn_layer = SimpleRNN(\n",
    "            units,\n",
    "            activation=RNN_CONFIG['activation'],\n",
    "            dropout=RNN_CONFIG['dropout_rate'],\n",
    "            recurrent_dropout=RNN_CONFIG['recurrent_dropout'],\n",
    "            return_sequences=True,\n",
    "            unroll=True  # Desenrollar para secuencias cortas\n",
    "        )\n",
    "        \n",
    "        if RNN_CONFIG['bidirectional']:\n",
    "            x = Bidirectional(rnn_layer)(x)\n",
    "        else:\n",
    "            x = rnn_layer(x)\n",
    "            \n",
    "        x = BatchNormalization(\n",
    "            epsilon=RNN_CONFIG['epsilon'],\n",
    "            momentum=0.9  # Aumentar momentum para actualización más rápida\n",
    "        )(x)\n",
    "    \n",
    "    # Último RNN sin return_sequences\n",
    "    final_rnn = SimpleRNN(\n",
    "        RNN_CONFIG['hidden_units'][-1],\n",
    "        activation=RNN_CONFIG['activation'],\n",
    "        dropout=RNN_CONFIG['dropout_rate'],\n",
    "        recurrent_dropout=RNN_CONFIG['recurrent_dropout'],\n",
    "        unroll=True\n",
    "    )\n",
    "    \n",
    "    if RNN_CONFIG['bidirectional']:\n",
    "        x = Bidirectional(final_rnn)(x)\n",
    "    else:\n",
    "        x = final_rnn(x)\n",
    "    \n",
    "    # Combinar características\n",
    "    x = Concatenate()([x, other_input])\n",
    "    \n",
    "    # Reducir capas densas\n",
    "    x = Dense(32, activation=RNN_CONFIG['activation'])(x)\n",
    "    x = BatchNormalization(epsilon=RNN_CONFIG['epsilon'])(x)\n",
    "    x = Dropout(RNN_CONFIG['dropout_rate'])(x)\n",
    "    \n",
    "    output = Dense(1)(x)\n",
    "    \n",
    "    return Model(inputs=[cgm_input, other_input], outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable()\n",
    "class GLU(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Gated Linear Unit como capa personalizada.\n",
    "    \"\"\"\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.dense = Dense(units * 2)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense(inputs)\n",
    "        return x[:, :self.units] * tf.nn.sigmoid(x[:, self.units:])\n",
    "\n",
    "@register_keras_serializable()\n",
    "class MultiHeadFeatureAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Atención multi-cabeza para características.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads: int, key_dim: int, dropout: float = 0.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=key_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        attention_output = self.attention(inputs, inputs, training=training)\n",
    "        return self.layernorm(inputs + attention_output)\n",
    "\n",
    "@register_keras_serializable()\n",
    "class EnhancedFeatureTransformer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Transformador de características mejorado con atención y ghost batch norm.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim: int, num_heads: int, \n",
    "                 virtual_batch_size: int, dropout_rate: float = 0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.feature_dim = feature_dim\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        \n",
    "        # GLU layers\n",
    "        self.glu1 = GLU(feature_dim)\n",
    "        self.glu2 = GLU(feature_dim)\n",
    "        \n",
    "        # Attention layer\n",
    "        self.attention = MultiHeadFeatureAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=feature_dim // num_heads,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "        \n",
    "        # Ghost Batch Normalization\n",
    "        self.ghost_bn1 = tf.keras.layers.BatchNormalization(\n",
    "            virtual_batch_size=virtual_batch_size\n",
    "        )\n",
    "        self.ghost_bn2 = tf.keras.layers.BatchNormalization(\n",
    "            virtual_batch_size=virtual_batch_size\n",
    "        )\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.glu1(inputs)\n",
    "        x = self.ghost_bn1(x, training=training)\n",
    "        x = self.attention(x, training=training)\n",
    "        x = self.glu2(x)\n",
    "        x = self.ghost_bn2(x, training=training)\n",
    "        return self.dropout(x, training=training)\n",
    "\n",
    "def custom_softmax(x: tf.Tensor, axis: int=-1) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Implementación de softmax con estabilidad numérica.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    x : tf.Tensor\n",
    "        Tensor de entrada\n",
    "    axis : int\n",
    "        Eje de normalización\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    tf.Tensor\n",
    "        Tensor normal\n",
    "    \"\"\"\n",
    "    exp_x = tf.exp(x - tf.reduce_max(x, axis=axis, keepdims=True))\n",
    "    return exp_x / tf.reduce_sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "def glu(x: tf.Tensor, n_units: int) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Gated Linear Unit.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    x : tf.Tensor\n",
    "        Tensor de entrada\n",
    "    n_units : int\n",
    "        Número de unidades\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    tf.Tensor\n",
    "        Tensor GLU\n",
    "    \"\"\"\n",
    "    return x[:, :n_units] * tf.nn.sigmoid(x[:, n_units:])\n",
    "\n",
    "def feature_transformer(x: tf.Tensor, feature_dim: int, batch_momentum: float=0.98) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Transformador de características.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    x : tf.Tensor\n",
    "        Tensor de entrada\n",
    "    feature_dim : int\n",
    "        Dimensión de las características\n",
    "    batch_momentum : float\n",
    "        Momento de la normalización por lotes\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    tf.Tensor\n",
    "        Tensor transform\n",
    "    \"\"\"\n",
    "    transform = Dense(feature_dim * 2)(x)\n",
    "    transform = glu(transform, feature_dim)\n",
    "    return BatchNormalization(momentum=batch_momentum)(transform)\n",
    "\n",
    "@register_keras_serializable()\n",
    "class TabNetModel(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Modelo TabNet personalizado con manejo de pérdidas de entropía.\n",
    "    \"\"\"\n",
    "    def __init__(self, cgm_shape, other_features_shape, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.cgm_shape = cgm_shape\n",
    "        self.other_shape = other_features_shape\n",
    "        self.entropy_tracker = tf.keras.metrics.Mean(name='entropy_loss')\n",
    "        \n",
    "        # Definir capas\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.feature_dropout = tf.keras.layers.Dropout(TABNET_CONFIG['feature_dropout'])\n",
    "        self.transformers = [\n",
    "            EnhancedFeatureTransformer(\n",
    "                feature_dim=TABNET_CONFIG['feature_dim'],\n",
    "                num_heads=TABNET_CONFIG['num_attention_heads'],\n",
    "                virtual_batch_size=TABNET_CONFIG['virtual_batch_size'],\n",
    "                dropout_rate=TABNET_CONFIG['attention_dropout']\n",
    "            ) for _ in range(TABNET_CONFIG['num_decision_steps'])\n",
    "        ]\n",
    "        \n",
    "        # Capas finales\n",
    "        self.final_dense1 = Dense(TABNET_CONFIG['output_dim'], activation='selu')\n",
    "        self.final_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.final_dropout = tf.keras.layers.Dropout(TABNET_CONFIG['attention_dropout'])\n",
    "        self.final_dense2 = Dense(TABNET_CONFIG['output_dim'] // 2, activation='selu')\n",
    "        self.final_norm2 = tf.keras.layers.LayerNormalization()\n",
    "        self.final_dense3 = Dense(TABNET_CONFIG['output_dim'], activation='selu')\n",
    "        self.output_layer = Dense(1)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        cgm_input, other_input = inputs\n",
    "        \n",
    "        # Procesamiento inicial\n",
    "        x = self.flatten(cgm_input)\n",
    "        x = Concatenate()([x, other_input])\n",
    "        \n",
    "        # Feature masking\n",
    "        if training:\n",
    "            feature_mask = self.feature_dropout(tf.ones_like(x))\n",
    "            x = tf.multiply(x, feature_mask)\n",
    "        \n",
    "        # Pasos de decisión\n",
    "        step_outputs = []\n",
    "        entropy_loss = 0.0\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            step_output = transformer(x, training=training)\n",
    "            \n",
    "            # Feature selection\n",
    "            attention_mask = Dense(x.shape[-1])(step_output)\n",
    "            mask = custom_softmax(attention_mask)\n",
    "            masked_x = tf.multiply(x, mask)\n",
    "            \n",
    "            step_outputs.append(masked_x)\n",
    "            \n",
    "            if training:\n",
    "                # Calcular entropía\n",
    "                entropy = tf.reduce_mean(tf.reduce_sum(\n",
    "                    -mask * tf.math.log(mask + 1e-15), axis=1\n",
    "                ))\n",
    "                entropy_loss += entropy\n",
    "        \n",
    "        # Combinar salidas con atención\n",
    "        combined = tf.stack(step_outputs, axis=1)\n",
    "        attention_weights = Dense(len(step_outputs), activation='softmax')(\n",
    "            tf.reduce_mean(combined, axis=2)\n",
    "        )\n",
    "        x = tf.reduce_sum(\n",
    "            combined * tf.expand_dims(attention_weights, -1),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Actualizar métrica de entropía\n",
    "        if training:\n",
    "            entropy_loss *= TABNET_CONFIG['sparsity_coefficient']\n",
    "            self.entropy_tracker.update_state(entropy_loss)\n",
    "            self.add_loss(entropy_loss)\n",
    "        \n",
    "        # Capas finales con residual\n",
    "        x = self.final_dense1(x)\n",
    "        x = self.final_norm1(x)\n",
    "        x = self.final_dropout(x, training=training)\n",
    "        \n",
    "        skip = x\n",
    "        x = self.final_dense2(x)\n",
    "        x = self.final_norm2(x)\n",
    "        x = self.final_dense3(x)\n",
    "        x = tf.keras.layers.Add()([x, skip])\n",
    "        \n",
    "        return self.output_layer(x)\n",
    "\n",
    "def create_tabnet_model(cgm_shape: tuple, other_features_shape: tuple) -> Model:\n",
    "    \"\"\"\n",
    "    Crea un modelo TabNet mejorado.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    cgm_shape : tuple\n",
    "        Forma de los datos CGM\n",
    "    other_features_shape : tuple\n",
    "        Forma de otras características\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    Model\n",
    "        Modelo TabNet compilado\n",
    "    \"\"\"\n",
    "    model = TabNetModel(cgm_shape, other_features_shape)\n",
    "    \n",
    "    # Build model\n",
    "    dummy_cgm = tf.keras.layers.Input(shape=cgm_shape[1:])\n",
    "    dummy_other = tf.keras.layers.Input(shape=(other_features_shape[1],))\n",
    "    model([dummy_cgm, dummy_other])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Convolutional Network (TCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable()\n",
    "class WeightNormalization(tf.keras.layers.Wrapper):\n",
    "    \"\"\"\n",
    "    Normalización de pesos para capas convolucionales.\n",
    "    \"\"\"\n",
    "    def __init__(self, layer, **kwargs):\n",
    "        super().__init__(layer, **kwargs)\n",
    "        self.layer = layer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.layer.build(input_shape)\n",
    "        self.g = self.add_weight(\n",
    "            name='g',\n",
    "            shape=(self.layer.filters,),\n",
    "            initializer='ones',\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        weights = self.layer.weights[0]\n",
    "        norm = tf.sqrt(tf.sum(tf.square(weights), axis=[0, 1]))\n",
    "        self.layer.kernel = weights * (self.g / norm)\n",
    "        outputs = self.layer.call(inputs)\n",
    "        return outputs\n",
    "\n",
    "@register_keras_serializable()\n",
    "class CausalPadding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Capa personalizada para padding causal.\n",
    "    \"\"\"\n",
    "    def __init__(self, padding_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.padding_size = padding_size\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.pad(inputs, [[0, 0], [self.padding_size, 0], [0, 0]])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1] + self.padding_size, input_shape[2])\n",
    "\n",
    "def create_tcn_block(input_layer: tf.Tensor, filters: int, kernel_size: int, \n",
    "                    dilation_rate: int, dropout_rate: float) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Crea un bloque TCN (Temporal Convolutional Network).\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    input_layer : tf.Tensor\n",
    "        Capa de entrada\n",
    "    filters : int\n",
    "        Número de filtros\n",
    "    kernel_size : int\n",
    "        Tamaño del kernel\n",
    "    dilation_rate : int\n",
    "        Tasa de dilatación\n",
    "    dropout_rate : float\n",
    "        Tasa de dropout\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    tf.Tensor\n",
    "        Salida del bloque TCN\n",
    "    \"\"\"\n",
    "    padding_size = (kernel_size - 1) * dilation_rate\n",
    "    padded_input = CausalPadding(padding_size)(input_layer)\n",
    "    \n",
    "    # Convolución con weight normalization\n",
    "    conv_layer = Conv1D(\n",
    "        filters=filters * 2,  # Double for gating\n",
    "        kernel_size=kernel_size,\n",
    "        dilation_rate=dilation_rate,\n",
    "        padding='valid',\n",
    "        activation=None\n",
    "    )\n",
    "    \n",
    "    if TCN_CONFIG['use_weight_norm']:\n",
    "        conv_layer = WeightNormalization(conv_layer)\n",
    "    \n",
    "    conv = conv_layer(padded_input)\n",
    "    \n",
    "    # Gating mechanism (GLU)\n",
    "    gate, linear = tf.split(conv, 2, axis=-1)\n",
    "    gate = tf.nn.sigmoid(gate)\n",
    "    conv = linear * gate\n",
    "    \n",
    "    # Normalization\n",
    "    if TCN_CONFIG['use_layer_norm']:\n",
    "        conv = LayerNormalization(epsilon=TCN_CONFIG['epsilon'])(conv)\n",
    "    else:\n",
    "        conv = BatchNormalization()(conv)\n",
    "    \n",
    "    # Spatial dropout\n",
    "    if TCN_CONFIG['use_spatial_dropout']:\n",
    "        conv = tf.keras.layers.SpatialDropout1D(dropout_rate)(conv)\n",
    "    else:\n",
    "        conv = Dropout(dropout_rate)(conv)\n",
    "    \n",
    "    # Residual connection\n",
    "    if input_layer.shape[-1] == filters:\n",
    "        cropped_input = input_layer[:, -conv.shape[1]:, :]\n",
    "        if TCN_CONFIG['residual_dropout'] > 0:\n",
    "            cropped_input = Dropout(TCN_CONFIG['residual_dropout'])(cropped_input)\n",
    "        return Add()([conv, cropped_input])\n",
    "    \n",
    "    return conv\n",
    "\n",
    "def create_tcn_model(input_shape: tuple, other_features_shape: tuple) -> Model:\n",
    "    \"\"\"\n",
    "    Crea un modelo TCN completo.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    input_shape : tuple\n",
    "        Forma de los datos CGM\n",
    "    other_features_shape : tuple\n",
    "        Forma de otras características\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    Model\n",
    "        Modelo TCN compilado\n",
    "    \"\"\"\n",
    "    cgm_input = Input(shape=input_shape[1:], name='cgm_input')\n",
    "    other_input = Input(shape=(other_features_shape[1],), name='other_input')\n",
    "    \n",
    "    # Proyección inicial\n",
    "    x = Conv1D(TCN_CONFIG['filters'][0], 1, padding='same')(cgm_input)\n",
    "    \n",
    "    # Bloques TCN con skip connections\n",
    "    skip_connections = []\n",
    "    \n",
    "    for filters in TCN_CONFIG['filters']:\n",
    "        for dilation_rate in TCN_CONFIG['dilations']:\n",
    "            tcn_out = create_tcn_block(\n",
    "                x,\n",
    "                filters=filters,\n",
    "                kernel_size=TCN_CONFIG['kernel_size'],\n",
    "                dilation_rate=dilation_rate,\n",
    "                dropout_rate=TCN_CONFIG['dropout_rate'][0]\n",
    "            )\n",
    "            skip_connections.append(tcn_out)\n",
    "            x = tcn_out\n",
    "    \n",
    "    # Combinar skip connections con normalización\n",
    "    if skip_connections:\n",
    "        target_len = skip_connections[-1].shape[1]\n",
    "        aligned_skips = [skip[:, -target_len:, :] for skip in skip_connections]\n",
    "        x = Add()(aligned_skips)\n",
    "        x = x / tf.sqrt(float(len(skip_connections)))  # Scale appropriately\n",
    "    \n",
    "    # Global pooling con concatenación de estadísticas\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "    x = tf.keras.layers.Concatenate()([avg_pool, max_pool])\n",
    "    \n",
    "    # Combinar con otras características\n",
    "    x = tf.keras.layers.Concatenate()([x, other_input])\n",
    "    \n",
    "    # MLP final con residual connections\n",
    "    skip = x\n",
    "    x = Dense(128, activation=TCN_CONFIG['activation'])(x)\n",
    "    x = LayerNormalization(epsilon=TCN_CONFIG['epsilon'])(x)\n",
    "    x = Dropout(TCN_CONFIG['dropout_rate'][0])(x)\n",
    "    x = Dense(128, activation=TCN_CONFIG['activation'])(x)\n",
    "    if skip.shape[-1] == 128:\n",
    "        x = Add()([x, skip])\n",
    "    \n",
    "    x = Dense(64, activation=TCN_CONFIG['activation'])(x)\n",
    "    x = LayerNormalization(epsilon=TCN_CONFIG['epsilon'])(x)\n",
    "    x = Dropout(TCN_CONFIG['dropout_rate'][1])(x)\n",
    "    \n",
    "    output = Dense(1)(x)\n",
    "    \n",
    "    return Model(inputs=[cgm_input, other_input], outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable()\n",
    "class PositionEncoding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Codificación posicional para el Transformer.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_position: int, d_model: int, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_position = max_position\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        positions = tf.range(self.max_position, dtype=tf.float32)[:, tf.newaxis]\n",
    "        dimensions = tf.range(self.d_model, dtype=tf.float32)[tf.newaxis, :]\n",
    "        angle_rates = 1 / tf.pow(10000.0, (2 * (dimensions // 2)) / tf.cast(self.d_model, tf.float32))\n",
    "        angle_rads = positions * angle_rates\n",
    "\n",
    "        # Apply sin to even indices, cos to odd indices\n",
    "        pos_encoding = tf.stack([\n",
    "            tf.sin(angle_rads[:, 0::2]),\n",
    "            tf.cos(angle_rads[:, 1::2])\n",
    "        ], axis=-1)\n",
    "\n",
    "        self.pos_encoding = tf.reshape(pos_encoding, [self.max_position, self.d_model])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        sequence_length = tf.shape(inputs)[1]\n",
    "        return inputs + self.pos_encoding[:sequence_length, :]\n",
    "\n",
    "def create_transformer_block(inputs, head_size, num_heads, ff_dim, dropout_rate, prenorm=True):\n",
    "    \"\"\"\n",
    "    Crea un bloque Transformer mejorado con pre/post normalización.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    inputs : tf.Tensor\n",
    "        Tensor de entrada\n",
    "    head_size : int\n",
    "        Tamaño de la cabeza de atención\n",
    "    num_heads : int\n",
    "        Número de cabezas de atención\n",
    "    ff_dim : int\n",
    "        Dimensión de la red feed-forward\n",
    "    dropout_rate : float\n",
    "        Tasa de dropout\n",
    "    prenorm : bool\n",
    "        Indica si se usa pre-normalización\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    tf.Tensor\n",
    "        Tensor procesado\n",
    "    \"\"\"\n",
    "    if prenorm:\n",
    "        # Pre-normalization architecture (better training stability)\n",
    "        x = LayerNormalization(epsilon=TRANSFORMER_CONFIG['epsilon'])(inputs)\n",
    "        x = MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=head_size,\n",
    "            value_dim=head_size,\n",
    "            use_bias=TRANSFORMER_CONFIG['use_bias'],\n",
    "            dropout=dropout_rate\n",
    "        )(x, x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        res1 = Add()([inputs, x])\n",
    "        \n",
    "        # Feed-forward network\n",
    "        x = LayerNormalization(epsilon=TRANSFORMER_CONFIG['epsilon'])(res1)\n",
    "        x = Dense(ff_dim, activation=TRANSFORMER_CONFIG['activation'])(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        x = Dense(inputs.shape[-1])(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        return Add()([res1, x])\n",
    "    else:\n",
    "        # Post-normalization architecture (original)\n",
    "        attn = MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=head_size,\n",
    "            value_dim=head_size,\n",
    "            use_bias=TRANSFORMER_CONFIG['use_bias'],\n",
    "            dropout=dropout_rate\n",
    "        )(inputs, inputs)\n",
    "        attn = Dropout(dropout_rate)(attn)\n",
    "        res1 = LayerNormalization(epsilon=TRANSFORMER_CONFIG['epsilon'])(inputs + attn)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        ffn = Dense(ff_dim, activation=TRANSFORMER_CONFIG['activation'])(res1)\n",
    "        ffn = Dropout(dropout_rate)(ffn)\n",
    "        ffn = Dense(inputs.shape[-1])(ffn)\n",
    "        ffn = Dropout(dropout_rate)(ffn)\n",
    "        return LayerNormalization(epsilon=TRANSFORMER_CONFIG['epsilon'])(res1 + ffn)\n",
    "\n",
    "def create_transformer_model(cgm_shape: tuple, other_features_shape: tuple) -> Model:\n",
    "    \"\"\"\n",
    "    Crea un modelo Transformer con entrada dual para datos CGM y otras características.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    cgm_shape : tuple\n",
    "        Forma de los datos CGM (samples, timesteps, features)\n",
    "    other_features_shape : tuple\n",
    "        Forma de otras características (samples, features)\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    Model\n",
    "        Modelo Transformer compilado\n",
    "    \"\"\"\n",
    "    cgm_input = Input(shape=cgm_shape[1:], name='cgm_input')\n",
    "    other_input = Input(shape=(other_features_shape[1],), name='other_input')\n",
    "    \n",
    "    # Proyección inicial y codificación posicional\n",
    "    x = Dense(TRANSFORMER_CONFIG['key_dim'] * TRANSFORMER_CONFIG['num_heads'])(cgm_input)\n",
    "    if TRANSFORMER_CONFIG['use_relative_pos']:\n",
    "        x = PositionEncoding(\n",
    "            TRANSFORMER_CONFIG['max_position'],\n",
    "            TRANSFORMER_CONFIG['key_dim'] * TRANSFORMER_CONFIG['num_heads']\n",
    "        )(x)\n",
    "    \n",
    "    # Bloques Transformer\n",
    "    for _ in range(TRANSFORMER_CONFIG['num_layers']):\n",
    "        x = create_transformer_block(\n",
    "            x,\n",
    "            TRANSFORMER_CONFIG['head_size'],\n",
    "            TRANSFORMER_CONFIG['num_heads'],\n",
    "            TRANSFORMER_CONFIG['ff_dim'],\n",
    "            TRANSFORMER_CONFIG['dropout_rate'],\n",
    "            TRANSFORMER_CONFIG['prenorm']\n",
    "        )\n",
    "    \n",
    "    # Pooling con estadísticas\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "    x = Concatenate()([avg_pool, max_pool])\n",
    "    \n",
    "    # Combinar con otras características\n",
    "    x = Concatenate()([x, other_input])\n",
    "    \n",
    "    # MLP final con residual connections\n",
    "    skip = x\n",
    "    x = Dense(128, activation=TRANSFORMER_CONFIG['activation'])(x)\n",
    "    x = LayerNormalization(epsilon=TRANSFORMER_CONFIG['epsilon'])(x)\n",
    "    x = Dropout(TRANSFORMER_CONFIG['dropout_rate'])(x)\n",
    "    x = Dense(128, activation=TRANSFORMER_CONFIG['activation'])(x)\n",
    "    if skip.shape[-1] == 128:\n",
    "        x = Add()([x, skip])\n",
    "    \n",
    "    x = Dense(64, activation=TRANSFORMER_CONFIG['activation'])(x)\n",
    "    x = LayerNormalization(epsilon=TRANSFORMER_CONFIG['epsilon'])(x)\n",
    "    x = Dropout(TRANSFORMER_CONFIG['dropout_rate'])(x)\n",
    "    \n",
    "    output = Dense(1)(x)\n",
    "    \n",
    "    return Model(inputs=[cgm_input, other_input], outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wavenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable()\n",
    "class WaveNetBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Bloque WaveNet mejorado con activaciones gated y escalado adaptativo.\n",
    "    \"\"\"\n",
    "    def __init__(self, filters, kernel_size, dilation_rate, dropout_rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        \n",
    "        # Gated convolutions\n",
    "        self.filter_conv = Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation_rate=dilation_rate,\n",
    "            padding='causal'\n",
    "        )\n",
    "        self.gate_conv = Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation_rate=dilation_rate,\n",
    "            padding='causal'\n",
    "        )\n",
    "        \n",
    "        # Normalization and regularization\n",
    "        self.filter_norm = BatchNormalization()\n",
    "        self.gate_norm = BatchNormalization()\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        \n",
    "        # Projections\n",
    "        self.residual_proj = Conv1D(filters, 1, padding='same')\n",
    "        self.skip_proj = Conv1D(filters, 1, padding='same')\n",
    "        \n",
    "        # Scaling factors\n",
    "        self.residual_scale = WAVENET_CONFIG['use_residual_scale']\n",
    "        self.use_skip_scale = WAVENET_CONFIG['use_skip_scale']\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # Gated activation\n",
    "        filter_out = self.filter_conv(inputs)\n",
    "        gate_out = self.gate_conv(inputs)\n",
    "        \n",
    "        filter_out = self.filter_norm(filter_out, training=training)\n",
    "        gate_out = self.gate_norm(gate_out, training=training)\n",
    "        \n",
    "        # tanh(filter) * sigmoid(gate)\n",
    "        gated_out = tf.nn.tanh(filter_out) * tf.nn.sigmoid(gate_out)\n",
    "        gated_out = self.dropout(gated_out, training=training)\n",
    "        \n",
    "        # Residual connection\n",
    "        residual = self.residual_proj(inputs)\n",
    "        residual = residual[:, -gated_out.shape[1]:, :]\n",
    "        residual_out = (gated_out * self.residual_scale) + residual\n",
    "        \n",
    "        # Skip connection\n",
    "        skip_out = self.skip_proj(gated_out)\n",
    "        if self.use_skip_scale:\n",
    "            skip_out = skip_out * tf.math.sqrt(self.residual_scale)\n",
    "        \n",
    "        return residual_out, skip_out\n",
    "\n",
    "def create_wavenet_block(x, filters, kernel_size, dilation_rate, dropout_rate):\n",
    "    \"\"\"\n",
    "    Crea un bloque WaveNet con conexiones residuales y skip connections.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    x : tf.Tensor\n",
    "        Tensor de entrada\n",
    "    filters : int\n",
    "        Número de filtros de la capa convolucional\n",
    "    kernel_size : int\n",
    "        Tamaño del kernel de la capa convolucional\n",
    "    dilation_rate : int\n",
    "        Tasa de dilatación de la capa convolucional\n",
    "    dropout_rate : float\n",
    "        Tasa de dropout\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    tf.Tensor\n",
    "        Tensor de salida del bloque WaveNet\n",
    "    \"\"\"\n",
    "    # Convolución dilatada\n",
    "    conv = Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                 dilation_rate=dilation_rate, padding='causal')(x)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    conv = Dropout(dropout_rate)(conv)\n",
    "    \n",
    "    # Conexión residual con proyección 1x1 si es necesario\n",
    "    if x.shape[-1] != filters:\n",
    "        x = Conv1D(filters, 1, padding='same')(x)\n",
    "    \n",
    "    # Alinear dimensiones temporales\n",
    "    x = x[:, -conv.shape[1]:, :]\n",
    "    res = Add()([conv, x])\n",
    "    \n",
    "    return res, conv\n",
    "\n",
    "def create_wavenet_model(cgm_shape: tuple, other_features_shape: tuple) -> Model:\n",
    "    \"\"\"\n",
    "    Crea un modelo WaveNet para predicción de series temporales.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    cgm_shape : tuple\n",
    "        Forma de los datos CGM (samples, timesteps, features)\n",
    "    other_features_shape : tuple\n",
    "        Forma de otras características (samples, features)\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    Model\n",
    "        Modelo WaveNet compilado\n",
    "    \"\"\"\n",
    "    cgm_input = Input(shape=cgm_shape[1:])\n",
    "    other_input = Input(shape=(other_features_shape[1],))\n",
    "    \n",
    "    # Proyección inicial\n",
    "    x = Conv1D(WAVENET_CONFIG['filters'][0], 1, padding='same')(cgm_input)\n",
    "    \n",
    "    # Saltar conexiones\n",
    "    skip_outputs = []\n",
    "    \n",
    "    # WaveNet stack\n",
    "    for filters in WAVENET_CONFIG['filters']:\n",
    "        for dilation in WAVENET_CONFIG['dilations']:\n",
    "            wavenet_block = WaveNetBlock(\n",
    "                filters=filters,\n",
    "                kernel_size=WAVENET_CONFIG['kernel_size'],\n",
    "                dilation_rate=dilation,\n",
    "                dropout_rate=WAVENET_CONFIG['dropout_rate']\n",
    "            )\n",
    "            x, skip = wavenet_block(x)\n",
    "            skip_outputs.append(skip)\n",
    "    \n",
    "    # Combinar skip connections\n",
    "    if skip_outputs:\n",
    "        target_len = skip_outputs[-1].shape[1]\n",
    "        aligned_skips = [skip[:, -target_len:, :] for skip in skip_outputs]\n",
    "        x = Add()(aligned_skips) / tf.sqrt(float(len(skip_outputs)))\n",
    "    \n",
    "    # Post-procesamiento\n",
    "    x = Activation(WAVENET_CONFIG['activation'])(x)\n",
    "    x = Conv1D(WAVENET_CONFIG['filters'][-1], 1, padding='same')(x)\n",
    "    x = Activation(WAVENET_CONFIG['activation'])(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Combinación con otras features\n",
    "    x = Concatenate()([x, other_input])\n",
    "    \n",
    "    # Capas densas finales con residual connections\n",
    "    skip = x\n",
    "    x = Dense(128)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(WAVENET_CONFIG['activation'])(x)\n",
    "    x = Dropout(WAVENET_CONFIG['dropout_rate'])(x)\n",
    "    x = Dense(128)(x)\n",
    "    if skip.shape[-1] == 128:\n",
    "        x = Add()([x, skip])\n",
    "    \n",
    "    output = Dense(1)(x)\n",
    "    \n",
    "    return Model(inputs=[cgm_input, other_input], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CREATORS = {\n",
    "    'CNN': create_cnn_model,\n",
    "    'Transformer': create_transformer_model,\n",
    "    'GRU': create_gru_model,\n",
    "    'Attention': create_attention_model,\n",
    "    'RNN': create_rnn_model,\n",
    "    'TabNet': create_tabnet_model,\n",
    "    'TCN': create_tcn_model,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones Visualización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(histories: dict, model_names: list):\n",
    "    \"\"\"\n",
    "    Visualiza el historial de entrenamiento de múltiples modelos.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    histories : dict\n",
    "        Diccionario con historiales de entrenamiento por modelo\n",
    "    model_names : list\n",
    "        Lista de nombres de modelos\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for name, history in histories.items():\n",
    "        plt.plot(history['loss'], label=f'{name} (train)')\n",
    "        plt.plot(history['val_loss'], label=f'{name} (val)', linestyle='--')\n",
    "    \n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Pérdida MSE')\n",
    "    plt.title('Comparación de Historiales de Entrenamiento')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, 'training_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_predictions_comparison(y_test: np.ndarray, predictions: dict):\n",
    "    \"\"\"\n",
    "    Visualiza comparación de predicciones de múltiples modelos.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    y_test : np.ndarray\n",
    "        Valores reales de prueba\n",
    "    predictions : dict\n",
    "        Diccionario con predicciones por modelo\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Scatter plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for name, y_pred in predictions.items():\n",
    "        plt.scatter(y_test, y_pred, alpha=0.5, label=name)\n",
    "    plt.plot([0, 15], [0, 15], 'r--')\n",
    "    plt.xlabel('Dosis Real (u. de insulina)')\n",
    "    plt.ylabel('Dosis Predicha (u. de insulina)')\n",
    "    plt.legend()\n",
    "    plt.title('Predicción vs. Real (Todos los Modelos)')\n",
    "    \n",
    "    # Residuals\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for name, y_pred in predictions.items():\n",
    "        plt.hist(y_test - y_pred, bins=20, alpha=0.5, label=name)\n",
    "    plt.xlabel('Residuo (u. de insulina)')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.legend()\n",
    "    plt.title('Distribución de Residuos')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, 'predictions_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(x_cgm, x_other, y, batch_size=32):\n",
    "    \"\"\"\n",
    "    Crea un dataset optimizado usando tf.data.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    x_cgm : np.ndarray\n",
    "        Datos CGM\n",
    "    x_other : np.ndarray\n",
    "        Otras características\n",
    "    y : np.ndarray\n",
    "        Etiquetas\n",
    "    batch_size : int\n",
    "        Tamaño del batch\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    tf.data.Dataset\n",
    "        Dataset optimizado\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        (x_cgm, x_other), y\n",
    "    ))\n",
    "    return dataset.cache().shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ensemble_prediction(predictions_dict: dict, weights: np.ndarray = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Combina predicciones de múltiples modelos usando un promedio ponderado.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    predictions_dict : dict\n",
    "        Diccionario con predicciones de cada modelo\n",
    "    weights : np.ndarray, opcional\n",
    "        Pesos para cada modelo. Si es None, usa promedio simple\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    np.ndarray\n",
    "        Predicciones combinadas del ensemble\n",
    "    \"\"\"\n",
    "    all_preds = np.stack(list(predictions_dict.values()))\n",
    "    if weights is None:\n",
    "        weights = np.ones(len(predictions_dict)) / len(predictions_dict)\n",
    "    return np.average(all_preds, axis=0, weights=weights)\n",
    "\n",
    "def optimize_ensemble_weights(predictions_dict: dict, y_true: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Optimiza los pesos del ensemble usando validación cruzada.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    predictions_dict : dict\n",
    "        Diccionario con predicciones de cada modelo\n",
    "    y_true : np.ndarray\n",
    "        Valores reales\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    np.ndarray\n",
    "        Pesos optimizados para cada modelo\n",
    "    \"\"\"\n",
    "    \n",
    "    def objective(weights):\n",
    "        # Normalizar pesos\n",
    "        weights = weights / np.sum(weights)\n",
    "        # Obtener predicción del ensemble\n",
    "        ensemble_pred = create_ensemble_prediction(predictions_dict, weights)\n",
    "        # Calcular error\n",
    "        return mean_squared_error(y_true, ensemble_pred)\n",
    "    \n",
    "    n_models = len(predictions_dict)\n",
    "    initial_weights = np.ones(n_models) / n_models\n",
    "    bounds = [(0, 1) for _ in range(n_models)]\n",
    "    \n",
    "    result = minimize(\n",
    "        objective,\n",
    "        initial_weights,\n",
    "        bounds=bounds,\n",
    "        constraints={'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n",
    "    )\n",
    "    \n",
    "    return result.x / np.sum(result.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model: Model, model_name: str, \n",
    "                           x_cgm_train: np.ndarray, x_other_train: np.ndarray, \n",
    "                           y_train: np.ndarray, x_cgm_val: np.ndarray, \n",
    "                           x_other_val: np.ndarray, y_val: np.ndarray,\n",
    "                           x_cgm_test: np.ndarray, x_other_test: np.ndarray, \n",
    "                           y_test: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Entrena y evalúa un modelo específico con características avanzadas de entrenamiento.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    model : Model\n",
    "        Modelo a entrenar\n",
    "    model_name : str\n",
    "        Nombre del modelo para guardado/logging\n",
    "    x_cgm_train, x_other_train, y_train : np.ndarray\n",
    "        Datos de entrenamiento\n",
    "    x_cgm_val, x_other_val, y_val : np.ndarray\n",
    "        Datos de validación\n",
    "    x_cgm_test, x_other_test, y_test : np.ndarray\n",
    "        Datos de prueba\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    tuple\n",
    "        (history, y_pred, metrics_dict)\n",
    "    \"\"\"\n",
    "    # Habilitar compilación XLA\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    \n",
    "    # Crear datasets optimizados\n",
    "    train_ds = create_dataset(x_cgm_train, x_other_train, y_train)\n",
    "    val_ds = create_dataset(x_cgm_val, x_other_val, y_val)\n",
    "    \n",
    "    # Configurar learning rate con decaimiento\n",
    "    initial_learning_rate = 0.001\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=1000,\n",
    "        decay_rate=0.9\n",
    "    )\n",
    "    \n",
    "    # Optimizador con gradient clipping\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=lr_schedule,\n",
    "        clipnorm=1.0\n",
    "    )\n",
    "    \n",
    "    # Habilitar entrenamiento con precisión mixta\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    \n",
    "    # Compilar modelo con múltiples métricas\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mse',\n",
    "        metrics=['mae', tf.keras.metrics.RootMeanSquaredError()]\n",
    "    )\n",
    "    \n",
    "    # Callbacks para monitoreo y optimización\n",
    "    callbacks = [\n",
    "        # Early stopping para evitar overfitting\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        # Reducción de learning rate cuando el modelo se estanca\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6\n",
    "        ),\n",
    "        # Guardado del mejor modelo\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            os.path.join(MODELS_DIR, f'best_{model_name}.h5'),\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True\n",
    "        ),\n",
    "        # TensorBoard para visualización\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=os.path.join(MODELS_DIR, 'logs', model_name),\n",
    "            histogram_freq=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=100,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Predecir y evaluar\n",
    "    y_pred = model.predict([x_cgm_test, x_other_test]).flatten()\n",
    "    \n",
    "    # Calcular métricas\n",
    "    metrics = {\n",
    "        'mae': mean_absolute_error(y_test, y_pred),\n",
    "        'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        'r2': r2_score(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    # Guardar modelo final\n",
    "    model.save(os.path.join(MODELS_DIR, f'{model_name}.keras'))\n",
    "    \n",
    "    # Restaurar política de precisión default\n",
    "    tf.keras.mixed_precision.set_global_policy('float32')\n",
    "    \n",
    "    return history, y_pred, metrics\n",
    "\n",
    "def train_model_parallel(name, input_shapes):\n",
    "    \"\"\"\n",
    "    Entrenamiento en paralelo de un modelo específico.\n",
    "    \n",
    "    Parámeteros:\n",
    "    -----------\n",
    "    name : str\n",
    "        Name of the model to create\n",
    "    input_shapes : tuple\n",
    "        Shapes for CGM and other inputs\n",
    "    \"\"\"\n",
    "    print(f\"\\nEntrenando modelo {name}...\")\n",
    "    \n",
    "    \n",
    "    model = MODEL_CREATORS[name](input_shapes[0], input_shapes[1])\n",
    "    \n",
    "    return name, train_and_evaluate_model(\n",
    "        model, name,\n",
    "        x_cgm_train, x_other_train, y_train,\n",
    "        x_cgm_val, x_other_val, y_val,\n",
    "        x_cgm_test, x_other_test, y_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(create_model_fn, x_cgm: np.ndarray, x_other: np.ndarray, \n",
    "                        y: np.ndarray, n_splits: int = 5) -> tuple:\n",
    "    \"\"\"\n",
    "    Realiza validación cruzada de un modelo.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    create_model_fn : callable\n",
    "        Función que crea el modelo\n",
    "    x_cgm : np.ndarray\n",
    "        Datos CGM\n",
    "    x_other : np.ndarray\n",
    "        Otras características\n",
    "    y : np.ndarray\n",
    "        Etiquetas\n",
    "    n_splits : int\n",
    "        Número de divisiones para validación cruzada\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    tuple\n",
    "        (media_metricas, std_metricas)\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(x_cgm)):\n",
    "        print(f\"\\nEntrenando fold {fold + 1}/{n_splits}\")\n",
    "        \n",
    "        # Dividir datos\n",
    "        x_cgm_train_fold = x_cgm[train_idx]\n",
    "        x_cgm_val_fold = x_cgm[val_idx]\n",
    "        x_other_train_fold = x_other[train_idx]\n",
    "        x_other_val_fold = x_other[val_idx]\n",
    "        y_train_fold = y[train_idx]\n",
    "        y_val_fold = y[val_idx]\n",
    "        \n",
    "        # Crear y entrenar modelo\n",
    "        model = create_model_fn()\n",
    "        history = train_and_evaluate_model(\n",
    "            model=model,\n",
    "            model_name=f'fold_{fold}',\n",
    "            x_cgm_train=x_cgm_train_fold,\n",
    "            x_other_train=x_other_train_fold,\n",
    "            y_train=y_train_fold,\n",
    "            x_cgm_val=x_cgm_val_fold,\n",
    "            x_other_val=x_other_val_fold,\n",
    "            y_val=y_val_fold,\n",
    "            x_cgm_test=x_cgm_val_fold,\n",
    "            x_other_test=x_other_val_fold,\n",
    "            y_test=y_val_fold\n",
    "        )\n",
    "        \n",
    "        scores.append(history[2])  # Append metrics dictionary\n",
    "    \n",
    "    # Calcular estadísticas\n",
    "    mean_scores = {\n",
    "        metric: np.mean([s[metric] for s in scores])\n",
    "        for metric in scores[0].keys()\n",
    "    }\n",
    "    std_scores = {\n",
    "        metric: np.std([s[metric] for s in scores])\n",
    "        for metric in scores[0].keys()\n",
    "    }\n",
    "    \n",
    "    return mean_scores, std_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = {\n",
    "    'batch_size': 32,\n",
    "    'epochs': 100,\n",
    "    'learning_rate': 0.001,\n",
    "    'patience': 10,\n",
    "    'mixed_precision': True\n",
    "}\n",
    "\n",
    "def train_model_sequential(args):\n",
    "    \"\"\"Train a model and return only picklable results\"\"\"\n",
    "    name, input_shapes = args\n",
    "    \n",
    "    try:\n",
    "        # Create datasets with prefetching\n",
    "        train_ds = (\n",
    "            create_dataset(x_cgm_train, x_other_train, y_train)\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    "        )\n",
    "        val_ds = (\n",
    "            create_dataset(x_cgm_val, x_other_val, y_val)\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    "        )\n",
    "        \n",
    "        # Create and compile model\n",
    "        model = MODEL_CREATORS[name](input_shapes[0], input_shapes[1])\n",
    "        \n",
    "        # Compile with gradient clipping\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(\n",
    "                learning_rate=train_config['learning_rate'],\n",
    "                clipnorm=1.0\n",
    "            ),\n",
    "            loss='mse',\n",
    "            jit_compile=False  # XLA disabled\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=train_config['epochs'],\n",
    "            batch_size=train_config['batch_size'],\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=train_config['patience'],\n",
    "                    restore_best_weights=True\n",
    "                ),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                    monitor='val_loss',\n",
    "                    factor=0.5,\n",
    "                    patience=5,\n",
    "                    min_lr=1e-6\n",
    "                ),\n",
    "                tf.keras.callbacks.ModelCheckpoint(\n",
    "                    f'checkpoints/{name}_best.h5',\n",
    "                    monitor='val_loss',\n",
    "                    save_best_only=True,\n",
    "                    mode='min'\n",
    "                )\n",
    "            ],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Predictions with error handling\n",
    "        try:\n",
    "            y_pred = model.predict(\n",
    "                [x_cgm_test, x_other_test],\n",
    "                batch_size=train_config['batch_size'],\n",
    "                verbose=1\n",
    "            ).flatten()\n",
    "            \n",
    "            return {\n",
    "                'name': name,\n",
    "                'history': history.history,\n",
    "                'predictions': y_pred,\n",
    "                'model': model\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Prediction error for {name}: {str(e)}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Training error for {name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def calculate_metrics(predictions, y_true):\n",
    "    \"\"\"Calculate metrics for predictions\"\"\"\n",
    "    return {\n",
    "        'mae': mean_absolute_error(y_true, predictions),\n",
    "        'rmse': np.sqrt(mean_squared_error(y_true, predictions)),\n",
    "        'r2': r2_score(y_true, predictions)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_features(x_cgm, x_other):\n",
    "    # Add derivative features for CGM\n",
    "    cgm_diff = np.diff(x_cgm.squeeze(), axis=1)\n",
    "    cgm_diff = np.pad(cgm_diff, ((0,0), (1,0), (0,0)), mode='edge')\n",
    "    \n",
    "    # Add rolling statistics\n",
    "    window = 5\n",
    "    rolling_mean = np.apply_along_axis(\n",
    "        lambda x: np.convolve(x, np.ones(window)/window, mode='same'),\n",
    "        1, x_cgm.squeeze()\n",
    "    )\n",
    "    \n",
    "    x_cgm_enhanced = np.concatenate([\n",
    "        x_cgm,\n",
    "        cgm_diff[..., np.newaxis],\n",
    "        rolling_mean[..., np.newaxis]\n",
    "    ], axis=-1)\n",
    "    \n",
    "    return x_cgm_enhanced, x_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento y Evaluación de los Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento y evaluación de modelos\n",
    "# Configure GPU memory at the very beginning\n",
    "try:\n",
    "    # Attempt to configure GPU memory\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        # Configure GPU memory growth\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU memory growth enabled\")\n",
    "    else:\n",
    "        print(\"No GPU devices found, using CPU\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"GPU configuration error: {e}\")\n",
    "\n",
    "# Disable XLA\n",
    "tf.config.optimizer.set_jit(False)\n",
    "\n",
    "# Training Configuration\n",
    "train_config = {\n",
    "    'batch_size': 32,\n",
    "    'epochs': 100,\n",
    "    'learning_rate': 0.001,\n",
    "    'patience': 10,\n",
    "    'mixed_precision': True\n",
    "}\n",
    "\n",
    "# Enable mixed precision if requested\n",
    "if train_config['mixed_precision']:\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    print(\"Mixed precision enabled\")\n",
    "\n",
    "# Entrenamiento y evaluación de modelos\n",
    "input_shapes = (x_cgm_train.shape, x_other_train.shape)\n",
    "models_names = ['CNN', 'Transformer', 'GRU', 'Attention', 'RNN', 'TabNet', 'TCN', 'WaveNet']\n",
    "\n",
    "histories = {}\n",
    "predictions = {}\n",
    "metrics = {}\n",
    "\n",
    "# Train models\n",
    "model_results = []\n",
    "for name in models_names:\n",
    "    print(f\"\\nEntrenando modelo {name}...\")\n",
    "    try:\n",
    "        result = train_model_sequential((name, input_shapes))\n",
    "        if result is not None:\n",
    "            model_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error entrenando {name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Process results in parallel\n",
    "print(\"\\nCalculando métricas en paralelo...\")\n",
    "with Parallel(n_jobs=-1, verbose=1) as parallel:\n",
    "    metric_results = parallel(\n",
    "        delayed(calculate_metrics)(\n",
    "            np.array(result['predictions']), \n",
    "            y_test\n",
    "        ) for result in model_results\n",
    "    )\n",
    "\n",
    "# Store results\n",
    "for result, metric in zip(model_results, metric_results):\n",
    "    name = result['name']\n",
    "    histories[name] = result['history']\n",
    "    predictions[name] = np.array(result['predictions'])\n",
    "    metrics[name] = metric\n",
    "\n",
    "# Evaluación por sujeto\n",
    "print(\"\\nRendimiento por sujeto:\")\n",
    "for subject_id in test_subjects:\n",
    "    mask = subject_test == subject_id\n",
    "    y_test_sub = y_test[mask]\n",
    "    \n",
    "    print(f\"\\nSujeto {subject_id}:\")\n",
    "    print(\"-\" * 40)\n",
    "    for name, y_pred in predictions.items():\n",
    "        y_pred_sub = y_pred[mask]\n",
    "        mae_sub = mean_absolute_error(y_test_sub, y_pred_sub)\n",
    "        rmse_sub = np.sqrt(mean_squared_error(y_test_sub, y_pred_sub))\n",
    "        r2_sub = r2_score(y_test_sub, y_pred_sub)\n",
    "        print(f\"{name:<15} MAE={mae_sub:.2f}, RMSE={rmse_sub:.2f}, R²={r2_sub:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de los Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de resultados\n",
    "plot_training_history(histories, models_names)\n",
    "plot_predictions_comparison(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After storing individual model results\n",
    "print(\"\\nCreando predicciones del ensemble...\")\n",
    "\n",
    "# Crear predicciones del ensemble\n",
    "ensemble_pred = create_ensemble_prediction(predictions)\n",
    "ensemble_metrics = calculate_metrics(ensemble_pred, y_test)\n",
    "\n",
    "# Agregar métricas del ensemble\n",
    "metrics['Ensemble'] = ensemble_metrics\n",
    "predictions['Ensemble'] = ensemble_pred\n",
    "\n",
    "# Optimizar pesos del ensemble\n",
    "print(\"\\nOptimizando pesos del ensemble...\")\n",
    "optimal_weights = optimize_ensemble_weights(predictions, y_test)\n",
    "\n",
    "# Crear predicción del ensemble con pesos optimizados\n",
    "ensemble_pred_optimized = create_ensemble_prediction(predictions, optimal_weights)\n",
    "ensemble_metrics_optimized = calculate_metrics(ensemble_pred_optimized, y_test)\n",
    "\n",
    "# Agregar métricas del ensemble optimizado\n",
    "metrics['Ensemble (Opt)'] = ensemble_metrics_optimized\n",
    "predictions['Ensemble (Opt)'] = ensemble_pred_optimized\n",
    "\n",
    "# Validación cruzada para cada modelo\n",
    "print(\"\\nRealizando validación cruzada...\")\n",
    "cv_results = {}\n",
    "\n",
    "for name in models_names:\n",
    "    print(f\"\\nValidación cruzada para {name}\")\n",
    "    model_creator = lambda name=name: MODEL_CREATORS[name](input_shapes[0], input_shapes[1])\n",
    "    mean_scores, std_scores = cross_validate_model(\n",
    "        create_model_fn=model_creator,\n",
    "        x_cgm=x_cgm,\n",
    "        x_other=x_other,\n",
    "        y=y\n",
    "    )\n",
    "    cv_results[name] = {\n",
    "        'mean': mean_scores,\n",
    "        'std': std_scores\n",
    "    }\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"\\nResultados de validación cruzada:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Modelo':<15} {'MAE':>12} {'RMSE':>12} {'R²':>12}\")\n",
    "print(\"-\" * 70)\n",
    "for name, results in cv_results.items():\n",
    "    mean = results['mean']\n",
    "    std = results['std']\n",
    "    print(f\"{name:<15} {mean['mae']:>8.2f}±{std['mae']:4.2f} \"\n",
    "          f\"{mean['rmse']:>8.2f}±{std['rmse']:4.2f} \"\n",
    "          f\"{mean['r2']:>8.2f}±{std['r2']:4.2f}\")\n",
    "\n",
    "# Actualizar visualizaciones\n",
    "plot_training_history(histories, models_names + ['Ensemble', 'Ensemble (Opt)'])\n",
    "plot_predictions_comparison(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas Comparativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir métricas comparativas\n",
    "print(\"\\nComparación de métricas:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Modelo':<15} {'MAE':>8} {'RMSE':>8} {'R²':>8}\")\n",
    "print(\"-\" * 50)\n",
    "for name, metric in metrics.items():\n",
    "    print(f\"{name:<15} {metric['mae']:8.2f} {metric['rmse']:8.2f} {metric['r2']:8.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
