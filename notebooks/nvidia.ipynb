{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install --upgrade pip\n",
    "%pip install polars numpy scikit-learn matplotlib joblib openpyxl fastexcel tensorflow tensorflow.keras torch \n",
    "\n",
    "# For TensorFlow on Mac, you need to install tensorflow-macos\n",
    "%pip install tensorflow-macos tensorflow-metal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from joblib import Parallel, delayed\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Configuración de matplotlib\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de la ruta del proyecto\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "SUBJECTS_RELATIVE_PATH = \"data/Subjects\"\n",
    "SUBJECTS_PATH = os.path.join(PROJECT_ROOT, SUBJECTS_RELATIVE_PATH)\n",
    "\n",
    "# Crear directorios para resultados\n",
    "FIGURES_DIR = os.path.join(PROJECT_ROOT, \"figures\", \"various_models\")\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "MODELS_DIR = os.path.join(PROJECT_ROOT, \"models\")\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# Número de trabajadores para paralelizar\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "subject_files = [f for f in os.listdir(SUBJECTS_PATH) if f.startswith(\"Subject\") and f.endswith(\".xlsx\")]\n",
    "print(f\"Total sujetos: {len(subject_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificación GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU Availability with PyTorch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# Small test operation on GPU\n",
    "if torch.cuda.is_available():\n",
    "    x = torch.randn(1000, 1000, device=device)\n",
    "    y = torch.matmul(x, x)\n",
    "    print(\"GPU test successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cgm_window(bolus_time, cgm_df: pl.DataFrame, window_hours: int = 2) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Obtiene la ventana de datos CGM para un tiempo de bolo específico.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    bolus_time : datetime\n",
    "        Tiempo del bolo de insulina\n",
    "    cgm_df : pl.DataFrame\n",
    "        DataFrame con datos CGM\n",
    "    window_hours : int, opcional\n",
    "        Horas de la ventana de datos (default: 2)\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    np.ndarray\n",
    "        Ventana de datos CGM o None si no hay suficientes datos\n",
    "    \"\"\"\n",
    "    window_start = bolus_time - timedelta(hours=window_hours)\n",
    "    window = cgm_df.filter(\n",
    "        (pl.col(\"date\") >= window_start) & (pl.col(\"date\") <= bolus_time)\n",
    "    ).sort(\"date\").tail(24)\n",
    "    \n",
    "    if window.height < 24:\n",
    "        return None\n",
    "    return window.get_column(\"mg/dl\").to_numpy()\n",
    "\n",
    "def calculate_iob(bolus_time, basal_df: pl.DataFrame, half_life_hours: float = 4.0) -> float:\n",
    "    \"\"\"\n",
    "    Calcula la insulina activa en el cuerpo (IOB).\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    bolus_time : datetime\n",
    "        Tiempo del bolo de insulina\n",
    "    basal_df : pl.DataFrame\n",
    "        DataFrame con datos de insulina basal\n",
    "    half_life_hours : float, opcional\n",
    "        Vida media de la insulina en horas (default: 4.0)\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    float\n",
    "        Cantidad de insulina activa\n",
    "    \"\"\"\n",
    "    if basal_df is None or basal_df.is_empty():\n",
    "        return 0.0\n",
    "    \n",
    "    iob = 0.0\n",
    "    for row in basal_df.iter_rows(named=True):\n",
    "        start_time = row[\"date\"]\n",
    "        duration_hours = row[\"duration\"] / (1000 * 3600)\n",
    "        end_time = start_time + timedelta(hours=duration_hours)\n",
    "        rate = row[\"rate\"] if row[\"rate\"] is not None else 0.9\n",
    "        \n",
    "        if start_time <= bolus_time <= end_time:\n",
    "            time_since_start = (bolus_time - start_time).total_seconds() / 3600\n",
    "            remaining = rate * (1 - (time_since_start / half_life_hours))\n",
    "            iob += max(0.0, remaining)\n",
    "    return iob\n",
    "\n",
    "def process_subject(subject_path: str, idx: int) -> list:\n",
    "    \"\"\"\n",
    "    Procesa los datos de un sujeto.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    subject_path : str\n",
    "        Ruta al archivo del sujeto\n",
    "    idx : int\n",
    "        Índice del sujeto\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    list\n",
    "        Lista de diccionarios con características procesadas\n",
    "    \"\"\"\n",
    "    print(f\"Procesando {os.path.basename(subject_path)} ({idx+1})...\")\n",
    "    \n",
    "    try:\n",
    "        cgm_df = pl.read_excel(subject_path, sheet_name=\"CGM\")\n",
    "        bolus_df = pl.read_excel(subject_path, sheet_name=\"Bolus\")\n",
    "        try:\n",
    "            basal_df = pl.read_excel(subject_path, sheet_name=\"Basal\")\n",
    "        except Exception:\n",
    "            basal_df = None\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar {os.path.basename(subject_path)}: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Conversión de fechas\n",
    "    cgm_df = cgm_df.with_columns(pl.col(\"date\").cast(pl.Datetime))\n",
    "    bolus_df = bolus_df.with_columns(pl.col(\"date\").cast(pl.Datetime))\n",
    "    if basal_df is not None:\n",
    "        basal_df = basal_df.with_columns(pl.col(\"date\").cast(pl.Datetime))\n",
    "    \n",
    "    cgm_df = cgm_df.sort(\"date\")\n",
    "\n",
    "    processed_data = []\n",
    "    for row in bolus_df.iter_rows(named=True):\n",
    "        bolus_time = row[\"date\"]\n",
    "        cgm_window = get_cgm_window(bolus_time, cgm_df)\n",
    "        \n",
    "        if cgm_window is not None:\n",
    "            iob = calculate_iob(bolus_time, basal_df)\n",
    "            hour_of_day = bolus_time.hour / 23.0\n",
    "            bg_input = row[\"bgInput\"] if row[\"bgInput\"] is not None else cgm_window[-1]\n",
    "            \n",
    "            features = {\n",
    "                'subject_id': idx,\n",
    "                'cgm_window': cgm_window,\n",
    "                'carbInput': row[\"carbInput\"] if row[\"carbInput\"] is not None else 0.0,\n",
    "                'bgInput': bg_input,\n",
    "                'insulinCarbRatio': row[\"insulinCarbRatio\"] if row[\"insulinCarbRatio\"] is not None else 10.0,\n",
    "                'insulinSensitivityFactor': 50.0,\n",
    "                'insulinOnBoard': iob,\n",
    "                'hour_of_day': hour_of_day,\n",
    "                'normal': row[\"normal\"] if row[\"normal\"] is not None else 0.0\n",
    "            }\n",
    "            processed_data.append(features)\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "all_processed_data = Parallel(n_jobs=-1)(\n",
    "    delayed(process_subject)(\n",
    "        os.path.join(SUBJECTS_PATH, f), \n",
    "        idx\n",
    "    ) for idx, f in enumerate(subject_files)\n",
    ")\n",
    "\n",
    "# Aplanar lista de listas\n",
    "all_processed_data = [item for sublist in all_processed_data for item in sublist]\n",
    "\n",
    "# Convertir a DataFrame \n",
    "df_processed = pl.DataFrame(all_processed_data)\n",
    "print(\"Muestra de datos procesados combinados:\")\n",
    "print(df_processed.head())\n",
    "print(f\"Total de muestras: {len(df_processed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEstadísticas de 'normal' por sujeto (antes de normalización):\")\n",
    "for subject_id in df_processed.get_column('subject_id').unique():\n",
    "    subject_data = df_processed.filter(pl.col('subject_id') == subject_id).get_column('normal')\n",
    "    stats = subject_data.describe()\n",
    "    print(f\"Sujeto {subject_id}: \"\n",
    "          f\"min={stats['min']:.2f}, \"\n",
    "          f\"max={stats['max']:.2f}, \"\n",
    "          f\"mean={stats['mean']:.2f}, \"\n",
    "          f\"std={stats['std']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División Ventana CGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir ventana CGM y otras características\n",
    "cgm_columns = [f'cgm_{i}' for i in range(24)]\n",
    "df_cgm = pl.DataFrame({\n",
    "    col: [row['cgm_window'][i] for row in all_processed_data]\n",
    "    for i, col in enumerate(cgm_columns)\n",
    "}, schema={col: pl.Float64 for col in cgm_columns})\n",
    "\n",
    "# Combinar con otras características\n",
    "df_final = pl.concat([\n",
    "    df_cgm,\n",
    "    df_processed.drop('cgm_window')\n",
    "], how=\"horizontal\")\n",
    "\n",
    "# Verificar valores nulos\n",
    "print(\"Verificación de valores nulos en df_final:\")\n",
    "df_final = df_final.drop_nulls()\n",
    "print(df_final.null_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar características\n",
    "scaler_cgm = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_other = StandardScaler()\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Normalizar CGM\n",
    "X_cgm = scaler_cgm.fit_transform(df_final.select(cgm_columns).to_numpy())\n",
    "X_cgm = X_cgm.reshape(X_cgm.shape[0], X_cgm.shape[1], 1)\n",
    "\n",
    "# Extraer subject_id como tensor separado\n",
    "X_subject = df_final.get_column('subject_id').to_numpy()\n",
    "\n",
    "# Normalizar otras características\n",
    "other_features = ['carbInput', 'bgInput', 'insulinOnBoard', 'insulinCarbRatio', \n",
    "                 'insulinSensitivityFactor', 'hour_of_day']\n",
    "X_other = scaler_other.fit_transform(df_final.select(other_features).to_numpy())\n",
    "\n",
    "# Normalizar etiquetas\n",
    "y = df_final.get_column('normal').to_numpy().reshape(-1, 1)\n",
    "y = scaler_y.fit_transform(y).flatten()\n",
    "\n",
    "# Verificar NaN\n",
    "print(\"NaN en X_cgm:\", np.isnan(X_cgm).sum())\n",
    "print(\"NaN en X_other:\", np.isnan(X_other).sum())\n",
    "print(\"NaN en X_subject:\", np.isnan(X_subject).sum())\n",
    "print(\"NaN en y:\", np.isnan(y).sum())\n",
    "\n",
    "if np.isnan(X_cgm).sum() > 0 or np.isnan(X_other).sum() > 0 or \\\n",
    "   np.isnan(X_subject).sum() > 0 or np.isnan(y).sum() > 0:\n",
    "    raise ValueError(\"Valores NaN detectados en X_cgm, X_other, X_subject o y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División por Sujeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# División por sujeto\n",
    "subject_ids = df_final.get_column('subject_id').unique().to_numpy()\n",
    "train_subjects, temp_subjects = train_test_split(subject_ids, test_size=0.2, random_state=42)\n",
    "val_subjects, test_subjects = train_test_split(temp_subjects, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de Máscaras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear máscaras \n",
    "train_mask = df_final.get_column('subject_id').is_in(train_subjects).to_numpy()\n",
    "val_mask = df_final.get_column('subject_id').is_in(val_subjects).to_numpy()\n",
    "test_mask = df_final.get_column('subject_id').is_in(test_subjects).to_numpy()\n",
    "\n",
    "X_cgm_train, X_cgm_val, X_cgm_test = X_cgm[train_mask], X_cgm[val_mask], X_cgm[test_mask]\n",
    "X_other_train, X_other_val, X_other_test = X_other[train_mask], X_other[val_mask], X_other[test_mask]\n",
    "X_subject_train, X_subject_val, X_subject_test = X_subject[train_mask], X_subject[val_mask], X_subject[test_mask]\n",
    "y_train, y_val, y_test = y[train_mask], y[val_mask], y[test_mask]\n",
    "subject_test = df_final.filter(pl.col('subject_id').is_in(test_subjects)).get_column('subject_id').to_numpy()\n",
    "\n",
    "print(f\"Entrenamiento CGM: {X_cgm_train.shape}, Validación CGM: {X_cgm_val.shape}, Prueba CGM: {X_cgm_test.shape}\")\n",
    "print(f\"Entrenamiento Otros: {X_other_train.shape}, Validación Otros: {X_other_val.shape}, Prueba Otros: {X_other_test.shape}\")\n",
    "print(f\"Entrenamiento Subject: {X_subject_train.shape}, Validación Subject: {X_subject_val.shape}, Prueba Subject: {X_subject_test.shape}\")\n",
    "print(f\"Sujetos de prueba: {test_subjects}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversión a Tensores PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a tensores PyTorch\n",
    "X_cgm_train = torch.tensor(X_cgm_train, dtype=torch.float32).to(device)\n",
    "X_cgm_val = torch.tensor(X_cgm_val, dtype=torch.float32).to(device)\n",
    "X_cgm_test = torch.tensor(X_cgm_test, dtype=torch.float32).to(device)\n",
    "X_other_train = torch.tensor(X_other_train, dtype=torch.float32).to(device)\n",
    "X_other_val = torch.tensor(X_other_val, dtype=torch.float32).to(device)\n",
    "X_other_test = torch.tensor(X_other_test, dtype=torch.float32).to(device)\n",
    "X_subject_train = torch.tensor(X_subject_train, dtype=torch.long).to(device)\n",
    "X_subject_val = torch.tensor(X_subject_val, dtype=torch.long).to(device)\n",
    "X_subject_test = torch.tensor(X_subject_test, dtype=torch.long).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataLoaders\n",
    "train_dataset = TensorDataset(X_cgm_train, X_other_train, X_subject_train, y_train)\n",
    "val_dataset = TensorDataset(X_cgm_val, X_other_val, X_subject_val, y_val)\n",
    "test_dataset = TensorDataset(X_cgm_test, X_other_test, X_subject_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos en PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo LSTM mejorado con embeddings de sujeto.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    num_subjects : int\n",
    "        Número total de sujetos para la capa de embedding\n",
    "    embedding_dim : int\n",
    "        Dimensión del embedding de sujetos (default: 8)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_subjects: int, embedding_dim: int = 8):\n",
    "        super(EnhancedLSTM, self).__init__()\n",
    "        self.subject_embedding = nn.Embedding(num_subjects, embedding_dim)\n",
    "        self.lstm1 = nn.LSTM(input_size=1, hidden_size=128, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=128, hidden_size=64, batch_first=True)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(64)\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        self.concat_dense = nn.Linear(64 + 6 + embedding_dim, 128)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "        self.output_layer = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, cgm_input, other_input, subject_ids):\n",
    "        subject_embed = self.subject_embedding(subject_ids)\n",
    "        lstm_out, _ = self.lstm1(cgm_input)\n",
    "        lstm_out, _ = self.lstm2(lstm_out)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        lstm_out = self.batch_norm1(lstm_out)\n",
    "        lstm_out = self.dropout1(lstm_out)\n",
    "        combined = torch.cat((lstm_out, other_input, subject_embed), dim=1)\n",
    "        dense_out = torch.relu(self.concat_dense(combined))\n",
    "        dense_out = self.batch_norm2(dense_out)\n",
    "        dense_out = self.dropout2(dense_out)\n",
    "        output = self.output_layer(dense_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCNTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo híbrido TCN-Transformer con embeddings de sujeto.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    num_subjects : int\n",
    "        Número total de sujetos para la capa de embedding\n",
    "    embedding_dim : int\n",
    "        Dimensión del embedding de sujetos (default: 8)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_subjects: int, embedding_dim: int = 8):\n",
    "        super(TCNTransformer, self).__init__()\n",
    "        self.subject_embedding = nn.Embedding(num_subjects, embedding_dim)\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=128, kernel_size=3, padding='same')\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=3, padding='same')\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=3, padding='same')\n",
    "        self.ln1 = nn.LayerNorm(128)\n",
    "        self.transformer = nn.TransformerEncoderLayer(d_model=128, nhead=8, dropout=0.2, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer, num_layers=2)\n",
    "        self.ln2 = nn.LayerNorm(128)\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.concat_dense1 = nn.Linear(128 + 6 + embedding_dim, 256)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        self.dense2 = nn.Linear(256, 128)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "        self.output_layer = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, cgm_input, other_input, subject_ids):\n",
    "        subject_embed = self.subject_embedding(subject_ids)\n",
    "        cgm_input = cgm_input.permute(0, 2, 1)\n",
    "        tcn_out = torch.relu(self.conv1(cgm_input))\n",
    "        tcn_out = self.pool1(tcn_out)\n",
    "        tcn_out = torch.relu(self.conv2(tcn_out))\n",
    "        tcn_out = self.pool2(tcn_out)\n",
    "        tcn_out = torch.relu(self.conv3(tcn_out))\n",
    "        tcn_out = tcn_out.permute(0, 2, 1)\n",
    "        tcn_out = self.ln1(tcn_out)\n",
    "        transformer_out = self.transformer_encoder(tcn_out)\n",
    "        transformer_out = transformer_out + tcn_out\n",
    "        transformer_out = self.ln2(transformer_out)\n",
    "        transformer_out = transformer_out.permute(0, 2, 1)\n",
    "        pooled = self.global_pool(transformer_out).squeeze(-1)\n",
    "        combined = torch.cat((pooled, other_input, subject_embed), dim=1)\n",
    "        dense_out = torch.relu(self.concat_dense1(combined))\n",
    "        dense_out = self.batch_norm1(dense_out)\n",
    "        dense_out = self.dropout1(dense_out)\n",
    "        dense_out = torch.relu(self.dense2(dense_out))\n",
    "        dense_out = self.batch_norm2(dense_out)\n",
    "        dense_out = self.dropout2(dense_out)\n",
    "        output = self.output_layer(dense_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mse(y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Función de pérdida MSE personalizada que penaliza más las sobrepredicciones.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    y_true : torch.Tensor\n",
    "        Etiquetas verdaderas\n",
    "    y_pred : torch.Tensor\n",
    "        Etiquetas predichas\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    torch.Tensor\n",
    "        Pérdida MSE\n",
    "    \"\"\"\n",
    "    error = y_true - y_pred\n",
    "    overprediction_penalty = torch.where(error < 0, 2 * error**2, error**2)\n",
    "    return torch.mean(overprediction_penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module, \n",
    "                train_loader: DataLoader, \n",
    "                val_loader: DataLoader, \n",
    "                epochs: int = 200, \n",
    "                patience: int = 30) -> tuple[list, list]:\n",
    "    \"\"\"\n",
    "    Entrena el modelo con early stopping y learning rate scheduling.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        Modelo a entrenar\n",
    "    train_loader : DataLoader\n",
    "        DataLoader de entrenamiento\n",
    "    val_loader : DataLoader\n",
    "        DataLoader de validación\n",
    "    epochs : int\n",
    "        Número máximo de épocas\n",
    "    patience : int\n",
    "        Épocas a esperar antes de early stopping\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    tuple[list, list]\n",
    "        Historiales de pérdida de entrenamiento y validación\n",
    "    \"\"\"\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=10, min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for cgm_batch, other_batch, subject_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(cgm_batch, other_batch, subject_batch).squeeze()\n",
    "            loss = custom_mse(y_batch, y_pred)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * len(y_batch)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for cgm_batch, other_batch, subject_batch, y_batch in val_loader:\n",
    "                y_pred = model(cgm_batch, other_batch, subject_batch).squeeze()\n",
    "                loss = custom_mse(y_batch, y_pred)\n",
    "                val_loss += loss.item() * len(y_batch)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialización y Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número total de sujetos para embeddings\n",
    "num_subjects = len(subject_ids)\n",
    "# Initialize models\n",
    "model_lstm = EnhancedLSTM(num_subjects=num_subjects, embedding_dim=8).to(device)\n",
    "model_tcn = TCNTransformer(num_subjects=num_subjects, embedding_dim=8).to(device)\n",
    "\n",
    "# Train models\n",
    "print(\"\\nEntrenando LSTM Mejorado...\")\n",
    "lstm_train_losses, lstm_val_losses = train_model(model_lstm, train_loader, val_loader)\n",
    "\n",
    "print(\"\\nEntrenando Transformer con TCN...\")\n",
    "tcn_train_losses, tcn_val_losses = train_model(model_tcn, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización Historial de Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Plot Training History\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(lstm_train_losses, label='Pérdida de Entrenamiento (LSTM)')\n",
    "plt.plot(lstm_val_losses, label='Pérdida de Validación (LSTM)')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida MSE Personalizada')\n",
    "plt.legend()\n",
    "plt.title('Historial de Entrenamiento - LSTM Mejorado')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(tcn_train_losses, label='Pérdida de Entrenamiento (TCN)')\n",
    "plt.plot(tcn_val_losses, label='Pérdida de Validación (TCN)')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida MSE Personalizada')\n",
    "plt.legend()\n",
    "plt.title('Historial de Entrenamiento - Transformer con TCN')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'training_history.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de los Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Evaluate Models\n",
    "def evaluate_model(model: nn.Module, loader: DataLoader) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Evalúa el modelo en el conjunto de datos proporcionado.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        Modelo a evaluar\n",
    "    loader : DataLoader\n",
    "        DataLoader con datos de evaluación\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    tuple[np.ndarray, np.ndarray]\n",
    "        Predicciones y valores reales desnormalizados\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for cgm_batch, other_batch, subject_batch, y_batch in loader:\n",
    "            y_pred = model(cgm_batch, other_batch, subject_batch).squeeze()\n",
    "            predictions.append(y_pred.cpu().numpy())\n",
    "            targets.append(y_batch.cpu().numpy())\n",
    "    predictions = np.concatenate(predictions)\n",
    "    targets = np.concatenate(targets)\n",
    "    \n",
    "    # Desnormalizar predicciones y etiquetas\n",
    "    predictions = scaler_y.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "    targets = scaler_y.inverse_transform(targets.reshape(-1, 1)).flatten()\n",
    "    return predictions, targets\n",
    "\n",
    "def rule_based_prediction(X_other: torch.Tensor, target_bg: float = 100.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Realiza predicciones basadas en reglas.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    X_other : torch.Tensor\n",
    "        Tensor con otras características\n",
    "    target_bg : float\n",
    "        Valor objetivo de glucosa en sangre\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    np.ndarray\n",
    "        Predicciones basadas en reglas\n",
    "    \"\"\"\n",
    "    inverse_transformed = scaler_other.inverse_transform(X_other.cpu().numpy())\n",
    "    carb_input = inverse_transformed[:, 0]\n",
    "    bg_input = inverse_transformed[:, 1]\n",
    "    icr = inverse_transformed[:, 3]\n",
    "    isf = inverse_transformed[:, 4]\n",
    "    \n",
    "    # Evitar división por cero\n",
    "    icr = np.where(icr == 0, 1e-6, icr)\n",
    "    isf = np.where(isf == 0, 1e-6, isf)\n",
    "    \n",
    "    carb_component = np.divide(carb_input, icr, out=np.zeros_like(carb_input), where=icr!=0)\n",
    "    bg_component = np.divide(bg_input - target_bg, isf, out=np.zeros_like(bg_input), where=isf!=0)\n",
    "    prediction = carb_component + bg_component\n",
    "    \n",
    "    return np.clip(prediction, 0, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LSTM\n",
    "y_pred_lstm, y_test_lstm = evaluate_model(model_lstm, test_loader)\n",
    "mae_lstm = mean_absolute_error(y_test_lstm, y_pred_lstm)\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_test_lstm, y_pred_lstm))\n",
    "r2_lstm = r2_score(y_test_lstm, y_pred_lstm)\n",
    "\n",
    "# Evaluate TCN\n",
    "y_pred_tcn, y_test_tcn = evaluate_model(model_tcn, test_loader)\n",
    "mae_tcn = mean_absolute_error(y_test_tcn, y_pred_tcn)\n",
    "rmse_tcn = np.sqrt(mean_squared_error(y_test_tcn, y_pred_tcn))\n",
    "r2_tcn = r2_score(y_test_tcn, y_pred_tcn)\n",
    "\n",
    "# Rule-based prediction\n",
    "y_rule = rule_based_prediction(X_other_test)\n",
    "y_test_denorm = scaler_y.inverse_transform(y_test.cpu().numpy().reshape(-1, 1)).flatten()\n",
    "\n",
    "mae_rule = mean_absolute_error(y_test_denorm, y_rule)\n",
    "rmse_rule = np.sqrt(mean_squared_error(y_test_denorm, y_rule))\n",
    "r2_rule = r2_score(y_test_denorm, y_rule)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nResultados generales:\")\n",
    "print(f\"LSTM Mejorado - MAE: {mae_lstm:.2f}, RMSE: {rmse_lstm:.2f}, R²: {r2_lstm:.2f}\")\n",
    "print(f\"Transformer con TCN - MAE: {mae_tcn:.2f}, RMSE: {rmse_tcn:.2f}, R²: {r2_tcn:.2f}\")\n",
    "print(f\"Basado en reglas - MAE: {mae_rule:.2f}, RMSE: {rmse_rule:.2f}, R²: {r2_rule:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendimiento por Sujeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRendimiento por sujeto:\")\n",
    "for subject_id in test_subjects:\n",
    "    mask = subject_test == subject_id\n",
    "    y_test_sub = y_test_denorm[mask]\n",
    "    y_pred_lstm_sub = y_pred_lstm[mask]\n",
    "    y_pred_tcn_sub = y_pred_tcn[mask]\n",
    "    y_rule_sub = y_rule[mask]\n",
    "    \n",
    "    if len(y_test_sub) > 0:\n",
    "        mae_lstm_sub = mean_absolute_error(y_test_sub, y_pred_lstm_sub)\n",
    "        rmse_lstm_sub = np.sqrt(mean_squared_error(y_test_sub, y_pred_lstm_sub))\n",
    "        r2_lstm_sub = r2_score(y_test_sub, y_pred_lstm_sub)\n",
    "        \n",
    "        mae_tcn_sub = mean_absolute_error(y_test_sub, y_pred_tcn_sub)\n",
    "        rmse_tcn_sub = np.sqrt(mean_squared_error(y_test_sub, y_pred_tcn_sub))\n",
    "        r2_tcn_sub = r2_score(y_test_sub, y_pred_tcn_sub)\n",
    "        \n",
    "        mae_rule_sub = mean_absolute_error(y_test_sub, y_rule_sub)\n",
    "        print(f\"Sujeto {subject_id}:\")\n",
    "        print(f\"  LSTM - MAE={mae_lstm_sub:.2f}, RMSE={rmse_lstm_sub:.2f}, R²={r2_lstm_sub:.2f}\")\n",
    "        print(f\"  TCN - MAE={mae_tcn_sub:.2f}, RMSE={rmse_tcn_sub:.2f}, R²={r2_tcn_sub:.2f}\")\n",
    "        print(f\"  Reglas - MAE={mae_rule_sub:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de los Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicciones vs. Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones vs Real\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(y_test_denorm, y_pred_lstm, label='LSTM', alpha=0.5, color='blue')\n",
    "plt.scatter(y_test_denorm, y_pred_tcn, label='TCN', alpha=0.5, color='green')\n",
    "plt.scatter(y_test_denorm, y_rule, label='Basado en Reglas', alpha=0.5, color='orange')\n",
    "plt.plot([0, 15], [0, 15], 'r--')\n",
    "plt.xlabel('Dosis Real (u. de insulina)')\n",
    "plt.ylabel('Dosis Predicha (u. de insulina)')\n",
    "plt.legend()\n",
    "plt.title('Predicciones vs Real (Todos los Sujetos)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribución Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(y_test_denorm - y_pred_lstm, bins=20, label='LSTM', alpha=0.5, color='blue')\n",
    "plt.hist(y_test_denorm - y_pred_tcn, bins=20, label='TCN', alpha=0.5, color='green')\n",
    "plt.hist(y_test_denorm - y_rule, bins=20, label='Basado en Reglas', alpha=0.5, color='orange')\n",
    "plt.xlabel('Residuo (u. de insulina)')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.legend()\n",
    "plt.title('Distribución de Residuos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE por Sujeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 2, 3)\n",
    "mae_lstm_subjects = []\n",
    "mae_tcn_subjects = []\n",
    "mae_rule_subjects = []\n",
    "\n",
    "for sid in test_subjects:\n",
    "    mask = subject_test == sid\n",
    "    if np.sum(mask) > 0:\n",
    "        mae_lstm_subjects.append(mean_absolute_error(y_test_denorm[mask], y_pred_lstm[mask]))\n",
    "        mae_tcn_subjects.append(mean_absolute_error(y_test_denorm[mask], y_pred_tcn[mask]))\n",
    "        mae_rule_subjects.append(mean_absolute_error(y_test_denorm[mask], y_rule[mask]))\n",
    "\n",
    "x = np.arange(len(test_subjects))\n",
    "width = 0.2\n",
    "plt.bar(x - width, mae_lstm_subjects, width, label='LSTM', color='blue')\n",
    "plt.bar(x, mae_tcn_subjects, width, label='TCN', color='green')\n",
    "plt.bar(x + width, mae_rule_subjects, width, label='Basado en Reglas', color='orange')\n",
    "plt.xlabel('Sujeto')\n",
    "plt.ylabel('MAE (u. de insulina)')\n",
    "plt.xticks(x, test_subjects)\n",
    "plt.legend()\n",
    "plt.title('MAE por Sujeto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R² por Sujeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 2, 4)\n",
    "r2_lstm_subjects = []\n",
    "r2_tcn_subjects = []\n",
    "r2_rule_subjects = []\n",
    "\n",
    "for sid in test_subjects:\n",
    "    mask = subject_test == sid\n",
    "    if np.sum(mask) > 0:\n",
    "        r2_lstm_subjects.append(r2_score(y_test_denorm[mask], y_pred_lstm[mask]))\n",
    "        r2_tcn_subjects.append(r2_score(y_test_denorm[mask], y_pred_tcn[mask]))\n",
    "        r2_rule_subjects.append(r2_score(y_test_denorm[mask], y_rule[mask]))\n",
    "\n",
    "plt.bar(x - width, r2_lstm_subjects, width, label='LSTM', color='blue')\n",
    "plt.bar(x, r2_tcn_subjects, width, label='TCN', color='green')\n",
    "plt.bar(x + width, r2_rule_subjects, width, label='Basado en Reglas', color='orange')\n",
    "plt.xlabel('Sujeto')\n",
    "plt.ylabel('R²')\n",
    "plt.xticks(x, test_subjects)\n",
    "plt.legend()\n",
    "plt.title('R² por Sujeto')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'model_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardado de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_lstm.state_dict(), os.path.join(MODELS_DIR, 'lstm.pt'))\n",
    "torch.save(model_tcn.state_dict(), os.path.join(MODELS_DIR, 'tcn.pt'))\n",
    "\n",
    "print(\"\\nModelos guardados en:\", MODELS_DIR)\n",
    "print(\"Figuras guardadas en:\", FIGURES_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
